<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: open data | Mountain Doodles]]></title>
  <link href="http://doodles.mountainmath.ca/blog/categories/open-data/feed.atom" rel="self"/>
  <link href="http://doodles.mountainmath.ca/"/>
  <updated>2016-01-31T16:32:26-08:00</updated>
  <id>http://doodles.mountainmath.ca/</id>
  <author>
    <name><![CDATA[MountainMath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Land Use Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/01/31/land-use/"/>
    <updated>2016-01-31T10:25:42-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/01/31/land-use</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/land_use/map"><img  src="http://doodles.mountainmath.ca/images/land use.png" style="width:50%;float:left;margin-right:10px;"></a>
Metro Vancouver has a <a href="http://www.metrovancouver.org/data">fairly good land use dataset</a> that I imported quite some time
ago so that <a href="https://mountainmath.ca/land_use/map">I could take a good look at it</a>. But then I forgot about it until
<a href="https://twitter.com/HealthyCityMaps">@HealthyCityMaps</a> <a href="https://twitter.com/HealthyCityMaps/status/692832946714050564">reminded me</a> just
at the time when the discussions about <a href="https://mountainmath.ca/map/assessment">assessmened values</a> in Vancouver were flaring up again.</p>

<p>So finally I decided to mark up the City of Vancouver assessment dataset with the land use from the Metro Vancouver land
use dataset. And I used the occasion to update my color schemes from two years ago and gave it a dark background that
works better with the <a href="http://colorbrewer2.org">colorbrewer</a> palettes.</p>

<p>The result of the land use and assessment data mashup is a much better understanding what kind of properties we have right now in Vancouver.</p>

<!-- more -->


<h3>Land Use for Assessment Data</h3>

<p>There is a strong relationship between land use and zoning. But that relationship is far from perfect, zoning is a funny
beast. For example, zoning laws prevent low-rise apartments to be built in areas zoned for single family housing. But
they permit single family housing in areas zoned for low-rise apartments. Or in areas zoned as commercial. The land
use dataset allows to tag every property based on what it is actually used for.</p>

<p>Of course the land use dataset is not perfect and has some issues, but overall it gives a far superior view on how land
is used when compared to soley relying on zoning. But putting the two datasets together one can even further filter down
to tease out actual land use.</p>

<h3>The Forgotten Single Family Houses</h3>

<p>Single family houses have received a lot of attention lately, not least because of the
<a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">incredible gains in land value during the last year</a>.</p>

<p>But how to narrow down the assessed properties to only single family housing? The easiest way to do this is to use zoning
as a proxy, so <a href="https://mountainmath.ca/map/assessment?filter=rs">only use RS zoned properties</a>. But there are a number
of problems with that.</p>

<p><a href="https://mountainmath.ca/map/assessment?filter=rs_not_sfh&amp;layer=14"><img  src="http://doodles.mountainmath.ca/images/rs_not_sfh.png" style="width:50%;float:right;"></a>
Firstly, only filtering by zone will still include lots of unwanted properties like schools and parks. I used to filter out
parks using the City of Vancouver parks dataset, but it actually does not capture all parks. And then I filtered by area
to get rid of large properties that typically house schools or other institutions. But that keeps smaller institutions in
and throws larger single family houses out. Using the land use data we can better focus in on all the residential properties
in the single family housing zone. But even then, there are <a href="https://mountainmath.ca/map/assessment?filter=rs_not_sfh&amp;layer=14">1,255 residential properties in &lsquo;RS&rsquo; zoned areas that are
not single family houses</a>.</p>

<p><a href="https://mountainmath.ca/map/assessment?filter=sfh_not_rs&amp;layer=6"><img  src="http://doodles.mountainmath.ca/images/sfh_not_rs.png" style="width:50%;float:left;margin-right:10px;"></a>
Secondly, and probably more importantly, it misses
<a href="https://mountainmath.ca/map/assessment?filter=sfh_not_rs&amp;layer=6">the 15% (12,111 total) of the single family stock that is situated in other zones</a>.
&lsquo;Upzoning&rsquo; a neighbourhood
only means that other forms of housing are also permitted, but people can still build single family homes there if they
would like.
<a href="https://mountainmath.ca/map/assessment?filter=sfh_not_rs&amp;layer=6">Zoom into the map</a> and choose different ways to colour
the properties if you want to learn more about where they are.</p>

<h3>All Single Family Houses</h3>

<p><a href="https://mountainmath.ca/map/assessment?filter=sfh&amp;layer=9"><img  src="http://doodles.mountainmath.ca/images/sfh.png" style="width:50%;float:right;"></a>
When adding in the land use data we can filter by actual land use. That immediately takes care of parks and schools and other institutions,
and at the same time it allows to find all &ldquo;single family or duplex&rdquo; properties. That gets us closer to what we are looking
for, but it also includes duplexes. But those are easy to detect in the assessment dataset, they show up with two separate
tax bills. So combining the two we get the actual <a href="https://mountainmath.ca/map/assessment?filter=sfh&amp;layer=9">map of all single family homes</a>.</p>

<p>Are these really <em>all</em> single family homes? Not quite. There are some issues with the land use dataset that mis-classify
a couple of properties. In particular some RS lots without a house are classified as &lsquo;Undeveloped&rsquo;, and some are classfied as
&lsquo;Single Detached &amp; Duplex&rsquo;, probably depending on what was there around the time the data was collected. The correct classification really
depends on what one is interested in, and will likely be outdated fast.</p>

<p>Moreover, the property assessment dataset of the City of Vancouver is to correctly link a couple of hundred properties
to their tax data. This won&rsquo;t be an issue when analysing the tax data, but it does cause a problem when mapping the properties
as they won&rsquo;t have any assessment or zoning data associated with them.</p>

<p>And there are issues on the fringes. Some properties in Southlands are used as single family homes on large lots, rather
than the limited agricultural that it is zoned at and labeled as in the land use dataset.</p>

<p>But overall, adding in the land
use information gives a huge improvement on narrowing down single family homes. We should remember that
&ldquo;single family homes&rdquo; are better described as &ldquo;single owner properties that can&rsquo;t be stratified&rdquo;,
as they may include secondary suites and laneway houses.</p>

<h3>Historical Data</h3>

<p>One shortcoming of the land use dataset is that there are no historic versions available. It would be very nice to be able
to see how land use changed over time, but that dataset won&rsquo;t do the trick. The City of Vancouver assessment dataset
comes with historic data all the way up to 2006, and can be used to do some fun
<a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">analysis</a>. But the historic dataset is incomlete in that
zoning information is only available from 2014 forward and in that historic property tax data can&rsquo;t always be linked to
physical properties if the property has been re-developed. But overall putting the datasets together allows to refine
the analysis previously made. For example re-running the <a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">thumb twiddling rates</a>
using only Single Family Homes, we get pretty much the same medians, but the averages are higher. We could also refine
our <a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">teardown analysis</a> to include only single family homes
and better filter out institutional properties, although some issues with the lack of historic data remain.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Work vs Twiddling Thumbs]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/"/>
    <updated>2016-01-24T10:28:37-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs</id>
    <content type="html"><![CDATA[<p><a href="http://www.mountainmath.ca/map/assessment?zoom=14&amp;lat=49.253&amp;lng=-123.1244&amp;layer=12"><img  src="http://doodles.mountainmath.ca/images/twiddling_thumbs.png" style="width:50%;float:left;margin-right:10px;"></a>In
the ongoing debade on affordability of housing in the City of Vancouver we regularly hear voices expressing concern for
&ldquo;<a href="http://news.nationalpost.com/full-comment/andrew-coyne-dont-blame-chinese-billionaires-for-vancouver-housing-prices">elderly folks of modest means who had been counting on using their home as a pension</a>&rdquo;
to argue against doing anything to ease the housing affordability problems many (younger) people are facing. In these
arguments the concern for &ldquo;elderly folks of modest means&rdquo; seems curiously focused to those who own property as opposed to,
for example, the 20% of the elderly people in Vancouver that live in poverty (15k total).</p>

<p>Which got me a little thinking about &ldquo;earning money&rdquo; in this day and age.</p>

<h3>Work</h3>

<!-- more -->


<p>Most people earn money by working. We are interested in after tax household income, for simplicity we will look at 2010 income
data reported in the 2011 census. The median after-tax household income in Vancouver
was $50k (the average was $65k), although there is <a href="http://censusmapper.ca/maps/224">large regional variability</a>.
The pre-tax households incomes were $56k for the median
and $80k for the average income.</p>

<p>Incomes have grown a bit since then, median income levels in
Vancouver grew by 7% from 2010 to 2013 (<a href="http://www5.statcan.gc.ca/cansim/a47">the latest year Stats Canada reports on</a>)
and will have grown a little more since then, but these are relatively small changes compared to other numbers I want
to talk about.</p>

<p><a href="http://censusmapper.ca/maps/189">Most of this income stems from employment income</a> which also includes self-employment,
but 22% of pre-tax income in Vancouver
came from outher sources, mostly investment income but also government transfer payments like child benefits.</p>

<p>Cumulative, the after tax income put $17.8bn into Vancouverite&rsquo;s pockets.</p>

<h3>Twiddling Thumbs</h3>

<p>Let&rsquo;s compare this to how much Vancouver home owner households earned last year by twiddling thumbs. Let&rsquo;s do a simple
calculation and focus on RS (&ldquo;One Family Dwelling&rdquo; zone) homes. We
will consider the change in land value only, changing the building value because of renovation or rebuilding certainly
does not happen by twiddling thumbs. And we will divide the land value rise of each property by the number of owners,
for RS that evenly divides up the land value increase between strata or duplex households that managed to sneak into the RS zoning.</p>

<p>The typical RS household made $267,000 last year (so that&rsquo;s the median land value rise in RS between 2015 and 2016
property tax assessment years, the average was $331,043), for a tity toal of $28,789,790,980. Not too shabby.
And considering that the typical owner household
lives in their home, these are tax-free earnings once the owners sell. That&rsquo;s substantially more than what the typical household
in Vancouver takes home. The total land value increase for just the RS zone properties amounts to $22.2bn out of
<a href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/">the $45.2bn by which land values in Vancouver increased overall</a>. For context,
recall that the overall income earned by all Vancouverites in 2010 was $17.8bn after tax (21.3bn before tax).</p>

<p>In other words, just the RS property owners earned more by twiddling their tumbs than the entire population of the
City of Vancouver did by &ndash; actually working. Yet another
indication of how <a href="http://censusmapper.ca/maps/37">out of touch the property market is with the local enconomy</a>.</p>

<h3>Mappig the Result of Thumb Twiddling</h3>

<p><a href="http://www.mountainmath.ca/map/assessment?zoom=14&amp;lat=49.253&amp;lng=-123.1244&amp;layer=12">There is a map for this</a>. (I probably would not take the time to write about this otherwise.) Last year&rsquo;s assessment
increase was quite large, not every year is like this (although it looks like the coming one will also be substantial).
To even things out a little I mapped the average yearly gross land value increase for each property. And for good measure I, divided
by the number of households for strata, duplex and other properties that are tied to several owners. For properties that I
could not trace back all the way to 2006 I averaged over as many years as I could find.</p>

<p><a href="http://www.mountainmath.ca/map/assessment?zoom=14&amp;lat=49.253&amp;lng=-123.1244&amp;layer=12"><img  src="http://doodles.mountainmath.ca/images/twiddling_thumbs.png" style="width:50%;float:right;"></a>
<a href="http://www.mountainmath.ca/map/assessment?zoom=14&amp;lat=49.253&amp;lng=-123.1244&amp;layer=12">Looking at the map</a> we see that
the average yearly increase depends to a large extend on the square footage of the property. Large properties naturally stand
out, unless they are stratified where the land value increase gets distributed across all the owners.</p>

<p>Averaged over 10 years, the yearly land value increase for the typical RS property in Vancouver was $82,700. Using Main as
a divider the numbers are $66,600 for the
typical east of Main and $145,100 for the typical west of Main property (using median increases). The average increases were $117,334 city wide,
$73,294 east and $175,773 west of Main.</p>

<p>A good portion of the east-west difference is due to larger lot size, but even the land values increase per m&sup2; for properties
on the west side was 35% higher than that of properties on the
east side. These are yearly increases, to be accumulated and collected tax free in most cases when the property is sold.</p>

<h3>Hourly Rate for Twiddling Thumbs</h3>

<p>Just for fun we can work out the hourly rate for twiddling thumbs. All we need to do is divide by the number of work
hours by year. We can look up the average work weeks for a Vancouver worker in the 2011 census and, for style points also consider the
number of household maintainers. After all, when we compare household income with land value increases we should consider how many
people in the household work. 63% of Vancouver households have only one household maintainer, but
<a href="http://censusmapper.ca/maps/223">33% have two and 4% three or more household maintainers</a> contributing to household income.
On average a Vancouver household <a href="http://censusmapper.ca/maps/222">worked a total of 62 person-weeks per year</a> to earn
that income. Assuming a 40 hour work week, we compute the average after-tax earning to be $26 per hour from regular work
and average after tax earnings of $113 per hour from twiddling thumbs. That&rsquo;s assuming all household maintainers are twiddling
the same amount of time that they dedicate to regular work. (In this calculation we use average not median income and land value
increases, medians don&rsquo;t lend themselves well to these kind of reasoning.)</p>

<p>Using the 10 year averaged numbers instead of focusing on just the last year we still get a healthy average after-tax twiddling thumbs rate of $47 per hour.</p>

<p>I am not entirely sure why anyone would want twiddling thumbs to stay so lucrative. Unless maybe the super self-centered thumb-twiddlers.
Or why earnings from thumb-twiddling should remain tax-free. While employment income is taxed (as it should be).</p>

<p>Considering to change your line work to thumb twiddling? To bad thumb twiddling is so unaffordable!</p>

<h2>Update</h2>

<p><a href="http://www.mountainmath.ca/map/special/34?layer=12"><img  src="http://doodles.mountainmath.ca/images/RS_original.png" style="width:50%;float:right;"></a>
I got some good feedback on the post so I decided to clear some things up and re-run the analysis with some small changes.</p>

<p>Linking a map of all properties when the analysis was only done on RS properties was a little confusing. So I added a map
<a href="http://www.mountainmath.ca/map/special/34?layer=12">of all RS zoned properties only</a> that the analysis was based on.</p>

<p>Some people correctly pointed out that there are some schools and other large properties that are RS zone but probably
should not be included in the analysis. So I decided to quickly re-run the analysis where I excluded properties with
area larger than 10,000m&amp;sup2. The cutoff is somewhat arbitrary, but it seemed about right so that schools and other large
properties are excluded, but most large single family lots are still included. The filter is not perfect, but probably
better than running the analysis without the filter. Here is the raw output with the area cutoff:</p>

<h5>Land Value Rise in RS (2015 - 2016)</h5>

<ul>
<li>City Wide: Average $318,877 Median $267,000, Count: 67269, Total: $21,450,566,401, Hourly: $128</li>
<li>East of Main: Average $235,871 Median $231,000, Count: 38319, Total: $9,038,373,901, Hourly: $95</li>
<li>West of Main: Average $428,745 Median $381,000, Count: 28950, Total: $12,412,192,500, Hourly: $172</li>
</ul>


<p>As expected, the area cutoff had almost no effect on the median, but did push the average down a bit.</p>

<p><a href="http://www.mountainmath.ca/map/special/37?layer=12"><img  src="http://doodles.mountainmath.ca/images/RS.png" style="width:50%;float:left;margin-right:10px;"></a>
The map shows the average increases over a 10 year timeframe, here are the numbers with the <a href="http://www.mountainmath.ca/map/special/37?layer=12">larger properties exluded</a>:</p>

<ul>
<li>City Wide: Average $112,588 Median $82,500, Count: 66556, Total: $7,493,456,449, Hourly: $45</li>
<li>East of Main: Average $70,109 Median $66,600, Count: 37960, Total: $2,661,346,243, Hourly: $28</li>
<li>West of Main: Average $168,978 Median $145,100, Count: 28596, Total: $4,832,110,206, Hourly: $68</li>
</ul>


<p>Also, I decided to do another run of the analysis including RT and RM zones (Two and Multiple Family Dwelling) next to RS (One Family Dwelling),
as well as Shaugnessy&rsquo;s FSD zone. Things get a little tricky in RT and RM. There are many single family properties in these
zones, so for those things work as expected. Then there are stratified properties, and we divide by the number of owners
to split up the land value rise. And there are larger rental buildings where the owner takes the entire land value increase.</p>

<p>Here is the raw output of the analysis:</p>

<h5>Land Value Rise in RS, RT, RM and FSD (2015 - 2016)</h5>

<ul>
<li>City Wide: Average $336,504 Median $265,000, Count: $84683, Total: $28,496,185,854, Hourly: $135</li>
<li>East of Main: Average $239,916 Median $230,000, Count: $47893, Total: $11,490,308,602, Hourly: $96</li>
<li>West of Main: Average $462,241 Median $375,000, Count: $36790, Total: $17,005,877,252, Hourly: $186</li>
</ul>


<p>To round things up, here are the numbers for RS, RT, RM and FSD zones averaged over 10 years:</p>

<h5>Land Value Rise in RS, RT, RM and FSD (2006 - 2016)</h5>

<p><a href="http://www.mountainmath.ca/map/special/36?layer=12"><img  src="http://doodles.mountainmath.ca/images/RS+RT+RM+FSD.png" style="width:50%;float:right;"></a></p>

<p>City Wide: Average $119,596 Median $83,000, Count: 83479, Total: $9,983,800,611, Hourly: $48
East of Main: Average $72,767 Median $67,200, Count: 47254, Total: $3,438,565,286, Hourly: $29
West of Main: Average $180,682 Median $143,600, Count: 36225, Total: $6,545,235,325, Hourly: $72</p>

<p>And for good measure, <a href="http://www.mountainmath.ca/map/special/36?layer=12">here is the map for the properties included in the last analysis</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Teardowns]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/"/>
    <updated>2016-01-18T09:52:33-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/special/17"><img  src="http://doodles.mountainmath.ca/images/teardowns.png" style="width:50%;float:left;margin-right:10px;"></a>
On the heels of the <a href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/">new assessment data</a> we
can start to slice the data in different ways to understand various aspects of the real estate landscape in Vancouver. The
fact that <a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data</a> makes historic data available
gives the ability to look for changes over time.</p>

<p><a href="https://mountainmath.ca/map/assessment">Our maps</a> explore this by visualizing some aspects of these changes for all
properties, but it might also be useful to filter the properties we show to focus in on specific criteria.</p>

<p>&ldquo;Teardowns&rdquo; always triggers lots of emotions in Vancouver. Without looking at the emotional side and trying to avoid any
judgement we will
investigate the data to understand what buildings have been torn down recently and predict which buildings will get torn
down next. And map them. Long story short, we predict that
1 in 6 buildings <a href="https://mountainmath.ca/map/special/17?layer=4">on this map</a> (and then some more with lower teardown probability)
will get torn down and rebuilt by 2026.</p>

<h3>Building age temporal distribution</h3>

<!-- more -->


<p>To start understanding teardowns and rebuilds let&rsquo;s look at the <a href="https://mountainmath.ca/map/assessment?layer=7">age of the building stock</a>.</p>

<p>To get a better overview of the building stock through time we can graph the number of buildings by age. We look at buildings, not units. So a stratified
building with 100 units would still count as one building in our graph. And it is not looking at how many buildings were
built in each year, but how many buildings that were built in a given year are still standing today.</p>

<p>We still have 7 buildings in Vancouver that were built before 1900 (the earliest from 1800). Skipping these we graph the
rest to get:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_age" style="height:200px;max-width:640px;" data-url="/data/building_age.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of Buildings by Building Age <span style="float:right;margin-right:10px;" id="building_age_value"></span></p>
</div>
</div>


<p>Starting with 1950 the distribution of buildings by age is quite uniform, with a short peak around the early 1990s.</p>

<p>The dip at the end is due to some lag in new buildings showing up in the property dataset. Looking at the more recent history
it is safe to assume that the number of buildings still standing corresponds well to the buildings of units built in that year.
So the pace of new buildings right now seems to fit in quite well with the recent history and is a little lower than the
peak in the early 1990s.</p>

<h3>Recent Building stock (and recent teardowns) spatial distribution</h3>

<p><a href="https://mountainmath.ca/map/special/15"><img  src="http://doodles.mountainmath.ca/images/rebuilds.png" style="width:50%;float:right"></a>
The next question is to focus on the spatial distribution of recent redevelopment by filtering out older buildings.
Being too lazy to add a bush for dynamic selection of time ranges I just made a static (in time) view only showing the
<a href="https://mountainmath.ca/map/special/15?zoom=13">6883 properties built after 2006</a>. It is quite safe to assume that most
of those new buildings replaced older ones that were torn down. So this map of new buildings is also a map of locations
of buildings that were torn down in the last 10 years.</p>

<p>What&rsquo;s interesting is when selecting
<a href="https://mountainmath.ca/map/special/15?zoom=16&amp;lat=49.2341&amp;lng=-123.1824&amp;layer=4">relative building value view</a> that
there are some properties that have been recently re-developed with increadibly low building value, like the property
at 5649 Dunbar St. <strike>This gives a window into some of the imperfections of the BC Assessment process where the building
value after re-development is not properly reflected in their dataset.</strike> In this case it seems to be
<a href="https://www.google.ca/maps/@49.2351182,-123.1853038,3a,75y,264.36h,81.1t/data=!3m6!1e1!3m4!1s_weW4bEWpmcKGA-Ppk0d_Q!2e0!7i13312!8i6656">a property whose only &ldquo;improvement&rdquo; seems to be the pavement on it</a>.</p>

<p>It also shows that recent building (or teardown) activity is fairly uniform across the city, with only some areas standing out as having little
development like the West End, parts of Kitsilano and Strathcona.</p>

<h3>What gets torn down and rebuilt next?</h3>

<p>The big question is of course where new buildings get built next. In a built up space like Vancouver there are few sites
left where building a new building does not mean tearing down an old one. So another way to ask that question is: What
gets torn down next?</p>

<h3>Teardown Probability</h3>

<p>Predicting which building will get torn down next is of course impossible. So what we try to do is assign a &ldquo;teardown
probability&rdquo; to each building.</p>

<p>Let&rsquo;s first try to understand why a particular building might get torn down as opposed to the one next door. Typically
buildings get torn down at the time when they change ownership. So if a building is not sold, it is far less likely to
get torn down. So what makes a building more likely to get torn down when it is sold? One hypothesis would be that the
value of the building relative to the land should play an important factor. Let&rsquo;s test this hypothesis using the data.</p>

<p>We take the 2006 tax dataset as a baseline and check how many of the buildings have been torn down by 2016. Refer to the
Methodology and Data section at the botton for the messy details. We only
count buildings, so we count a strata lot with 100 units in the same building as one building. Then we use the 2016 dataset
to check how many of them are still around, identifying them by their tax coordinate and again asking they be marked as
being built no later than 2006.</p>

<p>These criteria capture well what we are looking for, but they are not perfect. As a predictive variable we use the</p>

<div style="padding:5px;border: 1px solid grey;border-radius:4px;width:80%;margin:0 auto;">
<h5 style="text-align:center;">Teardown Coefficient</h5>
The *teardown coefficient* is the percentage of the total assessed value that is attributed to the building. More formally
it&#8217;s the ratio of the building value by the sum of the building and land values.
</div>


<p>So we sort the properties by their <em>teardown coefficient</em> using the
2006 tax assessment data and we check how each group fares.</p>

<p>First up a graph of the distribution of buildigs in 2006 by their <em>teardown coefficient</em>.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_buildings_by_teardown" style="height:200px;max-width:640px;" data-url="/data/buildings_by_teardown.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of Buildings by *teardown coefficient* <span style="float:right;margin-right:10px;" id="buildings_by_teardown_value"></span></p>
</div>
</div>


<p>Next up the number of buildings in each category that got torn down and rebuilt by 2016:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_teardowns" style="height:200px;max-width:640px;" data-url="/data/teardowns.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of Torn Down Buildings by *teardown coefficient* <span style="float:right;margin-right:10px;" id="teardowns_value"></span></p>
</div>
</div>


<p>We see that our initial hypothesis seems to hold up quite well. The number of buildings that got torn down and rebuilt
decreases as the <em>teardown coefficient</em> increases. Remember that we defined the <em>teardown coefficient</em> to be the percentage
of the building value out of the total value of the property.</p>

<p>Refer to the methodology and data section for further information on how these numbers were extracted.</p>

<p>To explore this further let&rsquo;s graph the frequency with which a building in a given <em>teardown coefficeint</em> range gets torn down.
To keep things cleaner where we only plot up to a <em>teardown coefficient</em> of 50%:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_teardown_probability" style="height:200px;max-width:640px;" data-url="/data/teardown_probability.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Probability of Building being torn down<span style="float:right;margin-right:10px;" id="teardown_probability_value"></span></p>
</div>
</div>


<p>We see that the <em>teardown coefficient</em> has high predictive value for a building to be torn down and
being rebuilt in the following 10 years. Buildings with a <em>teardown coefficient</em> below 5% have about an 18% chance, and the
probability declines exponentially down to zero at a <em>teardown coefficient</em> of about 50%.</p>

<p>If we were more serious about this
we would fit and exponential curve to the data and compute how well it fits the data, repeat the computation for
other time frames, run it on individual neighbourhoods and maybe also on <a href="https://data.surrey.ca">data from other municipalities</a>
to properly validate our model. We could also refine the model by refining our filters, see the methodology and data section for
more details.</p>

<p>And we could add other factors that likely effect the teardown probability, like building age, proximity to arterials and
others. Of course these are not independent factors, so this kind of analysis requires more time.</p>

<h3>Predicting Teardowns</h3>

<p>Now to the main part: Predicting teardowns. How many buildings will get torn down and rebuilt in the next 10 years? Let&rsquo;s
use what we have just learned to extrapolate.</p>

<p>First up the graph of the 2016 building stock by <em>teardown coefficient</em>:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_buildings_by_teardown_2" style="height:200px;max-width:640px;" data-url="/data/buildings_by_teardown_2.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> 2016 Building stock by *teardown coefficient* <span style="float:right;margin-right:10px;" id="buildings_by_teardown_2_value"></span></p>
</div>
</div>


<p>To estimate how many buildings will get torn down and rebuilt in each category we simply multiply each bin with the teardown probability
from the frequency graph above:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_teardowns_2" style="height:200px;max-width:640px;" data-url="/data/teardowns_2.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Estimate of Buildings rebuilt by 2026 <span style="float:right;margin-right:10px;" id="teardowns_2_value"></span></p>
</div>
</div>


<p>Bottom line, we predict around 8,000 buildings to be torn down and rebuilt by 2026. That&rsquo;s significantly more than the
around 5,900 buildings that we identified as going through this process during the prior 10 years.</p>

<h3>Open Question</h3>

<p>There are lots of assumptions that went into this estimate. While we are confident in our analysis
that properties with low <em>teardown coefficient</em> are the ones most likely to be torn down, it is less clear if the number
of properties being torn down grows linearly as the properties with low <em>teardown coefficient</em> grow. In our case the number
of properties with <em>teardown coefficient</em> below 5% grew from 20492 (21% of the 2006 stock) to 32509 (33.5% of the 2016) stock,
which may be out of the range where our simplistic extrapolation holds. One could try to understand this by carefully
analyzing all available tax years, and not just the two extremes of the available spectrum.</p>

<h3>Mapping Teardowns</h3>

<p><a href="https://mountainmath.ca/map/special/17"><img  src="http://doodles.mountainmath.ca/images/teardowns.png" style="width:50%;float:right"></a>Now that we
understand how to assign a teardown probability to buildings, let&rsquo;s map them! To keep things as simple as
possible let&rsquo;s focus in on the homes with a <em>teardown coefficient</em> below 5%. They make up the bulk in our prediction and
have the simple interpretation that a little more than 1 in 6 of these will get replaced by something else by 2026. So
<a href="https://mountainmath.ca/map/special/17">here is the interactive map</a> of just these 31301 buildings, where we have
filtered out some parks, marinas and rail lines. And this only accounts
for the 5,700 buildings predicted to be torn down with a <em>teardown coefficient</em> below 5% cutoff and neglects the roughly 2,000 more that
are predicted to be torn down that have a <em>teardown coefficient</em> above 5%.</p>

<h3>Methodology and Data</h3>

<p>Only for people who love getting their hands dirty or who want to reproduce or expand on the analysis.</p>

<p>First thing to note is that there is no way to detect &ldquo;teardowns&rdquo; in the dataset, the only way is to look at what has been
rebuilt and what has &lsquo;dropped off&rsquo;. To be more precise, there data fields to look at is the &ldquo;land coordinate&rdquo;, which links
a taxable property to a physical structure, and the &ldquo;year built&rdquo;. And both fields have problems.</p>

<p>The &ldquo;land coordinate&rdquo;
gets de-commissioned and re-assigned during certain re-develpments. And the city dataset provides no way to link the
old one to the new one. One way to do that is through the polygons that mark the property boundaries, that would allow
tracking of complex re-assemblies of land. But the city does not publish historic records of property polygons.</p>

<p>The &ldquo;year built&rdquo; also has lots of issues. Sometimes it is blank even though it records the value of the building as
greater than zero. Sometimes the &ldquo;year built&rdquo; will be set to a date later than the date of the dataset, for example the
2006 tax dataset has buildings with &ldquo;year built&rdquo; all they way up to 2013.</p>

<p>Then comes the issue of filtering. We decided to filter out parks, rail lines and marinas without structures on them. The
algorithm is somewhat simplistic, it&rsquo;s the same one that was used to filter properties for the maps. Additionally we
filter out properties from the heritage dataset. There is definitely
room for improvement here, but without a clear question of what exactly to measure
(only single family homes, or also condos or apartements, treat commercial separately, &hellip;)
it does not make much sense to invest energy into this. After all, this is just looking for a rough model.</p>

<p>So how do we detect rebuilds? We take the land coordinates from properties identified as park or heritage and sieve through
the 2006 tax data to retrieve all records that don&rsquo;t match these land coordinates and have a &ldquo;year built&rdquo; column set
as 2006 or earlier or don&rsquo;t have a &ldquo;year built&rdquo; set at all but change from zero to non-zero building value from 2006 to 2016.</p>

<p>Pretty messy. We mapped <a href="https://mountainmath.ca/map/special/15">about 6,900 buildings were built after 2006</a>,
but only traced 5,869 buildings in the 2006 tax dataset as
being torn down and rebuilt. That difference is largely explained by different selection criteria. The map only considers
properties with a &ldquo;year built&rdquo; field set, but for the analysis we also added properties that don&rsquo;t have that field set
but go from zero building value in 2006 to non-zero building value in 2016 which gets us to 7,784 &ldquo;rebuilds&rdquo;. On the other hand
in the analysis we don&rsquo;t consder the roughly 140 heritage buildings that would pass our filter of being built after 2006, and
the 2016 tax dataset has 2,422 more buildings than the 2006 dataset, some of which can be seen
<a href="https://mountainmath.ca/map/special/13">on this map</a> and are due to subdivisions being split off of the original
property.</p>

<p>Anyway, if you want to get your hand dirty on this shoot me a message and I will hook you up with my scripts.</p>

<script>


function bar_graph(div,shiftAxis,domainFormatter,rangeFormatter,domainLabelFormatter){
    if (!domainFormatter) domainFormatter=d3.format("d")
    if (!rangeFormatter)
     rangeFormatter = function (y) {
        return y;
     };
     if (!domainLabelFormatter) domainLabelFormatter=domainFormatter;

var margin = {top: 20, right: 20, bottom: 40, left: 70},
    width = parseInt(div.style("width")) - margin.left - margin.right,
    height = parseInt(div.style("height")) - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);


var xAxis = d3.svg.axis()
    .scale(x)
    .tickFormat(domainFormatter)
    .orient("bottom");


var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .tickFormat(rangeFormatter)
    .ticks(5, rangeFormatter);

var svg = div.append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var data_url=div[0][0].dataset.url;

d3.json(data_url, function(error, json) {
  if (error) throw error;
  var graphData=json[0];
  var data=graphData.data;
  x.domain(data.map(function(d) { return d.date }));
  y.domain([0, d3.max(data, function(d) { return d.count; })]);
  
  var domainTickValues=[];
  var skip=Math.round(40/x.rangeBand());
  if (skip<=0) skip=1;
  for (var i=0;i<x.domain().length;i++) {
    if (i % skip==0) domainTickValues.push(x.domain()[i]);
  }
  if (x.domain().length % 5 !=0) domainTickValues.push(x.domain()[x.domain().length-1]);
  xAxis.tickValues(domainTickValues);

  var xShift=shiftAxis ?  x.rangeBand()/2.0 * 1.1 : 0;
  
  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(" + xShift + "," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis);
//    .append("text")
//      .attr("transform", "rotate(-90)")
//      .attr("y", 6)
//      .attr("dy", ".71em")
//      .style("text-anchor", "end")
//      .text("Probability");

  svg.selectAll(".bar")
      .data(data)
    .enter().append("rect")
      .attr("class", graphData.class + " bar")
      .attr("fill", graphData.color)
      .attr("x", function(d) { return x(d.date); })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.count); })
      .attr("height", function(d) { return height - y(d.count); })
      .on('mouseover',function(d){
         d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('click',function(d){
       d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('touch',function(d){
         d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});

      
});

}

var percentageFormatter=d3.format(".1%");
var binFormatter=function(d){return percentageFormatter(d-0.025) + ' - ' + percentageFormatter(d);}
//ready_for_graph(d3.select("#graph"));
bar_graph(d3.select("#graph_age"),false);
//ready_for_graph(d3.select("#graph_buildings_by_teardown"),percentageFormatter);
//ready_for_graph(d3.select("#graph_teardowns"),percentageFormatter);
bar_graph(d3.select("#graph_buildings_by_teardown"),true,percentageFormatter,null,binFormatter);
bar_graph(d3.select("#graph_teardowns"),true,percentageFormatter,null,binFormatter);
bar_graph(d3.select("#graph_teardown_probability"),true,percentageFormatter,percentageFormatter,binFormatter);
bar_graph(d3.select("#graph_buildings_by_teardown_2"),true,percentageFormatter,null,binFormatter);
bar_graph(d3.select("#graph_teardowns_2"),true,percentageFormatter,null,binFormatter);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updated Vancouver Assessment Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/"/>
    <updated>2016-01-17T11:34:09-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/assessment?layer=4"><img  src="http://doodles.mountainmath.ca/images/relative_land.jpg" style="width:50%;float:right"></a>
The friendly folks at
<a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data</a> just
<a href="https://twitter.com/VanOpenData/status/688060388190097408">updated their property assessment data</a> with the fresh 2016
property tax assessments. Time to run the script to update the <a href="https://mountainmath.ca/map/assessment">Vancouver Assessment Map</a>
with the new data. And for good measure I pasted over some of the thematic map engine from <a href="https://censusmapper.ca">CensusMapper</a>
to improve the mapping performance.</p>

<h4>Maps</h4>

<!-- more -->


<p><img  src="http://doodles.mountainmath.ca/images/map_menu.png" style="width:25%;margin-left:5px;float:right">The <a href="https://mountainmath.ca/map/assessment">interactive assessment map</a> offers several views. In the panel on the top
right we can select how to view the data. It offers standard thematic maps for value change, total value, building value
and building age and zoning. And there are some options that warrant more explanation:</p>

<ul>
<li><a href="https://mountainmath.ca/map/assessment?layer=5"><em>Relative Land Value</em>:</a> The colours on the map show each property by the
land value per m&sup2;. We can immediately spot the east-west land gradient, as well as how zoning affects land value.
When zooming in we also see the effect of lot size on land value.</li>
<li><a href="https://mountainmath.ca/map/assessment?layer=10"><em>Tax Density</em>:</a> This map looks at the tax dollars collected
by the city by area. It tells us the relative rate at what each property is contributing to city services. We can again
observe the impact of exclusionary zoning.</li>
<li><a href="https://mountainmath.ca/map/assessment?layer=4"><em>Relative Value of Building</em>:</a>
<a href="https://mountainmath.ca/map/assessment?layer=4"><img  src="http://doodles.mountainmath.ca/images/teardown.jpg" style="width:50%;float:right"></a>
This map simply divides the building value
by the total property value. There are many ways to interpret this map, my favourite is to use this as a &ldquo;Teardown Predictor&rdquo;.
Essentially, as the percentage of the building value approaches zero the probability that it will get torn down in the
near future increases. Imagine someone spending $1.5m to buy a property with a house valued at $37k. Many people don&rsquo;t
mind living in a house worth $37k, but someone who is spending $1.5m would probably prefer to buy a different property
with a higher quality house. Or spend more money to upgrade the house. How will it be upgraded? Renovating is in most
cases economically unsound, most people will choose to tear down and rebuild. In fact, the <em>teardown threshold</em> is
likely higher than the 2.5% in the example given. The percentage of properties in Vancouver where the building value is
less than 2.5% of the total value has slightly decreaed in the last year from 17.9% to 17.8%, but the percentage of properties
with building value less than 5% of the total value has increased from 32% to 33.5% during the last year.</li>
</ul>


<h4>The Data</h4>

<p>The data originates with BC Assessment, which estimates land and building values of each property based on recent sales
of comparable properties. The assessment was done in summer 2015 and is based on sales before that, so at this point in
time the data lags the market by about one year. Values for individual properties may well be off, depending how well
renovations and improvements were reported and how well the BC Assessment estimates work for the given property. On
average they should reflect the market about half a year to one year ago.</p>

<p>Sadly, BC Assessment does not give out their data with a license that would allow mapping it the way I do, so we have
to rely on municipalities to release it through their open data portals. The format of the data from each municipality is
different, so lazy me is only importing data from City of Vancouver, although some other nearby municipalities are also releasing
there data.</p>

<p>The motivation behind the map was to understand the building stock. Some effort was made to filter out parks, but the
algorithm is far from perfect and will often includes parks that host building
structures, as well as marinas with structures on them.</p>

<p>The new city dataset does not include the 2016 tax levy, so we still only show the 2015 tax levies until CoV updated their dataset.</p>

<h4>History</h4>

<p>Here is a quick history of the overall land and building values aggregated for Vancouver between 2006 and 2016.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph" style="height:200px;max-width:640px;" data-lines="/data/vancouver_stats.json"></div>
<div class="legend no-margin">
  <p><i style="background:blue"></i> Land Value <span style="float:right;margin-right:10px;" id="land_value"></span></p>
  <p><i style="background:green"></i> Building Value <span style="float:right;margin-right:10px;" id="building_value"></span></p>
</div>
</div>


<p>When looking at all properties in the city, the increase in land
value year over year was 21.4% ($45.2bn), while overall building values increased by 7.3% ($5bn). Hover, click or touch
the points in the graph to get the values for the corresponding year.</p>

<h4>Neighbourhoods</h4>

<p>Lastly a quick overview over the neighbourhoods. Land and building values have not increased evenly throughout in the year.
I aggregated all tax data by neighbourhood and split it into land value and building value increases.
These numbers should be used as guidance only, they mix lots of different types of properties and include parks.</p>

<p>Here is the breakdown by neighbourhood:</p>

<ul>
<li>Renfrew-Collingwood: Land: 30.6% ($1.2bn), Building: 5.8% ($46.0m)</li>
<li>Sunset: Land: 26.6% ($1.6bn), Building: 5.9% ($74.2m)</li>
<li>Oakridge: Land: 17.6% ($1.2bn), Building: 12.6% ($207.4m)</li>
<li>Downtown: Land: 16.9% ($0.8bn), Building: 3.2% ($91.2m)</li>
<li>Kerrisdale: Land: 18.7% ($1.3bn), Building: 3.3% ($54.1m)</li>
<li>Victoria-Fraserview: Land: 28.0% ($1.5bn), Building: 5.6% ($65.2m)</li>
<li>Grandview-Woodland: Land: 24.9% ($1.7bn), Building: 0.7% ($17.4m)</li>
<li>West End: Land: 22.6% ($2.5bn), Building: 3.5% ($188.4m)</li>
<li>Hastings-Sunrise: Land: 25.1% ($1.1bn), Building: 4.9% ($63.8m)</li>
<li>Killarney: Land: 17.8% ($0.3bn), Building: 14.1% ($241.6m)</li>
<li>Marpole: Land: 17.8% ($0.3bn), Building: 14.1% ($241.6m)</li>
<li>Kitsilano: Land: 21.6% ($3.3bn), Building: 4.3% ($134.5m)</li>
<li>Shaughnessy: Land: 18.6% ($1.7bn), Building: 7.4% ($117.7m)</li>
<li>West Point Grey: Land: 20.2% ($2.3bn), Building: 5.2% ($88.3m)</li>
<li>Fairview: Land: 19.4% ($2.0bn), Building: -1.0% (-$52.4m)</li>
<li>Downtown Eastside: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)</li>
<li>Kensington-Cedar Cottage: Land: 24.9% ($1.7bn), Building: 0.7% ($17.4m)</li>
<li>Riley Park: Land: 26.4% ($1.0bn), Building: 4.0% ($40.2m)</li>
<li>Mount Pleasant: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)</li>
<li>South Cambie: Land: 20.9% ($1.5bn), Building: 21.4% ($268.1m)</li>
<li>Strathcona: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)</li>
<li>Dunbar Southlands: Land: 21.8% ($1.0bn), Building: 11.2% ($126.6m)</li>
<li>Arbutus Ridge: Land: 21.4% ($1.6bn), Building: 1.1% ($17.3m)</li>
</ul>


<p>It becomes immediately clear that the increase in property values is mostly driven by land, note that total value increases
for land and buildings are reported in billions and millions, respectively. The building stock does not
have time to catch up, with the exception of South Cambie. Fairview stands out with declining overall building values.</p>

<script>
var ready_for_graph = function() {
    var d3lines=[];
    var padding = {top: 20, right: 20, bottom: 30, left: 90};
    var prevChartWidth = 0, prevChartHeight = 0;
    var updateTransistionMS = 750; // milliseconds
    var sourceData, lineData, xScale, yScale, line;

    var symbol;
    var prefix;
    var numberFormatter = function (y) {
        return '$' + Math.round(prefix.scale(y*10))/10.0 + symbol;
    };

    var graphs=d3.select("#graph");
    var div=graphs[0][0];
    if (div==null|| div.childElementCount!=0) {return;}
    var data_url=div.dataset.url;

    // create svg and g to contain the chart contents
    var baseSvg = graphs.append("svg");
    var chartSvg=baseSvg
        .append("g")
        .attr("class", "chartContainer")
        .attr("transform", "translate(" + padding.left + "," + padding.top + ")");

    // create the x axis container
    chartSvg.append("g")
        .attr("class", "x axis");

    // create the y axis container
    chartSvg.append("g")
        .attr("class", "y axis");
    var line;
    var largest=null;
    var lineData;
    if (div.dataset.lines) {
        d3.json(div.dataset.lines,function(error,json){
        lineData=json;
        var domain=[null,null];
        var range=[null,null];
        for (var i=0;i<lineData.length;i++){
            lineData[i].data.forEach(function(d) {
                d.date = +d.date;
                d.count = +d.count;
                if (domain[0]==null || domain[0]> d.date) domain[0]= d.date;
                if (domain[1]==null || domain[1]< d.date) domain[1]= d.date;
                if (range[0]==null || range[0]> d.count) range[0]= d.count;
                if (range[1]==null || range[1]< d.count) range[1]= d.count;
            });
        }
        xScale=d3.scale.linear().domain(domain);
        var toAdd=(range[1]-range[0])/10;
        range[0]-=toAdd;
        range[1]+=toAdd;
        yScale=d3.scale.linear()
            .domain(range);

        line = d3.svg.line()
            .x(function(d) { return xScale(d.date); })
            .y(function(d) { return yScale(d.count); })
            .interpolate("linear");

        xAxis = d3.svg.axis()
            .scale(xScale)
            .orient("bottom")
            .tickFormat(d3.format("d"))
            .tickValues(domain);

        yAxis = d3.svg.axis()
            .scale(yScale)
            .orient("left")
            .tickFormat(numberFormatter)
            .ticks(5);

        prefix = d3.formatPrefix(range[1]);
        if (prefix.symbol=='K') {
            symbol='k'
        } else if (prefix.symbol=='M') {
                symbol='m'
        } else if (prefix.symbol=='G') {
            symbol='bn'
        } else if (prefix.symbol=='T') {
            symbol='tn'
        }
        updateChart(true);
        });

    }


    function updateChart(init)
    {
        // get the height and width subtracting the padding
//    var innerHeight = window.innerHeight - 20;
        var innerWidth = window.innerWidth - 20;
        var divWidth=$(div).width();
        if (divWidth==0) divWidth=$(div.parentElement.parentElement).width();
        var maxWidth=parseInt($(div).css('max-width'));
        if (divWidth==0) divWidth=innerWidth*0.8;
        if (divWidth>maxWidth) divWidth=maxWidth;
        var chartWidth = divWidth-padding.left-padding.right;//960 - margin.left - margin.right,
        var chartHeight = $(div).height()-padding.top-padding.bottom;//500 - margin.top - margin.bottom;


        // only update if chart size has changed
        if ((prevChartWidth != chartWidth) ||
            (prevChartHeight != chartHeight)) {
            prevChartWidth = chartWidth;
            prevChartHeight = chartHeight;

            //set the width and height of the SVG element
            chartSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);
            baseSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);

            // ranges are based on the width and height available so reset
            xScale.range([0, chartWidth]);
            yScale.range([chartHeight, 0]);

            if (init) {
                // if first run then just display axis with no transition
                chartSvg.select(".x")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                chartSvg.select(".y")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .call(yAxis);
            }
            else {
                // for subsequent updates use a transistion to animate the axis to the new position
                var t = chartSvg.transition().duration(updateTransistionMS);

                t.select(".x")
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                t.select(".y")
                    .call(yAxis);
            }

            for (var i = 0; i < lineData.length; i++) {
                var sourceData=lineData[i].data;
                var color=lineData[i].color;
                var label=lineData[i].label;
                var className=lineData[i].class;

                // bind up the data to the line
                var lines = chartSvg.selectAll("path.line."+className)
                    .data([lineData[i].data]); // needs to be an array (size of 1 for our data) of arrays

                // transistion to new position if already exists
                lines.transition()
                    .duration(updateTransistionMS)
                    .attr("d", line);

                // add line if not already existing
                lines.enter().append("path")
                    .attr("class", "line")
                    .attr("stroke", color)
                    .attr("stroke-width", 2)
                    .attr('fill','none')
                    .attr("d", line);

                // bind up the data to an array of circles
                var circle = chartSvg.selectAll("circle."+className)
                    .data(sourceData);

                // if already existing then transistion each circle to its new position
                circle.transition()
                    .duration(updateTransistionMS)
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", function (d) {
                        return yScale(d.count);
                    });

                // if new circle then just display
                circle.enter().append("circle")
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", function (d) {
                        return yScale(d.count);
                    })
                    .attr("r", 4)
                    .attr('fill', 'transparent')
                    .style("stroke", color)
                    .style("stroke-width", 8)
                    .attr("class", className)
                    .on('mouseover',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + numberFormatter(d.count)) 
                    }).on('click',function(d){
                     d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + numberFormatter(d.count)) 
                    }).on('touch',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + numberFormatter(d.count)) 
                    }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});

            }
        }
    }

    // look for resize but use timer to only call the update script when a resize stops
    var resizeTimer;
    window.onresize = function(event) {
        clearTimeout(resizeTimer);
        resizeTimer = setTimeout(function()
        {
            updateChart(false);
        }, 100);
    }


};
ready_for_graph();
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Canvas vs SVG]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/12/27/canvas-vs-svg/"/>
    <updated>2015-12-27T15:53:02-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/12/27/canvas-vs-svg</id>
    <content type="html"><![CDATA[<h4>CensusMapper Behind the Scenes</h4>

<p>The idea behind <a href="https://censusmapper.ca">CensusMapper</a> is that it takes away all the technical barriers to dealing with census data. So how does
CensusMapper work behind the scenes?</p>

<h4>CensusMapper Data Workflow</h4>

<!-- more -->


<p>The general setup is quite simple. We use the lean javascript open mapping platform <a href="http://leafletjs.com">leaflet</a>
as the base for mapping information. Leaflet handles the logic of dealing with zoom and pan and keeping track of the
geographic boundaries that should be mapped. That information gets then passed on to the CensusMapper servers.</p>

<p>CensusMapper will then send the appropriate census geographic polygons to the browser for leaflet to display. Once the
geographic data is available for mapping, some custom code checks what kind of information the user wants to display and
requests the census data required to make the map. The census information is then assembled on the server, sent down
and attached to the polygons and drawn
on the browser window within leaflet. This two-tier process allows the highly dynamic mapping in CensusMapper where the
data-heavy geographic polygons are kept separately thus can be cached and re-used.</p>

<h4>Drawing Census Data</h4>

<p>There are a number of ways how we can display census data in the browser. At CensusMapper we have played with three
different technologies to map data that vary in performance and browser support. They all have in common that they
won&rsquo;t run on Internet Explorer 8 or earlier, but we have just about reached the point in time where it is acceptable to
ignore IE8- in products meant for the &ldquo;general internet audience&rdquo;.</p>

<h5>SVG</h5>

<p><a href="https://en.wikipedia.org/wiki/Scalable_Vector_Graphics">SVG</a> is what our maps have been using so far. SVGs are fairly high level, which means it&rsquo;s very little work
to implement and map information. One simply passes a polygon to the browser, tells it how to color it, and the browser
takes care of the rest. SVG elements can easily be styled via CSS, so there is essentially no work involved to deal with
highlight on hover, scaling for retina displays, patterns for census data quality flags, etc. We use
<a href="https://github.com/mbostock/d3">d3.js</a> to attach the geographic and census data right to the SVG elements for easy
manipulation.</p>

<p>While mapping data this way is very easy, for CensusMapper there are two problems.</p>

<ol>
<li>We are restricted in how we can display information by the capabilities of SVG.</li>
<li>SVG rendering is done by the browser, and not all browsers are equal. Most importantly, SVG rendering in Internet
Explorer is excruciatingly slow. So slow, that CensusMapper becomes essentially useless within Internet Explorer. We
felt compelled to add a warning messeage that displayed when people opened CensusMapper with Internet Explorer. And
when you do that, that&rsquo;s a sure sign that your app has a serious problem.
<img src="/images/chrome.png" alt="Browser Warning" />
So how to get around those issues? Enter Canvas.</li>
</ol>


<h5>Canvas</h5>

<p><a href="https://en.wikipedia.org/wiki/Canvas_element">Canvas</a> offers a way to draw images in a browser. Unlike SVG, the drawing has to be done &ldquo;by hand&rdquo;. And the result is just
an image, with no clear way to tell where it came from. There is no way to attach any information to individual
structures drawn on a canvas. All the logic for highlight on hover, figuring out what data is associated with the mouse
position, dealing with retina displays, etc. needs to be added by hand.</p>

<p>On the upside, a good canvas implementation is a lot faster than SVG. And it opens the door to changes in how the data
is handled that bring additional performance improvements. In particular, we can now chop up census polygons and render
the pieces separately, greatly cutting down on the size of the downloaded data, as well as the complexity of the
polygons that get rendered. And the performance improvements are noticeable across all browsers and platforms.</p>

<p>At the end of the day it is actually not that much work and we flipped the switch on this just before the Christmas
break. CensusMapper is now running using canvas instead SVG for
the main maps. We
kept the look and feel the same, so unless you dig into the code you won&rsquo;t notice the difference.
Some parts of CensusMapper still utilized SVGs, like the d3-based
<a href="http://doodles.mountainmath.ca/blog/2015/10/24/census-drilldown/">Census Wheel</a>.</p>

<h5>WebGL</h5>

<p><a href="https://en.wikipedia.org/wiki/WebGL">WebGL</a> also draws on a canvas element, but the work is offloaded onto the GPU (graphics processor) giving enormous
speed improvements. Regular canvas rendering is fast enough for our purposes, but with WebGL we can do more complex
renderings that previously we could not even dream of: <a href="https://en.wikipedia.org/wiki/OpenGL_Shading_Language">Shaders</a>
and Interactive 3D data maps. We had previously
<a href="https://mountainmath.ca/census3">toyed with 3D data visualization</a> to explore Vancouver&rsquo;s household density in 3D using Three.js,
but did not pursue this further because of the complexities of writing code for navigating a Canada-wide map. Then we came
across the super-customizable 3D open mapping platform built by <a href="https://mapzen.com/projects/tangram/">Mapzen</a>, and that
suddenly made it extremely easy to do interactive 3D data mapping live in the browser. A quick test
<a href="https://mountainmath.ca/vancouver_lidar/map">using Vancouver&rsquo;s open LIDAR generated building height data</a> showed how easy Mapzen&rsquo;s
tangram engine is to use.</p>

<p>After digging deeper into tangram, and with help from the friendly people at Mapzen, we figured out a way to fit
CensusMapper&rsquo;s two-stage data workflow into tangram&rsquo;s mapping engine. The result are real-time 3D maps where height
and color of each geographic area can be independently (and dynamically) controlled. Here is an example where mouseover
trigged the area west of Coal Harbor to &lsquo;pop up&rsquo;.
<img src="/images/webGL.jpg" alt="webGL" /></p>

<p>At the same time we gain the ability to easily pull in all kinds of other data and map it. On our canvas or svg maps we
added regular image tiles, either a road and label&rsquo;s overlay or a base map (which then requires opacity to be added to
the census data that is mapped on top of that) as orientation aid. Short of baking our own image tiles we have very
little control over the look and feel of this. With Mapzen&rsquo;s tangram we can very easily pick and style individual
geographic objects from Mapzen&rsquo;s OSM vector tile server, resulting in crisp and clear maps. In the above example we
decide dynamically what level of roads to render, how to style them, what labels to display and we also added bodies of
water, where we filter by size depending on the zoom level.</p>

<p>At this stage it is still an ongoing project to get this production-ready. One obvious obstacle is that WebGL browser
support is still lagging. And on top of that it also requires updated graphics card drivers, which is a big problem on
windows machines that are already a couple of years old. So for now we still need to have a plain canvas or svg fallback.</p>

<p>And then there are the details that need to get worked out. 3D maps sounds great, but it will take us some time to figure
out how to best utilize this in thematic maps. But even without utilizing 3D capabilities, the dynamic shaders and increased
rendering performance are already pushing the boundary of what&rsquo;s possible in web maps.</p>
]]></content>
  </entry>
  
</feed>
