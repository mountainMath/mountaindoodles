<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: open data | Mountain Doodles]]></title>
  <link href="http://doodles.mountainmath.ca/blog/categories/open-data/feed.atom" rel="self"/>
  <link href="http://doodles.mountainmath.ca/"/>
  <updated>2017-08-25T22:11:03-07:00</updated>
  <id>http://doodles.mountainmath.ca/</id>
  <author>
    <name><![CDATA[MountainMath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[dot-density]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/08/24/dot-density/"/>
    <updated>2017-08-24T22:28:40-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/08/24/dot-density</id>
    <content type="html"><![CDATA[<p>I started writing this blog post in December 2015, when CensusMapper quite a bit younger and
I hacked together some basic dot-density maps. I never much liked the results and have been
slowly improving and thinking about them. I am still not entirely happy with the current
implementation, but it is slowly getting there. The final impulse to finsish this post was
the work on <code>cancensus</code>, and R wrapper for the CensusMapper API my explorations in multi-category
dot density maps in R, now tied up into the new <code>dotdensity</code> package.</p>

<h4>One person at a time</h4>

<p><img  src="http://doodles.mountainmath.ca/images/recent_immigrants_cropped.png" style="width:50%;float:right;margin-left:10px;">
Dot-density maps look pretty cool.
They have been <a href="https://www.google.com/search?q=dot-density+map+images&amp;client=safari&amp;rls=en&amp;tbm=isch&amp;tbo=u&amp;source=univ&amp;sa=X&amp;ved=0ahUKEwjuy9Ch6PjJAhUivXIKHWcvDb4QsAQIHA&amp;biw=1253&amp;bih=812">flying around the itnernet</a>
lately, so we have been thinking about how to offer them in CensusMapper.</p>

<p>So what&rsquo;s so great about dot-density maps? Essentially two things:</p>

<ol>
<li>They are very simple to interpret. One dot = one person is something everyone understands immediately.</li>
<li>They can show several variables at once, for example mapping both male and female cyclist like above or
<a href="http://www.nytimes.com/interactive/2015/07/08/us/census-race-map.html">mapping ethnic segregation</a>.</li>
</ol>


<p>But once one looks closer there are lots of issues that need to be dealt with.</p>

<!-- more -->


<h4>The devil is in the details</h4>

<p>As simple as the the basic dot-density promise of <em>one dot = one person</em> is, it must fundamentally remain a lie for
census maps. We simply don&rsquo;t have the level of detail that the maps suggest. We don&rsquo;t know the location of people with
the accuracy depicted in the maps, and we certainly don&rsquo;t know the categories that give the color with the accuracy
suggested in the maps.</p>

<p>That&rsquo;s the most serious drawback of dot-density maps, they often suggest a level of detail that simply isn&rsquo;t there. Does
that outweigh the advantages? For that it is useful to take a quick look at the alternative that we have been using
at CensusMapper</p>

<h4>Choropleth Maps</h4>

<p>Choropleth maps are the staple of census maps. Every census region gets colored depending on a value of a census
variable (or function derived from census variables). While this is also quite simple, in practive there are a number of
problems with that:</p>

<ol>
<li><p>Low population bias. Let&rsquo;s look at an example to see how this works. Take a
<a href="http://censusmapper.ca/maps/132?zoom=12&amp;lat=49.2462&amp;lng=-123.0761">map of the percentage of children living in poverty</a>.
We immediately see where the percentage of child poverty is high. And child poverty is a problem. But what we don&rsquo;t see
is how many poor children live in each area. So while we see the relative magnitude, we don&rsquo;t see the absolute magnitude
of the problem. So for example the <a href="http://censusmapper.ca/maps/132?zoom=17&amp;lat=49.2647&amp;lng=-123.1429">DA north-west of Broadway and Fir</a>
sports 66.7% of children living in poverty, but there are only 30 children in the area. The
<a href="http://censusmapper.ca/maps/132?zoom=17&amp;lat=49.2435&amp;lng=-123.1516">DA north-east of Arbutus and 33rd</a> has a similar rate
of 69.1%, but there are 345 children in that area. So when trying to understand child poverty in the west side of Vancouver
one should focus on the latter not the former, but on the map they appear identical. Dot-density maps do a much better
job at representing this properly as they would simply draw a dot for each child in poverty, contrasted by a dot for
each child not in poverty as <a href="http://censusmapper.ca/maps/216?zoom=13&amp;lat=49.2465&amp;lng=-123.1425">you can see here</a>. Another
way to deal with this issue is via <a href="http://doodles.mountainmath.ca/blog/2017/04/10/surprise/">a surprise map</a> which
we have explained <a href="">in a previous post</a></p></li>
<li><p>Choropleth maps are difficult to understand. If you read this far you probably have dealt with a fair share of maps
and won&rsquo;t appreciate how some people struggle understanding these. But the amount of time I have spent explaining to
journalists what they see in the <a href="http://censusmapper.ca/maps/137">halloween map of trick or treat density</a> is (torturous)
testament of the difficulties people have with these kind of maps. I don&rsquo;t think I would have gotten any questions if I
had simply use [the dot-density version] instead.</p></li>
<li><p>One can only show one variable at a time. There are ways to stretch this a little, for example
<a href="http://andywoodruff.com/blog/value-by-alpha-maps/">value by alpha maps</a> are one way around this that tackle the population
bias. Another way is the <a href="http://censusmapper.ca/maps/162">RGB maps CensusMapper has</a>. But this does not exactly make it
easier to interpret. Dot-density versions of this is certainly <a href="http://censusmapper.ca/maps/202">easier to interpret</a>
plus one is not limited by the three categories of the RGB maps.</p></li>
</ol>


<h4>Dot-Density Maps</h4>

<p>One other big challenge with dot-density maps is that they are surprisingly hard to make. Right now we at CensusMapper
just have the bare minimum in place to produce these kind of maps: A way to randomly place the required number of dots
into each geographic region colored by the given categories. Just when I was about to try myself at dot-density maps I
saw a <a href="https://twitter.com/pwramsey/status/677502052210085888">helpful tweet</a> telling me exactly what to pay attention
to, so the implementation was quick and painless. &ndash; Well, not quite, I still had to deal with issues due to polygons
clipped server side and the fact that census areas are often multi-polygons.</p>

<p> Of course I had to try this out on CensusMapper. The current implementation suffers from a number of (minor) issues though.</p>

<h4>CensusMapper Dot-Density Issues</h4>

<p><a href="https://censusmapper.ca/maps/797#12/49.2430/-123.0103"><img  src="http://doodles.mountainmath.ca/images/recent_immigrants_da.png" style="width:50%;float:right;margin-left:10px;"></a>
1. Clipping. There are still some minor issues due to clipping that can lead to the number of dots being off by a small
proportion. I won&rsquo;t bore you with the technical details, but the good news is that it can be worked out at the expense
of adding some more custom code on the client.
2. Dynamic dot-value scaling. CensusMapper maps allow for zooming from country-level down to street level. The one dot
= one person paradigm does not work very well on all scales. Visually as well as computationally. The smallest unit to
draw is one pixel (or 1 quarter of a pixel on 2x retina displays), and at some point (at the
latest when having to draw 33 million dots randomly within different regions in Canada) your browser performance will
tank. To fix this we need to dynamically adjust the value of each dot. Instead of 1 dot = 1 person it will be 1 dot =
10 people at lower zoom levels. And at higher zoom levels at some point one dot will have to start to get larger to be
more visible. Dynamically changing scales can be confusing though. As we zoom we keep the size of each dot relative to
the map constant, but if we re-scale we change the size of each dot relative to the map scale to make it clearer to the user that
wer are rescaling.
<a href="https://censusmapper.ca/maps/797#10/49.2430/-123.0103"><img  src="http://doodles.mountainmath.ca/images/recent_immigrants_ct.png" style="width:50%;float:left;margin-right:10px;"></a>
3. Non-uniform distribution of population. The current code has the problem of placing the dots randomly in each census
geography regardles of where people actually live. This goes back to the fundamental issues that dot-density maps suggest
a level of precision that simply is not there. But it definitely is odd to see dots in the Pacific Spirit Park or
<a href="https://twitter.com/mikeklassen/status/684981658013990913">camping out on Burnaby Mountain</a>.
The good news is that there is a partial fix to this. We have population counts at a finer census geometry:
Census Blocks. And at the Census
Block level we see that nobody lives in the Pacific Spirit Park, or on most of Burnaby Mountain. So to fix this we
simply need to shift the way we decide what census geography to display. This is quite difficult to fix within the
CensusMapper paradigm of highly dynamic maps where nothing is pre-computed.
4. Visual feedback on hover / select. For Choropleth maps we highlight regions on hover so that the user knows what geographic
area the variables in the legend and in the popup are for. This is something that is not too difficult to add, but we
will have to wait for the next bigger CensusMapper map refresh.</p>

<h4>Static dot-density maps</h4>

<p>Statis maps is one way where the above issues don&rsquo;t appear. And most importantly, we can fix issue 3. completely by
taking the time to weight the placement of dots by census block level data. The new
<a href="https://github.com/mountainMath/cancensus"><code>cancensus</code> R package</a> now makes it
super easy to import cenusus data into R, and we wrote a <a href="https://github.com/mountainMath/dotdensity"><code>dotdensity</code> R package</a>
to implement common functions that deal
with the usual pitfalls of multi-category dot-density maps. By moving from CensusMapper to R we trade the dynamic nature
of CensusMapper for crisper images and improved processing and dot-placement. Often we aren&rsquo;t interested in Canada-wide
maps that are the staple of CensusMapper, but only want to focus on one particular region. Or maybe a couple of regions,
and the <code>cancensus</code> and <code>dotdensity</code> packages still make it very easy to change the region and make the same map for a
different geographic region. Or make changes to the variables we want to map. In particular in conjunction with the
<a href="">CensusMapper API helper</a> that reduces the selection of geographies and variables to a couple of mouse clicks and
let&rsquo;s you copy and paste the R code to import the data through <code>cancensus</code>.</p>

<p>The dot-density package has two main functions that we use</p>

<p><code>dot_density.compute_dots</code> takes care of converting geographic shapes with counts for each category into dots. This
is fairly straight-forward, but we need to pay attention to two potential pitfalls.</p>

<ul>
<li>The order of the dots need to be
randomized so we don&rsquo;t draw all items of one particular category (colour) last, so that these end up on top and appear
more prominent than others.</li>
<li>When we scale so that 1 dot represents more than 1 unit in our category count, we need to employ statistical rounding,
not just regular rounding, otherwise the overall count of the dots may not represent the overall averages. To see this,
suppose we want to map German speakers, and we scale so that 1 dot corresponds to 50 German speakers. If German speakers
are uniformly distributed in each area so that there are 20 German speakers in each area, regular rounding will produce a
map without any German speakers at all. Statistical rounding will properly reflect the total number of German speakers,
but they will be randomly placed in each area. Not ideal, we probably should adjust our scale. But better than random
rounding. And adjusting the scale is not always an option, for example there might be one cluster of German speakers and a uniform
distribution everywhere else.</li>
</ul>


<p>The <code>dot_density.compute_dots</code> takes care of these issues under the hood. (And so does CensusMapper.)</p>

<p><code>dot_density.proportional_re_aggregate</code> takes two nested geographies, for example census subdivisions and census tracts. It will
compare counts across the geographic levels and adjust the lower-level geography counts with the more accurate higher-level data.
If lower level geographic data has been suppressed due to quality or privacy concerns the overall counts at that aggregation level
won&rsquo;t accurately reflect the overall data. The <code>dot_density.proportional_re_aggregate</code> will re-distribute the missing counts
proportionately among the lower level geographies, weighted appropriately.</p>

<p>The same function can also be used to weight the dots we want to draw by block level data, so this will produce maps that avoid
placing dots in parks or other unpopulated areas, spacing them according to population density at the block level.</p>

<h4>Examples</h4>

<p>When the data is easily accessible and well-organized, it is incredibly easy to run analysis or visualize it. In CensusMapper,
we can make dot-density maps within a matter of minutes, and with the <code>cancensus</code> and <code>dotdensity</code> packages it&rsquo;s no different in R.</p>

<p>Hear are a couple of examples, first from CensusMapper with links to the live maps, and then from the vignettes embedded into the R package.</p>

<h5>CensusMapper</h5>

<p>Occupied dwelling units by type
<a href="https://censusmapper.ca/maps/727#12/49.2719/-123.0967"><img  src="http://doodles.mountainmath.ca/images/dwellings-dot-density.png"></a></p>

<h5>R</h5>

<p>Language Spoken at Home
<img src="https://github.com/mountainMath/dotdensity/raw/master/images/vancouver-languages.png" alt="Languages" /></p>

<p>Household Size
<img src="https://github.com/mountainMath/dotdensity/raw/master/images/toronto-hh-size.png" alt="Household Size" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Density]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/08/23/density/"/>
    <updated>2017-08-23T10:15:29-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/08/23/density</id>
    <content type="html"><![CDATA[<p>Density in Vancouver has been one of the recurring themese on this blog, and there are many
different ways to come at it. We have <a href="http://doodles.mountainmath.ca/blog/2016/02/29/land-use/">looked at density in terms of land use</a>
to understand how much land is devoted to what purpose in Metro Vancouver and it&rsquo;s municipalities. We have looked
at density in terms of <a href="http://doodles.mountainmath.ca/blog/2016/03/02/property-taxes-and-land-use/">tax density</a> to
understand how property tax revenue depends on land use and zoning. We have looked at
<a href="http://doodles.mountainmath.ca/blog/2016/05/20/density/">density in terms of built floor space ratio</a>.</p>

<p>And of course we have looked at <a href="https://censusmapper.ca/maps/591">population density through CensusMapper</a>,
and this time we want to do a quick variation on that theme.</p>

<!-- more -->


<h2>Population Density</h2>

<p>Recently I have heard renewed chatter about how dense Vancouver is or is not. Before diving deepr into this it is
important to distinguish two types of density that often get mixed up.</p>

<h3>Gross Density</h3>

<p>Gross (population) density simply looks
at the total population divided by (land) area. Ignoring census undercounts, we can simply look at the 2016 census numbers
to compute these.</p>

<h3>Net Density</h3>

<p><em>Net density</em> takes residential land (instead of all land)
as it&rsquo;s base. One measure of net density is the <em>floor space ratio</em> (FSR), which we have
<a href="https://mountainmath.ca/map/assessment?zoom=13&amp;lat=49.25&amp;lng=-123.1182&amp;layer=16&amp;mapBase=2">approximated and mapped in the past</a>
and that also includes commercial space next to residential living space. More details on this are in
<a href="(http://doodles.mountainmath.ca/blog/2016/05/20/density/">this older blog post</a>).</p>

<p>Net density is typically what people refer to in the countext of building developments. Sadly it&rsquo;s hard to
get a hold of good data sources that would allow for meaningful comparisons across the country. The
provincial assessment authorities have that data, but in Canada this data only shared after significant financial
commitments.</p>

<h2>How Dense is Vancouver?</h2>

<p>How does Vancouver stack up against the rest of Canada? That, as always, depends on the details.
In particular, on whether we are talking about Metro Vancouver or the City of Vancouver, and on
what good comparables are. With the new <a href="https://github.com/mountainMath/cancensus">cancensus R package</a>
it&rsquo;s straight forward to start hacking away
at this question. Let&rsquo;s start simple by looking at the 10 census metropolitan
areas and 10 census subdivisions (cities) with the largest gross density. And to keep things
somewhat in check, let&rsquo;s only look at census subdivisions with at least 50,000 people. (If we
drop the population restriction we get the cities of Westmount, Côte-Saint-Luc, Hampstead,
and White Rock gate-crash our top 10 list.)</p>

<p><img  src="http://doodles.mountainmath.ca/images/densest_CMAs.png" style="width:49%;"><img  src="http://doodles.mountainmath.ca/images/densest_CSDs.png" style="width:49%;"></p>

<p>We see that the City of Vancouver takes the top spot among the cities, Metro Vancouver comes in
behind Toronto, Red Deer and Montréal. Great for a game of trivia, but it&rsquo;s hard to learn
much of significance from this.</p>

<p>The reason is that we don&rsquo;t know how these densities come about. Metro Vancouver contains the North Shore mountains,
a very large swath of land where nobody lives. Yet this counts to our area (or at least the horizontal projection of it).
Toronto has the green belt. How does one compare these things?</p>

<h2>Distribution of Density</h2>

<p>A good first step is to look at the distribution of density within the cities. Again <em>cancensus</em> makes it easy
to map dissemination block level density. We change gears a tiny pit and simply focus Canada&rsquo;s 9 largest (by population) cities.</p>

<p><img  src="http://doodles.mountainmath.ca/images/density_map.png" style="width:99%;margin:5px auto;"></p>

<p>The large grey areas, with fewer than 1 person per hectare, jump out immediately. As the colour gradient suggest, there
are some denser areas around the centres, but the exact extent of these are hard to grasp. A better way to get a grip on
the proportion is to abandon the geographic coordinates and show the data as a tree map.</p>

<p><img  src="http://doodles.mountainmath.ca/images/density_area.png" style="width:99%;margin:5px auto;"></p>

<p>Now we much more clearly how the density in the different cities is made up, the proportion of low and high density areas.
The grey areas, with less than 1 person per hectare, are the parks, industrial and commercial land base of the city. Areas with
fewer than 25 people per hectare are densities usually found in broad suburban sprawl. In Vancouver, Shaughnessey, Southlands, or the Drummond Drive
area of West Point Grey are examples.</p>

<p>We can see that only Toronto, Montreal and Vancouver have significant portions of land beyond the 100 people per hectare density, with
Montréal devoting a larger portion of it&rsquo;s land to that density than any other of the cities.
In Vancouver the 50 to 100 people per hectare density dominates, althoguh the cutoffs are quite arbitrary to this should not be
over-interpreted, whereas in Toronto the  1-50 and 50-100 areas are larger.</p>

<p>Instead of asking how much area is devoted to what density, we can also ask what share of the population lives in what density.</p>

<p><img  src="http://doodles.mountainmath.ca/images/density_population.png" style="width:99%;margin:5px auto;"></p>

<p>This visualizes the density as felt by the population that lives in it. Comparing Montréal and Vancouver we see that about half of Montréal&rsquo;s citizens
live in 50 ro 150 people per hectare density, whereas in Vancouver half live in 25 to 100 people per hectare density. To make up for that, Vancouver has
a much higher portion of the population living in over 300 people per hectare density than Montréal.</p>

<h2>Next Steps</h2>

<p>There are several obvious ways to expand on this. One would be to take better comparables, so instead of comparing cities one could compare
areas with similar population centred around census metropolitan area centres. Comparing Vancouver to Toronto as cities is tricky, Toronto has
a comparable population to the whole of Metro Vancouver. So a better comparison could be to take the old City of Toronto boundaries and compare
these against Toronto. Or throw Burnaby, Richmond, New Westminster and North Vancouver into the mix with the City of Vancouver.</p>

<p>With the <a href="https://github.com/mountainMath/cancensus">cancensus R package</a> this is quite easy to do. And totally reproducible,
as I have <a href="https://github.com/mountainMath/density-explorations">uploaded the R notebook I used to create all the included images to github</a>.</p>

<p>Feel free to clone or fork it and adapt it for your own purposes.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RS Population Change]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/03/06/rs-population-change/"/>
    <updated>2017-03-06T11:06:57-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/03/06/rs-population-change</id>
    <content type="html"><![CDATA[<p><a href="https://censusmapper.ca/maps/583"><img src="http://doodles.mountainmath.ca/images/pop_change_ct.png" style="width:50%;float:right;margin-left:10px;"></a>
With reporting on the new census numbers gaining traction, and now
Mayor Robertson
<a href="http://www.cbc.ca/news/canada/british-columbia/gregor-robertson-statement-vancouver-character-homes-review-1.4011100">picking up on single family neighbourhoods losing population</a>
we thought it is time to crunch some numbers.</p>

<p>Why does it need number crunching? All the reporting so far is based on looking at CT (Census Tract) aggregates, like e.g. in the
map shown and linked to the right. But there is actually no
single CT in the City of Vancouver that only contains RS zoning. Deducing results by just looking on CT aggregates can lead
to misleading reporting, like we have seen with unoccupied dwellings in the &ldquo;Marine Gateway Neighbourhood&rdquo;. Given how prominent
this topic has become it is high time to dig into the details.</p>

<h3>TL;DR</h3>

<p>In summary, we can confirm that RS (single family), RT (duplex) and FSD neighbourhoods have been dropping population.
Slightly. Looking separately at the
east and the west side, we notice that population in these neighbourhoods dropped by about 1% on the west side and increased
slightly on the east side.</p>

<p>In all groupings that we looked at the household size dropped and the rate of unoccupied dwellings increased. This was counter-acted
by a growth in dwelling units, mostly confined to RS zones where laneway houses and suites were added (or newly discovered
in the 2016 census).</p>

<p>We split the analysis into <em>core</em> regions, blocks that lie completely within RS, RT and FSD zoning, and <em>fringe</em> regions,
blocks that have RS, RT or FSD zoning as well as other zoning. Fringe regions grew in population and had overall lower rates
of unoccupied units when compared to core regions.</p>

<!-- more -->


<h3>Comparing Censuses</h3>

<p>Comparing data across censuses is hard. For one, definitions change from one census to the next
and thus variables aren&rsquo;t always comparable. Four our immediate goal of comparing
population, private dwellings, and private households between the 2011 and 2016 censuses
that is not a concern.</p>

<p>Comparing data is relatively easy when census geographies are large (i.e. CT, CSD level or higher)
and the census geography matches exactly the area that we are interested in. For CSDs (Municipalities)
this is often the case, but at the sub-municipal level, the CTs (Census Tracts) or other sub-municipal
aggregation levels rarely line up with the regions one is interested in.</p>

<p>For example, if one is interested in changes in population in RS (single family zoned) neighbourhoods in
Vancouver, looking at selected CTs will only give some initial indication. The reason is that
there is actually no CT in the City of Vancouver that is entirely RS zoned. There are several that
come close (the closest one is CT 9330015.01 around 41st and Thyne, which actually
increased population from 5,364 in 2011 to 5,485 in 2016) but it shows how tricky
it is to answer the really simple question how the population changed in RS neighbourhoods.</p>

<p>So how to deal with this issue? The cleanest way is a custom tabulation from StatCan, but that takes
time, costs money, and may still
<a href="https://twitter.com/vb_jens/status/838561779970011136">result in problems when data was not geocoded correctly</a>,
which is next to impossible to detect in custom tabulations.</p>

<p>An alternative way is to compare censuses at finer aggregation levels, that is at DAs (Dissemination Areas) or
DBs (Dissemination Blocks).</p>

<h3>Comparing Censuses at DA or DB levels</h3>

<p>For our concrete example, that means we look for DBs within RS zoning and work with these.</p>

<p>There are several difficulties with this approach. The most important is that the finer data we look at,
the more likely we pick up problems with Census data (yes, there are problems) and mistake them for real world data.
For our case through, we can avoid some of that by aggregating over all DBs within RS zoning to even out some
of these issues. Morever, we can visually inspect the data to look for any particular DB that seems to be problematic
and do some ground-truthing to see if the issues are only in the data or actually on the ground.</p>

<p>The next difficulty is technical in nature. Census geographies, including DBs and DAs, can change from one census to
another and thus may not be comparable. In order to get reliable results we need to make sure that we work with
a common set of geographies for both the 2011 and 2016 censuses. Luckily this is only a technical problem that can be
overcome as census geographies don&rsquo;t just change randomly but still retain some basic comparability.</p>

<p>And it&rsquo;s perfect timing, since we just created a &ldquo;least common denominator&rdquo; tiling derived from 2011 and 2016 DBs and DAs. At
CensusMapper we work with &ldquo;cartographic&rdquo; DBs (so we clip our major bodies of water), which leads to a minor issue where
between 2011 and 2016 things were clipped slightly differently which yielded two DBs in 2011 (with 16 and 17 people in it)
being clipped out (and having no people in it) in 2017, likely a combination of adjusting some tolerances in the StatCan
algorithms as well as some minor changes in geocoding that moved the population into an adjacent DB
(like e.g. happened with 59150971009 and 59150971008 south of King Edward split along Carnarvon). Apart from that, the
result is a tiling of Canada by DB and DA-based geographies that allow for consistent comparisons across the two censuses.</p>

<p>In numbers there were 493,192 cartographic DBs in 2016, 489,676 in 2016, and these resulted in a least common denominator tiling
by 445,953 DB-based geographies.</p>

<h3>RS, RT and FSD Zones and Population Change</h3>

<p><img src="http://doodles.mountainmath.ca/images/filter_all.png" style="width:50%;float:right;margin-left:10px;">
Back to our original question, how did population change in RS zoning. Before we go there, we think it makes more sense to expand
the question to ask for RS, RT (duplex) and FSD (First Shaughnessy) combined as these are about equally restrictive in what
we allow there.</p>

<p>Grabbing the <a href="http://data.vancouver.ca/datacatalogue/zoning.htm">latest available zoning data</a> and uploading it to
CensusMapper makes it easy to download the 2011 and 2016 dissemination blocks that intersect RS, RT and FSD zoning. We removed the RS part
that snakes along the downtown beaches and covers Stanley Park, as well as the sliver creeping up False Creek and covering the marinas there.</p>

<p>When we intersect the census data with the zones, we also compute the overlap each DB has with the zoning and disregard any region
with less than 10% overlap. Moreover we divide the dissemination blocks into <em>core</em> blocks where
the overlap is greater than 99% and <em>fringe</em> blocks, where the overlap is less than 99%.</p>

<p>One should remember that a significant
portion (a majority actually) of RS, RT and FSD dwellings are contained in &ldquo;fringe&rdquo; areas.
So it is best to focus on the rates of change, we would expect the total number of population decline
of all RS, RT and FSD zoned properties to be higher.</p>

<p>Here are the results:</p>

<pre><code>RS, RT, FSD
core: 236 DB, fringe: 475 DB
Population 2011 - 2016
core pop change: -69, fringe pop change: 6673, core total pop: 124916, fringe total pop: 284231
Dwellings 2011 - 2016
core dw change: 2812, fringe dw change: 7142, core total dw: 47008, fringe total dw: 121488
Households 2011 - 2016
core hh change: 1194, fringe hh change: 5127, core total hh: 42298, fringe total hh: 111801
</code></pre>

<p><img src="http://doodles.mountainmath.ca/images/filter_core.png" style="width:50%;float:right;margin-left:10px;">
What we see is that the population in the &ldquo;core&rdquo; DBs did drop. Slightly. At the same time the number of dwellings
increased quite noticeably by 6.3% in the core, with essentially all of the dwelling growth located within RS zones
(as opposed to RT and FSD). So most of that dwelling
growth is due to suites and laneway houses. Note that we only capture 47008 dwellings in the &ldquo;core&rdquo; RS, RT, FSD areas,
which is less
than half of the dwelling units in RS, RT and FSD with the remaining dwellings are located within the &ldquo;fringe&rdquo; regions.</p>

<p><a href="https://censusmapper.ca/maps/596"><img src="http://doodles.mountainmath.ca/images/van_pop_comp.png" style="width:50%;float:left;margin-right:10px;"></a>
Interestingly, the number of households grew much slower than the number of dwellings in the &ldquo;core&rdquo; regions, increasing the rate
of unoccupied units from 7% to 10%. Following our decomposition of population growth <a href="https://censusmapper.ca/maps/596">mapped here</a>
and <a href="http://doodles.mountainmath.ca/blog/2017/02/10/2016-census-data/">explained in more detail in a previous post</a>, we see
that the population growth of -69 in the &ldquo;core&rdquo; regions can be decomposed into:</p>

<ul>
<li>-3,699 due to declining household size,</li>
<li>-4,321 due to increase in unoccupied dwellings, and</li>
<li>7,952 due to increase in dwelling units.</li>
</ul>


<p><img src="http://doodles.mountainmath.ca/images/filter_fringe.png" style="width:50%;float:right;margin-left:10px;">
We can do the same analysis for the &ldquo;fringe&rdquo; areas, where RS, RT and FSD zoning mixes with other zones. Here we get a population
increase by 2.4%, driven by an increase
in dwelling units by 6.2%, and dampened by shrinking household size and a
more modest increase in the rate of unoccupied units from 6.7% in 2011 to 8% in 2016. We note that
the rate of unoccupied units increased significantly less on the fringe when compared to the core.</p>

<p>Breaking up the population growth of 6,673 people as before we have:</p>

<ul>
<li>-6,667 due to declining household size,</li>
<li>-3,996 due to increase in unoccupied dwellings, and</li>
<li>17,336 due to increase in dwelling units.</li>
</ul>


<h4>The West Side</h4>

<p>Lastly, we probably can&rsquo;t talk about this without running a separate analysis for the west side. So here we go.</p>

<pre><code>RS, RT, FSD
core: 88 DB, fringe: 184 DB
Population 2011 - 2016
core pop change: -510, fringe pop change: 1631, core total pop: 42878, fringe total pop: 105309
Dwellings 2011 - 2016
core dw change: 572, fringe dw change: 2426, core total dw: 16735, fringe total dw: 49210
Households 2011 - 2016
core hh change: -18, fringe hh change: 1226, core total hh: 15066, fringe total hh: 44907
</code></pre>

<p>We see that this confirms conventional wisdom that the population decline in the core areas of RS, RT, FSD is stronger
on the west side (and in fact population did increase overall in the core areas on the east side). The rate of unoccupied (by usual residents)
units was quite similar to the overall RS, RT, FSD, climbing from 6.7% in 2011 to 10% in 2016.</p>

<p>Again splitting up the population change of -510 people in the &ldquo;core&rdquo; area into components we get:</p>

<ul>
<li>-458 due to declining household size,</li>
<li>-1,587 due to increase in unoccupied dwellings, and</li>
<li>1,535 due to increase in dwelling units.</li>
</ul>


<p>For the &ldquo;fringe&rdquo; west side areas we again observe that population increased at 1.5%, dwellings by 5.1% and
the rate of unoccupied units grew slower from 6.6% to 8.7%. Splitting up the population change of 1,631 people:</p>

<ul>
<li>-1,278 due to declining household size,</li>
<li>-2,466 due to increase in unoccupied dwellings, and</li>
<li>5,376 due to increase in dwelling units.</li>
</ul>


<h3>Glossary</h3>

<p>We are a little loose with our use of language. In this post &ldquo;unoccupied&rdquo; is always short for &ldquo;not occupied by usual residents&rdquo;,
so in simpler terms &ldquo;not used as primary residence&rdquo;. &ldquo;Occupied&rdquo; refers to &ldquo;used as primary residence&rdquo;.</p>

<p>West side and east side were divided along the longitude for Ontario road.</p>

<p>Core regions are Dissemination Blocks that have at least 99% overlap with RS, RT or FSD zoning.</p>

<p>Fringe regions are Dissemination Blocks that have between 10% and 99% overlap with RS, RT or FSD zoning.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[More on Teardowns]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/02/21/more-on-teardowns/"/>
    <updated>2017-02-21T10:59:27-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/02/21/more-on-teardowns</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/teardowns"><img src="http://doodles.mountainmath.ca/images/teardowns_animated.gif" style="width:50%;float:right;margin-left:10px;"></a>
A little over a year ago we ran some analysis
<a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">on teardowns</a>
of single family homes in the City of Vancouver. We used the City of Vancouver
open data to understand why some single family homes got torn down and other&rsquo;s don&rsquo;t.</p>

<p>Relying entirely on open data, there were some important questions that could not
be answered. So together with Joe Dahmen at UBC&rsquo;s School Of Architecture And Landscape Architecture
we came back to the question
and folded in transaction data from BC Assessment to add some more details and rigor.</p>

<p>The result turned out quite similar to what our initial cruder methods came
up with, but it lead to some important refinements.</p>

<p>We won&rsquo;t go into the details of the findings here, you can
<a href="https://mountainmath.ca/teardowns" target="_blank" class='btn btn-default'>read the online data story</a>
if you are interested. Instead we will go into a little more details how
the analysis was done and what is still missing.</p>

<!-- more -->


<p>The most critical piece that we added was transaction data, that is
which properties got sold in what year. Almost all properties that got
torn down were associated with a property transaction in the 4 years
around it getting torn down rebuilt.</p>

<p>This allowed us to refine the question from &ldquo;why did building A get torn
own and building B did not&rdquo; to ask the same question only considering
transacted buildings.</p>

<p>Conditioning on the most important determinant of a building getting torn down,
the transaction, we could focus in much better on what building-specific
parameters are driving teardowns.</p>

<h3>Variables</h3>

<p>We had annual assessment data pegged at July 2005 through July 2016, although
we excluded the July 2016 data for some parts of the analysis as the value
gains that year where
<a href="http://doodles.mountainmath.ca/blog/2017/01/16/2017-assessment-data/">quite extraordinary</a>
and prices have come down
a bit since then. We felt that this most recent assessment may not be a good
launching point to project the future from.</p>

<p>Unfortunately, the number of variables for teardowns that we have is
quite limited. We only have good data on assessed land values, assessed
building values and lot area. For a very small subset of about 500 buildings
we also have the building age of the building that got torn down. We have
GFA estimates for buildings that got torn down after 2009 through the
<a href="http://doodles.mountainmath.ca/blog/2016/03/05/physical-sfh-form-over-time/">analysis of LIDAR data</a>
that we did, but those estimates are quite crude and again only cover a portion of our
time frame.</p>

<p>A crucial variable that we are still missing is the actual time of the building
demolition. We inferred this from the time a new building got completed on that
property, but this inevitably introduces noise to the data. It makes it
difficult to pick the right time to calculate the relative building value. Moreover,
there may be the occasional property that got built on vacant land, so nothing got torn down.
This was less an issue for the analysis part, where we had ways to filter out such properties,
but it did cause some problems with the visualization part of the project. We did filter out
some properties manually that we could identify as being built on vacant land within
the timeframe of the visualization, namely some properties on Deering Island.</p>

<p>On top of that, the decision to demolish the building was often made long
before the building got torn down. Waiting times on demolition permits can be quite long, depending
on the property. Having access to building permit data would help sharpen
this variable. The word from the friendly open data folks is that the
City of Vancouver is working on making these public, maybe an FOI request
can help them speed up the process.</p>

<h3>Noise</h3>

<p>The most important source of noise in our data is that fact that assessment
data is only accurate <em>on average</em>. For particular buildings it can be substantially
off. We suspect that this is one of the reasons why for
buildings that are assessed to be essentially worthless,
the teardown probability tops out at a little above 60%. So someone
paying $2.5 million for a house that is worth only $10,000 to move in and live
in that house makes absolutely no sense. If the building like this did not get torn down,
we hypothesize one of three scenarios:</p>

<ol>
<li>The building was purchased as a pure investment vehicle and rented out
until an opportune time to re-develop or sell the property.</li>
<li>The assessment grossly undervalued the building.</li>
<li>The building was extensively renovated.</li>
</ol>


<p>We have looked through the data and have found little evidence that scenario 3 is
playing out in significant numbers. Extensive renovations show up in assessment
data via building value gains and the &ldquo;year improved&rdquo;. We don&rsquo;t have
data to assess the other two hypotheses.</p>

<h3>Model</h3>

<p>Given that limited variables we trained a handful of models on our data to see
how to best predict future teardowns. In all models we used, the relative
building value was the single most predictive variable, accounting for well over
80% of explanatory power no matter what methods we used. Moreover, the
performance of more complex machine learning models was not markedly better
that using a simple logistic regression. Similarly, dropping all other variables
except the relative building value only slightly decreased the skill of our
model.</p>

<p>One way to improve on our model is to use a proper survival analysis that
can better account for data that is only available for certain time frames.
For example, teardown early in our time frame suffer from the shortcoming that
we don&rsquo;t have transaction data reaching back far enough to link the teardown
to a transaction. Or more to the point, be able to compare it to other
transacted properties that didn&rsquo;t get torn down. Similar problems occur
at the end of our time frame, and with variables that are only available
in certain sub time frames.</p>

<script>
function resetImages(){
    $('img').each(function(img){
        imgsrc = $(img).attr('src');
        if (imgsrc.slice(imgsrc.length-4)=='.gif') {
            $(img).attr('src', '');
            $(img).attr('src', imgsrc);

        }
    });
    setTimeout(function(){
        resetImages();
    },25000);
}
setTimeout(function(){
    resetImages();
},25000);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bumper Year for Thumb Twiddlers]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/01/18/bumper-year-for-thumb-twiddlers/"/>
    <updated>2017-01-18T11:10:43-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/01/18/bumper-year-for-thumb-twiddlers</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/assessment?zoom=12&amp;lat=49.2494&amp;lng=-123.1241&amp;layer=12"><img src="http://doodles.mountainmath.ca/images/twiddling_thumbs_2006_2017.png" style="width:50%;float:right;margin-left:10px;"></a>
Almost a year has passed since we first noticed how
<a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">sitting on single family homes and twiddling thumbs generates more income than working</a>.
And not just at the level of individual single family households. In the
City of Vancouver, the cumulative
land value gains of just the single family homes eclipsed the cumulative taxable earnings
reported to the CRA for the entire population.</p>

<p>With the new assessment data available now, it is time to run the numbers
and see how our thumb-twiddlers fared vs workers this year. If you thought
last year&rsquo;s twiddling thumbs returns were crazy high, you better hold onto
your hats!</p>

<!-- more -->


<h3>Work</h3>

<p>Not much changed in terms of the income people earned. CANSIM does not make the
taxfiler data freely available at the municipal level, we we will again estimate
the cumulative income for residents of the City of Vancouver by using 2010
Census data and extrapolating by applying the Metro Vancouver rate of income
growth.</p>

<p>This way we get a cumulative $22.3bn pre-tax or $18.6bn after-tax income in 2010,
<a href="http://www5.statcan.gc.ca/cansim/a26?lang=eng&amp;retrLang=eng&amp;id=1110009&amp;&amp;pattern=&amp;stByVal=1&amp;p1=1&amp;p2=37&amp;tabMode=dataTable&amp;csid=">which grew around 13% between 2010 and 2014</a>.
Extrapolating this for another two years to 2016 we estimate an income growth of around 20% since 2010,
which then pegs the cumulative income for the City of Vancouver at
$26.8bn pre-tax or $22.3bn after-tax (ignoring financial drag and other issues).</p>

<h3>Twiddling Thumbs</h3>

<p>Let&rsquo;s compare this to how much Vancouver home owner households &ldquo;earned&rdquo; last year by twiddling thumbs while sitting on their
property. To keep things simple and consistent with last year, we again focus on just the single family homes (SFH).
And again, we only consider land value changes, after all changing the
building value because of renovation or rebuilding certainly does not happen by twiddling thumbs.</p>

<p>The median SFH land value increased was $435,000 (the average was $594,005), for a cumulative land value increase
of $46.7bn, accounting for about half of the
<a href="http://doodles.mountainmath.ca/blog/2017/01/16/2017-assessment-data/">total land value increase of the City of Vancouver</a>
or two thirds of the land value increase for residential (and mixed) land uses.</p>

<p>So while last year the twiddling thumbs return were still comparable to the cumulative income in the City of Vancouver,
this year the thumb twiddlers blow the income earners out of the water.
<span style="font-weight:bolder;">Just by by twiddling their thumbs, the SFH property owners alone earned twice as much
as the entire population of the
City of Vancouver did by actually working</span>. And in most cases homeowners won&rsquo;t pay taxes on
their thumb-twiddle earnings, although the
<a href="http://doodles.mountainmath.ca/blog/2016/10/04/secondary-suites-and-taxes/">CRA recently made it harder with their new reporting rules</a>.</p>

<p>We are glossing over a couple of details here, for example the numbers are not adjusted for inflation, and costs like
property taxes and property transfer tax are not taken into account.</p>

<p>Of course, comparing income from work to income from twiddling thumbs is not entirely fair. The income from twiddling thumbs
is frozen in the property until the owner sells. But gains accumulate over the years, and eventually everyone sells and realizes
the gains (or passes them on to the next generation). And it&rsquo;s always good to remember that
&ldquo;<a href="https://twitter.com/GRIDSVancouver/status/813516103490015232">house-rich cash-poor homeowners are one transaction away from simply being rich renters</a>&rdquo;.</p>

<h3>The Long Term</h3>

<p><a href="https://mountainmath.ca/map/values?filter=sfh&zoom=13&lat=49.2482&lng=-123.1213&layer=25&mapBase=2&year=2016"><img src="http://doodles.mountainmath.ca/images/twiddling_thumbs_animated_2017.gif" style="width:50%;float:left;margin-right:10px;"></a>
Most people don&rsquo;t trade houses like stocks, so what really matters is the long term gains, not the year to year changes. The 11 year
timeframe for which we have data roughly matches the average holding time for a single family house. The year over year
changes in land value vary dramatically, as the image shows and can be explored further using the
<a href="https://mountainmath.ca/map/values?filter=sfh&amp;zoom=13&amp;lat=49.2482&amp;lng=-123.1213&amp;layer=25&amp;mapBase=2&amp;year=2016">interactive version</a>.</p>

<p>Over 11 years, the single family home land value quadrupled. The median (nominal) single family home land value increase over that timespan
was $1,233,000, or $112,000 per year. Broken down further, that&rsquo;s $2,339,000 ($213,000 per year) for the median west side home
and $1,031,000 ($94,000 per year) for the median east side home.</p>

<p><a href="https://mountainmath.ca/map/assessment?filter=sfh&amp;zoom=13&amp;lat=49.2494&amp;lng=-123.1241&amp;layer=12"><img  src="http://doodles.mountainmath.ca/images/twiddling_thumbs_2006_2017.png" style="width:50%;float:right;"></a>
We <a href="https://mountainmath.ca/map/assessment?filter=sfh&amp;zoom=13&amp;lat=49.2494&amp;lng=-123.1241&amp;layer=12">mapped the land value gain averaged over 11 years</a>.
We can observe that the average yearly increase depends on the square footage of the property as well as the location. The rates are mostly uniform
throughout the city (with some notable exceptions), but properties starting out with a higher value will see higher total value
increases. It&rsquo;s probably fair to say that even using the 11 year average, most homeowners &ldquo;earned&rdquo; more money twiddling thumbs
than pursuing a more traditional employment.</p>

<h3>Hourly Rate for Twiddling Thumbs</h3>

<p>Using the <a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">same methods as last year</a>
we can compute the hourly earnings of thumb twiddlers. For the July 2016 to July 2017 timeframe, thumb twiddlers
in the City of Vancouver averaged a tidy $239 per hour.
Using the 11 year averaged numbers instead of focusing on just the last year we still get a healthy average
thumb twiddling rate of $62 per hour.</p>

<p>Another bumper year for thumb twiddlers!
Considering to change your line work to thumb twiddling? To bad thumb twiddling is so unaffordable!</p>

<h3>Data Dump</h3>

<p>Here is the raw output of the stats run on the single family properties for anyone interested.</p>

<h5>SFH Land Value Rise (2016 - 2017)</h5>

<ul>
<li>City Wide: Average $594,005 Median $435,000, Count: 78648, Total: $46,717,325,799, Hourly: $239</li>
<li>Eastside: Average $373,306 Median $358,000, Count: 47988, Total: $17,914,233,199, Hourly: $150</li>
<li>Westside: Average $939,435 Median $866,000, Count: 30660, Total: $28,803,092,600, Hourly: $378</li>
</ul>


<h5>SFH Land Value Rise (2006 - 2017)</h5>

<ul>
<li>City Wide: Average $153,777 Median $112,090, Count: 77809, Total: $11,965,271,272, Hourly: $62</li>
<li>Eastside: Average $96,892 Median $93,727, Count: 47545, Total: $4,606,748,985, Hourly: $39</li>
<li>Westside: Average $243,144 Median $212,636, Count: 30264, Total: $7,358,522,287, Hourly: $98</li>
</ul>


<p>The total number of units for the 2006 to 2017 analysis are a little lower since not all properties can be traced over that time
period without resorting to more tedious analysis.</p>

<script>
function resetImages(){
    $('img').each(function(img){
        imgsrc = $(img).attr('src');
        if (imgsrc.slice(imgsrc.length-4)=='.gif') {
            $(img).attr('src', '');
            $(img).attr('src', imgsrc);

        }
    });
    setTimeout(function(){
        resetImages();
    },25000);
}
setTimeout(function(){
    resetImages();
},25000);
</script>

]]></content>
  </entry>
  
</feed>
