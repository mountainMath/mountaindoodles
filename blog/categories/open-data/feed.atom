<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: open data | Mountain Doodles]]></title>
  <link href="http://doodles.mountainmath.ca/blog/categories/open-data/feed.atom" rel="self"/>
  <link href="http://doodles.mountainmath.ca/"/>
  <updated>2017-01-16T23:03:56-08:00</updated>
  <id>http://doodles.mountainmath.ca/</id>
  <author>
    <name><![CDATA[MountainMath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[2017 Vancouver Assessment Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/01/16/2017-assessment-data/"/>
    <updated>2017-01-16T19:04:19-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/01/16/2017-assessment-data</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/values?filter=[sfh]&amp;zoom=13&amp;layer=9"><img  src="http://doodles.mountainmath.ca/images/value_animation.gif" style="width:50%;float:right"></a>
The friendly folks at
<a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data</a> just
<a href="https://twitter.com/VanOpenData/status/688060388190097408">updated their property assessment data</a> with the fresh 2017
property tax assessments. Time to run the script to update the <a href="https://mountainmath.ca/map/assessment">Vancouver Assessment Map</a>
with the new data, just <a href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/">like we did last year</a>.</p>

<h4>Maps</h4>

<!-- more -->


<p>By now we have a variety of maps that highlight different aspects of Vancouver real estate. Our basic
<a href="https://mountainmath.ca/map/assessment">interactive assessment map</a> offers a variety of ways to slice and display the data. It&rsquo;s
mostly focused on functionality, some of which we described in
<a href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/">last year&rsquo;s post</a>, as well as other posts like
the one on <a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">twiddling thumbs vs working</a>.</p>

<p>We also have interactive views focusing on how real estate prices varied over time, for example the <a href="http://doodles.mountainmath.ca/blog/2016/04/01/on-dirt-and-houses/">houses and dirt map</a>
that separates out the values of the structures and the land, and also allows to filter by type of housing, to animate changes over time.</p>

<p>For people that like simpler maps we also have a <a href="https://mountainmath.ca/map/values?filter=[sfh]&amp;zoom=13&amp;layer=9">plain total value over time map</a>
that allows to interactively step through the years and see how single family housing in Vancouver changed over time. Here we also
added the ability to visualize year-over-year value changes, which also make for good studying how BCAssessment changed their valuation
algorithm over the years.</p>

<h4>The Data</h4>

<p>The data originates with BC Assessment, which estimates land and building values of each property based on recent sales
of comparable properties. The values are pegged at July 1 of each year, with the the most recent available now being July 1 2016.
The estimates for the values do not reflect changes in the market since then. Moreover, the estimates can be quite off on an individual
property level, but are unbiased. That means that any statistics derived from a large subsample should fairly accurately
reflect actual market conditions for July 1st. Lastly, the assessed values will still change a bit as some will be successfully appealed.</p>

<p>While City of Vancouver, as well as the City of Surrey, make this data available for general use through their open data portal,
BC Assessment does not give out their data with a license that would allow mapping of individual properties. They also make data
on individual properties available through their <a href="https://evaluebc.bcassessment.ca">eValue website</a>,</p>

<p>The format of the data the municipalities are giving out through their open data portal is
different, so lazy me is only importing data from City of Vancouver.</p>

<p>The motivation behind the map was to understand the building stock, so we filter out parks and some other properties.</p>

<p>The new city dataset does not include the 2017 tax levy, so we still only show the 2016 tax levies until CoV updated their dataset.</p>

<h4>History</h4>

<p>Here is a quick history of the overall land and building values aggregated for Vancouver between 2006 and 2017.</p>

<p>We upped our game a bit from last year and added a bit of interactivity. Use the dropdown menus to drill down
into city neighbourhoods, view values for all properties or just residential properties and display as total value or year-over-year
percentage change.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<select id="nbhd-select"></select>
<select id="keys-select"></select>
<select id="type-select">
<option value='total'>Total Value</option>
<option value='percent'>Percentage Change</option>
</select>
<div id="graph" style="height:200px;max-width:640px;" data-lines="/data/detail_history.json"></div>
<div class="legend no-margin">
</div>
</div>


<p>When looking at all properties in the city, the increase in land
value year over year was 21.4% ($45.2bn), while overall building values increased by 7.3% ($5bn). Hover, click or touch
the points in the graph to get the values for the corresponding year.</p>

<script>
var ready_for_graph = function() {
    var d3lines=[];
    var padding = {top: 20, right: 20, bottom: 30, left: 90};
    var prevChartWidth = 0, prevChartHeight = 0;
    var updateTransistionMS = 750; // milliseconds
    var jsonData, xScale, yScale, line,neighbourhoods;

    var currentValue='City of Vancouver';
    var currentKeys=['land','building'];
    var currentModePercentage=false;

    var hash={
            land: {label:"Land Value", color:"green"},
              building: {label:"Building Value", color:"blue"},
              res_land: {label:"Residential Land Value", color:"darkgreen"},
              res_building: {label:"Residential Building Value", color:"darkblue"},
              land_p: {label:"Land Value Increase", color:"green"},
              building_p: {label:"Building Value Increase", color:"blue"},
              res_land_p: {label:"Residential Land Value Increase", color:"darkgreen"},
              res_building_p: {label:"Residential Building Value Increase", color:"darkblue"}
                };
    var keys=Object.keys(hash);

    var symbol;
    var prefix;
    var numberFormatter = function (y) {
        return '$' + Math.round(prefix.scale(y*10))/10.0 + symbol;
    };

    var graphs=d3.select("#graph");
    var div=graphs[0][0];
    if (div==null|| div.childElementCount!=0) {return;}
    var data_url=div.dataset.url;

    // create svg and g to contain the chart contents
    var baseSvg = graphs.append("svg");
    var chartSvg=baseSvg
        .append("g")
        .attr("class", "chartContainer")
        .attr("transform", "translate(" + padding.left + "," + padding.top + ")");

    // create the x axis container
    chartSvg.append("g")
        .attr("class", "x axis");

    // create the y axis container
    chartSvg.append("g")
        .attr("class", "y axis");
    var line;
    var largest=null;
    var lineData;
    if (div.dataset.lines) {
        d3.json(div.dataset.lines,function(error,json){
        jsonData=json;
        neighbourhoods=Object.keys(jsonData);
        var keyHash={all:{label: 'All Properties',data:['land','building']},res:{label: 'Residential Properties',data:['res_land','res_building']}};
        var keyOptions=Object.keys(keyHash);
        d3.select('#nbhd-select').selectAll('option').data(neighbourhoods).enter().append('option').attr('value',function(d){return d}).text(function(d){return d});
        $('#nbhd-select').on('change',function(d){
            currentValue=this.value;
            updateChart({init:true,keys:currentKeys,data:jsonData[currentValue],percentage:currentModePercentage})
        });
        d3.select('#keys-select').selectAll('option').data(keyOptions).enter().append('option').attr('value',function(d){return d}).text(function(d){return keyHash[d].label});
        $('#keys-select').on('change',function(d){
            currentKeys=keyHash[this.value].data;
            updateChart({init:true,keys:currentKeys,data:jsonData[currentValue],percentage:currentModePercentage})
        });
        $('#type-select').on('change',function(d){
            currentModePercentage=this.value=='percent';
            updateChart({init:true,keys:currentKeys,data:jsonData[currentValue],percentage:currentModePercentage})
        });
        lineData=json[currentValue];
        var domain=[null,null];
        var range=[null,null];
        lineData.forEach(function(d) {
             d.date = +d.date;
             if (domain[0]==null || domain[0]> d.date) domain[0]= d.date;
             if (domain[1]==null || domain[1]< d.date) domain[1]= d.date;
             keys.forEach(function(k){
                d[k]=+d[k];
                if (range[0]==null || range[0]> d[k]) range[0]= d[k];
                if (range[1]==null || range[1]< d[k]) range[1]= d[k];
            });
        });
        xScale=d3.scale.linear().domain(domain);
        var toAdd=(range[1]-range[0])/10;
        range[0]-=toAdd;
        range[1]+=toAdd;
        yScale=d3.scale.linear()
            .domain(range);

        line = d3.svg.line()
            .x(function(d) { return xScale(d.date); })
            .y(function(d) { return yScale(0); })
            .interpolate("linear");
        xAxis = d3.svg.axis()
            .scale(xScale)
            .orient("bottom")
            .tickFormat(d3.format("d"))
            .tickValues(domain);

        yAxis = d3.svg.axis()
            .scale(yScale)
            .orient("left")
            .tickFormat(numberFormatter)
            .ticks(5);

        prefix = d3.formatPrefix(range[1]);
        if (prefix.symbol=='K') {
            symbol='k'
        } else if (prefix.symbol=='M') {
                symbol='m'
        } else if (prefix.symbol=='G') {
            symbol='bn'
        } else if (prefix.symbol=='T') {
            symbol='tn'
        }
        updateChart({init:true,keys:currentKeys,data:jsonData[currentValue],percentage:currentModePercentage});
        });

    }


    function updateChart(options)
    {
        var lineData=options.data;
        var init=options.init;
        var keys=options.keys;

        lineData.forEach(function(d,i) {
             keys.forEach(function(k){
                d[k]=+d[k];
                if (i>0) d[k+'_p']=d[k]/lineData[i-1][k]-1;
             });
        });

        if (options.percentage) {
            keys=keys.map(function(k){return k+'_p'});
            lineData=lineData.slice(1);
        }

        var domain=[null,null];
        var range=[null,null];
        lineData.forEach(function(d,i) {
             d.date = +d.date;
             if (domain[0]==null || domain[0]> d.date) domain[0]= d.date;
             if (domain[1]==null || domain[1]< d.date) domain[1]= d.date;
             keys.forEach(function(k){
                if (range[0]==null || range[0]> d[k]) range[0]= d[k];
                if (range[1]==null || range[1]< d[k]) range[1]= d[k];
            });
        });

        //if (options.percentage) domain[0]+=1;

        var toAdd=(range[1]-range[0])/10;
        range[0]-=toAdd;
        if (!options.percentage) range[0]=Math.max(0,range[0]);
        range[1]+=toAdd;
        yScale.domain(range);
        xScale.domain(domain);
        var formatter;
        if (options.percentage) {
            formatter=d3.format('.1%');
        } else {
            formatter=numberFormatter;
        }
        yAxis.tickFormat(formatter);

        var legend=d3.select('.legend');
        legend.empty();
        legend.selectAll('.item').data(keys)
            .enter().append('p').attr('class','item')
            .html(function(k){return '<i style="background:'+hash[k].color+'"></i> '+hash[k].label+' <span style="float:right;margin-right:10px;" id="'+k+'_value"></span>'});
        //legend.selectAll('.item').exit().remove();


        // get the height and width subtracting the padding
//    var innerHeight = window.innerHeight - 20;
        var innerWidth = window.innerWidth - 20;
        var divWidth=$(div).width();
        if (divWidth==0) divWidth=$(div.parentElement.parentElement).width();
        var maxWidth=parseInt($(div).css('max-width'));
        if (divWidth==0) divWidth=innerWidth*0.8;
        if (divWidth>maxWidth) divWidth=maxWidth;
        var chartWidth = divWidth-padding.left-padding.right;//960 - margin.left - margin.right,
        var chartHeight = $(div).height()-padding.top-padding.bottom;//500 - margin.top - margin.bottom;


        // only update if chart size has changed
        if (true || (prevChartWidth != chartWidth) || (prevChartHeight != chartHeight)) {
            prevChartWidth = chartWidth;
            prevChartHeight = chartHeight;

            //set the width and height of the SVG element
            chartSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);
            baseSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);

            // ranges are based on the width and height available so reset
            xScale.range([0, chartWidth]);
            yScale.range([chartHeight, 0]);




            if (init) {
                // if first run then just display axis with no transition
                chartSvg.select(".x")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                chartSvg.select(".y")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .call(yAxis);
            }
            else {
                // for subsequent updates use a transistion to animate the axis to the new position
                var t = chartSvg.transition().duration(updateTransistionMS);

                t.select(".x")
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                t.select(".y")
                    .call(yAxis);
            }

            var sourceData=lineData;

            function addSeries(key){
                var g=d3.select(this);
                var color=hash[key].color;
                var label=hash[key].label;
                var className=key;

                // bind up the data to the line
                var lines = g.selectAll("path.line")
                    .data([sourceData]); // needs to be an array (size of 1 for our data) of arrays

                var valueFunction=function(d){return d[key]};
                var yFunction=function(d){return yScale(valueFunction(d))};
                var ff=key[key.length-1]=='p' ? d3.format('.1%') : numberFormatter;
                var formatFunction=function(d){return ff(valueFunction(d))};
                 var line=d3.svg.line()
                      .x(function(d) { return xScale(d.date); })
                      .y(yFunction)
                      .interpolate("linear");

               // transistion to new position if already exists
                lines.transition()
                    .duration(updateTransistionMS)
                    .attr("d", line);


                // add line if not already existing
                lines.enter().append("path")
                    .attr("class", "line")
                    .attr("stroke", color)
                    .attr("stroke-width", 2)
                    .attr('fill','none')
                    .attr("d", line);

                lines.exit().remove();

                // bind up the data to an array of circles
                var circles = g.selectAll("circle")
                    .data(sourceData);

                // if already existing then transition each circle to its new position
                circles.transition()
                    .duration(updateTransistionMS)
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", yFunction);

                // if new circle then just display
                circles.enter().append("circle")
                    .attr("class", className)
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", yFunction)
                    .attr("r", 4)
                    .attr('fill', 'transparent')
                    .style("stroke", color)
                    .style("stroke-width", 8)
                    .on('mouseover',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + formatFunction(d))
                    }).on('click',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + formatFunction(d))
                    }).on('touch',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + formatFunction(d))
                    }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});
                circles.exit().remove();
                }
            }

            var selection=chartSvg.selectAll('g.series').data(keys);
            selection.exit().remove();
            selection.enter().append('g').attr('class','series');
            chartSvg.selectAll('g.series').each(addSeries);
    }

    // look for resize but use timer to only call the update script when a resize stops
    var resizeTimer;
    window.onresize = function(event) {
        clearTimeout(resizeTimer);
        resizeTimer = setTimeout(function()
        {
                updateChart({init:false,keys:currentKeys,data:jsonData[currentValue],percentage:currentModePercentage});
        }, 100);
    }


};
ready_for_graph();



function resetImages(){
    $('img').each(function(img){
        imgsrc = $(img).attr('src');
        if (imgsrc.slice(imgsrc.length-4)=='.gif') {
            $(img).attr('src', '');
            $(img).attr('src', imgsrc);

        }
    });
    setTimeout(function(){
        resetImages();
    },25000);
}
setTimeout(function(){
    resetImages();
},25000);

</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updated Property Tax Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/12/13/updated-property-tax-data/"/>
    <updated>2016-12-13T20:10:22-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/12/13/updated-property-tax-data</id>
    <content type="html"><![CDATA[<p>The property tax data for the City of Vancouver has been available for a while now, and with new assessment data becoming
available soon everyone&rsquo;s worried about what their property taxes will look like. The City just passed a 3.9% increase
in their budget, so on average everyone will pay 3.9% more taxes than they did last year.</p>

<p>The exact change in property taxes varies from property to property. There is a <a href="https://patrickjohnstone.ca/2013/01/on-assessments-and-mil-rates.html">nice overview</a>
on how this works in general, for the City of Vancouver there is an added complication of land value averaging meant to
soften sudden land value increases, that effectively serves to lower taxes for single family homeowners in a rising market.</p>

<p>If that&rsquo;s all to abstract for you, keep reading.</p>

<!-- more -->


<p><a href="https://mountainmath.ca/assessment_gl/map?zoom=15&lat=49.2672&lng=-123.1449" target="_blank"><img  src="http://doodles.mountainmath.ca/images/pt_animated.gif" style="width:50%;float:left;margin-right:10px;"></a>
To make the change in property taxes a little more transparent I have added a time slider to my
<a href="https://mountainmath.ca/assessment_gl/map?zoom=15&amp;lat=49.2672&amp;lng=-123.1449">Tax Density by Land Use Map</a> that I have
<a href="http://doodles.mountainmath.ca/blog/2016/03/02/property-taxes-and-land-use/">described previously</a>. So now people can
go back in time and see how property taxes changed and compare it to their neighbours. At the same time I
have updated the data on my <a href="">regular assessment data maps</a> to be based on the 2016 tax data, more background on the
tax data is in <a href="http://doodles.mountainmath.ca/blog/2015/05/31/density-in-vancouver/">this post</a>.</p>

<p>Check out the <a class='btn' href="https://mountainmath.ca/assessment_gl/map?zoom=14&lat=49.2814&lng=-123.1312" target="_blank">interactive map</a>.</p>

<p>This map also serves as a good reality check on the tax productivity of the land.</p>

<p>Some caveats: I am missing data for some years or some
properties, and this map aggregates property taxes for
all strata lots in a stratified property, you will have to dive into the data yourself if you want to see how it changed
on individual strata lots. Zoning and land use data stay at 2016 and don&rsquo;t animate back in time because of availability.</p>

<p>Special thanks to <a href="https://mapzen.com">Mapzen</a> for making it so ridiculously easy to make these maps and for
<a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data</a> and
<a href="http://www.metrovancouver.org/data">Metro Vancouver Open Data</a> for making that data available.</p>

<script>
function resetImages(){
    $('img').each(function(img){
        imgsrc = $(img).attr('src');
        if (imgsrc.slice(imgsrc.length-4)=='.gif') {
            $(img).attr('src', '');
            $(img).attr('src', imgsrc);

        }
    });
    setTimeout(function(){
        resetImages();
    },25000);
}
setTimeout(function(){
    resetImages();
},25000);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Character Retention]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/11/26/character-retention/"/>
    <updated>2016-11-26T20:28:45-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/11/26/character-retention</id>
    <content type="html"><![CDATA[<p>Today I went to the City of Vancouver Character Retention open house. It was quite informative, city staff were helpful and knowledgeable,
and the display board and feedback form asked many good questions. But I came away with a couple of points that I think need to be addressed further:  </p>

<ul class="indented">
<li>Faux character retention vs character design guidelines. </li>
<li>Understanding economic drivers of teardown decisions.</li>
<li>Evaluation of RT character retention policies.</li>
<li>Need to separate character retention from gentle ground-oriented density. </li>
<li>New Carrots</li>
</ul>


<h3>TL;DR</h3>

<p>It gets a little wonky, so here the very short version:</p>

<ul class="indented bordered">
<li>
Current and proposed &#8220;character retention&#8221; is hollow, just retains shell. Should be handled in design guidelines.
</li>
<li>
Real character (or heritage) retention should take closer look at underlying economic drivers to become more effective.
</li>
<li>
Gentle, ground-oriented density like 4-plexes is desperately needed in RS and RT, should be decoupled from &#8220;character retention&#8221;.
</li>
</ul>


<!-- more -->


<h3>Faux Retention  </h3>

<p><img  src="http://doodles.mountainmath.ca/images/4-plex.png" style="width:50%;float:right;margin-left:10px;"> 
The existing character retention guidelines in RT, which seem to serve as a model for the expansion of character retention 
to RS, often don&rsquo;t &ldquo;retain&rdquo; all that much. </p>

<p>What the existing character retention process entails is essentially a &ldquo;character home&#8221; 
being gutted down to the studs (in most cases), potentially moved to a corner of the property and a foundation added. Then 
the entire house, including structural elements, gets build up again and infill added to the back.  </p>

<p>The result of the process is a main house with exterior form resembling a character home, and infill in the back matching
 the style. This process is almost indistinguishable from a new built to &ldquo;character design guidelines&rdquo;. The difference in 
landfill waste is minimal (and much better addressed through recycing policies) and there is no difference to the eye, 
as can easily be seen when comparing the new built infill to the &ldquo;retained&rdquo; front house, like in the Mt Pleasant 4-plex pictured above. </p>

<h3>Economics</h3>

<p>  Character retention is hard to do. It takes incentives to make it happen. The more the character retention process 
can leverage some underlying economic drivers, the more effective it will be.</p>

<p>  A while back <a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">we ran some analysis</a>
on 11 years of property-level assessment data in the City of Vancouver that was made available 
through the open data catalogue. The upshot is that the single most important factor that predicts if a building gets
 torn down is the (assessed) value of the building alone relative to the total value of the property. We call this 
quotient the &ldquo;teardown coefficient&rdquo; and found that if it is below 5%, the building has a 1/6 chance to get torn down 
within 8 to 10 years. More details and background on this
 <a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">can be found here</a>, a refinement of this analysis is works in progress.  
Currently, 34% of the SFH building stock in Vancouver fall into that category. Often people assume that the risk of a 
building getting torn down simply depends on the building age. But the relationship is not that simple as the following graph shows.  </p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;"> 
<div id="graph_sfh" style="height:200px;max-width:640px;" data-url="/data/sfh_age.json"></div> 
<div class="legend no-margin"></div> 
</div>


<p>  </p>

<p>The blue denotes properties facing little teardown pressure, the red is for properties with high teardown pressure. 
We can see that the pre-1920 building stock holds up reasonably well. In fact, 33% of the pre-1920 building stock with
 high teardown risk. For the 1920 to 1935 stock we have 59% of buildings in teardown territory, and for 1935 to 1965
 buildings that number jumps to 78%. This makes the particular choice of the character cutoff year 1940 a bit of a head-scratcher.</p>

<p>  <a href="https://mountainmath.ca/map/assessment?filter=[sfh,teardowns]&layer=100&zoom=13&lat=49.2489&lng=-123.1081&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/teardowns2.png" style="width:50%;float:right;margin-left:10px;"></a> 
This informs the scale of the teardown pressure the building stock of a particular vintage is facing. We can similarly 
map the individual properties to understand their geographic distribution. One should note that assessment data, while 
unbiased in aggregate, can be quite off when looking at specific buildings. So when mapping for example
 <a href="https://mountainmath.ca/map/assessment?filter=[sfh,teardowns]&amp;layer=4">all single family homes with high teardown pressure</a> 
it might mis-identify some buildings as teardown candidates, or miss others. However, in aggregate the data will be
 quite robust and one can easily see that there are no obvious geographic biases in teardown risk. To quantify some of 
this, 31% of eastside SFH vs 39% of westside SFH face high teardown risk.  </p>

<p>Understanding the teardown pressure, as well as the geographic distribution and the pressures by vintage, gives an 
important baseline to the heritage and the character discussions. Houses far above the 5% threshold face little
 teardown pressure, and they need little, if any, policy protection to retain them. On the other hand, for houses
 below the teardown cutoff it makes very little economic sense to retain a building, and will require a 
lot of effort and policy protection to try and retain them. And even with these protections, there is a good chance
 that the building will eventually go or face &ldquo;demolition by neglect&rdquo;.  </p>

<p><img  src="http://doodles.mountainmath.ca/images/demo_by_neglect.png" style="width:40%;float:left;margin-right:10px;">  
One example of this is the property at 4755 Belmont Ave, which the city lists as heritage building of primary significance. </p>

<p>Effective retention policies are likely going to aim at the building stock around and somewhat above the teardown 
threshold of 5%. Ideally character retention policies should aim to strengthen the economic viability of buildings with
character merit so that they don’t fall into the range where the teardown pressure becomes overwhelming.</p>

<p>With this in mind, and setting the age cutoff to 1935 instead of 1940, we can investigate the existing pre-1935 building
 stock by teardown pressure, separating out the stock that faces 
<a href="https://mountainmath.ca/map/assessment?filter=[sfh,years_1935,tdc__0.05]&amp;layer=100&amp;zoom=13&amp;lat=49.2489&amp;lng=-123.1081&amp;mapBase=2">high teardown pressure</a> 
from the <a href="https://mountainmath.ca/map/assessment?filter=[sfh,years__1935,tdc_0.05_0.1]&amp;layer=101&amp;zoom=13&amp;lat=49.2489&amp;lng=-123.1081&amp;mapBase=2">stock with moderate teardown pressure</a> 
and the <a href="https://mountainmath.ca/map/assessment?filter=[sfh,years__1935,tdc_0.1]&amp;layer=102&amp;zoom=13&amp;lat=49.2489&amp;lng=-123.1081&amp;mapBase=2">stock with low teardown pressure</a>. 
  <a href="https://mountainmath.ca/map/assessment?filter=[sfh,years__1935]&layer=110&zoom=13&lat=49.2489&lng=-123.1148&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/teardown_pressure.png" style="width:50%;float:right;margin-left:10px;"></a> 
And we can view <a href="https://mountainmath.ca/map/assessment?filter=[sfh,years__1935]&amp;layer=110&amp;zoom=13&amp;lat=49.2489&amp;lng=-123.1148&amp;mapBase=2">all three at the same time</a>.   </p>

<p>Without taking these underlying economic factors into consideration, character (as well as heritage) retention policies are 
likely to be less effective at retention than they otherwise would be. And have broader adverse effects on buildings
 without significant heritage or character merit.</p>

<p>The city’s approach to deal with heritage retention seems to be to target all properties built before 1940,
indiscriminate of what shape the buildings are in. In the next days the consultant report the city ordered will become
available, maybe there are some nuggets in there that make this whole endeavour sound reasonable.</p>

<h3>RT </h3>

<p>The current character retention effort is entirely focused on RS.  This seems to be motivated by the fact that RT already
 has character retention guidelines. The way it works is that much of RT is downzoned, with extra density conditional on
 character retention. This seems to be the basic blueprint of the proposal for RS, so let&rsquo;s take a close look how this works.  </p>

<p><a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-6]&layer=20&zoom=16&lat=49.2587&lng=-123.1063&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/rt-6.png" style="width:50%;float:right;margin-left:10px;"></a> 
Mount Pleasant gives a good example of what this can look like. In 
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-6]&amp;layer=20&amp;zoom=16&amp;lat=49.2587&amp;lng=-123.1063&amp;mapBase=2">RT-6 zoning</a> 
we have allowed stratified 4 and 5-plexes on single family lots under the character retention policies. We have 
<a href="http://doodles.mountainmath.ca/blog/2016/08/04/rt/">looked at this before</a>, this results in the property getting sliced up and stratified into
 3 to 5 family-sized ground-oriented units. In terms of prices, the assessments pegged at July 2015 
(which became surprisingly accurate again) puts RT-6 single family lots between $1m and $6m (median $1.9m), and units in those
 multiplexes between $300k and $1.8m (median $800k).</p>

<p>  <a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-7]&layer=20&zoom=15&lat=49.2636&lng=-123.1706&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/rt-7.png" style="width:50%;float:left;margin-right:10px;"></a> 
This kind of development is great. We need more of that, much more. 
But this same model can&rsquo;t be immediately taken and expanded across the city. One look across town to 
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-7]&amp;layer=20&amp;zoom=15&amp;lat=49.2636&amp;lng=-123.1706&amp;mapBase=2">RT-7</a>, 
where similar guidelines are in place, shows that multi-plexes only appear in the pocket at 16th and Arbutus. That&#8217;s 
mainly because the Mt Pleasant model of character &ldquo;retention&rdquo; requires large lots, and most of RT-7 is on smaller lots
 that make the process of &ldquo;retention&rdquo; into multi-plexes according to current city rules unattractive. The rules need to 
be amended to take lots size (and other parameters) into account to unlock this kind of development across the city.  </p>

<p>This resulted in RT-6 only having 12% of the building stock currently facing high teardown pressure, compared to 34% of
 RT-7. In the pocket of RT-7 east of MacDonald, where lots are larger and more properties underwent the character retention
 process, only 17% of the building stock faces high teardown pressure. And yes, RT-6 started out with buildings in a
better state than RT-7. Which is another part of the reason why the character retention program was more successful in RT-6.  </p>

<p>We can try to understand better where the character retention program was utilized and where it wasn&rsquo;t by looking at 
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT,years_2000,nzone_RT-11]&amp;layer=20&amp;zoom=13&amp;lat=49.2497&amp;lng=-123.1232&amp;mapBase=2">all the properties that have been built since 2000 in RT</a>, 
keyed by whether it was a single family, duplex or multi-plex that got built. We removed RT-11 from the map, it is too
 new to be meaningful. It&rsquo;s amazing to me how in some pockets almost no multi-plexes get built. We should try and fine-tune 
this process before transplanting it to RS.  RT-11 offers a cautionary tale. Looking at what got
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-11,years_2000]&amp;layer=20&amp;zoom=15&amp;lat=49.2393&amp;lng=-123.0509&amp;mapBase=2">built in current RT-11 after 2000</a> 
we see lots of single family homes. A huge missed opportunity. If we had upzoned that area a decade earlier we could 
have allowed for homes that suit our families much better. </p>

<p>In summary, we should look carefully at how character retention performed in RT and going forward devise policies for RS and RT.
It’s time to drop that artificial divide that has been overtaken by the reality that RS already allows 3 units on each property, more than RT.</p>

<h3>Hostage Taking</h3>

<p>The display boards suggests that the current plan for &ldquo;character retention&rdquo; in RS is essentailly to copy over the RT
blueprint with some fine-tuning. Downzone RS and give density for undergoing &ldquo;character retention&rdquo;.</p>

<p>  In essence that&rsquo;s holding gentle, ground-oriented density hostage to character retention, and it&rsquo;s a terrible thing to do. Whatever the 
question is, downzoning can&rsquo;t be part of the answer any more. Vancouver is starved for gentle, sensible, ground-oriented 
family-friendly development.</p>

<p>  Originally, when the character retention guidelines for RT were made, this kind of gentle density came in the Trojan horse 
of character retention. And even if much of that was faux character, I don&rsquo;t mind. I take that kind of density any way I can.  
But we are quite a bit further along now. Unaffordability skyrocketed. Families are starved for housing options. </p>

<p>We need to scale this process up. And just expanding  the area where we deploy it to also cover RS won&rsquo;t do the trick.
We average about 10 RT character retention projects 
a year. Out of 11k RT properties. If we scale that up to all 68k RS properties, we will increase that number to about 70 
a year. Just for perspective to see how inadequate that is, we tear down around 1000 single family homes a year. 
And replace them with single family homes.  </p>

<p>What we need is a program that brings this type of density to RS and RT independent of the character retention program.
And let’s drop the &ldquo;retention&rdquo; pretence. If the character home look is what Vancouverites want in return for gentle
density, then let&rsquo;s prescribe the exterior look of new-builts and let the design review process  handle it.  </p>

<h3>New Carrots  </h3>

<p>If density won&rsquo;t be the main carrot (or a stick) for character retention, than what can pull up the slack? Property taxes is an obvious one.
 The owner could be allowed to re-claim property taxes against improvements and maintenance that aim to maintain or
 underline the character merit of the home. And these re-claimed taxes become cumulatively payable, with interest, in the
 event of demolition. Of course this would raise everyone else&rsquo;s property taxes, but I think that&rsquo;s only fair in return
 for the community dictating character-homeowners the exterior look of their home.</p>

<p>  The idea behind this is that this aims to directly strengthen the economic viability of the building, thus removing the 
economic drivers that favour tearing down the building. And over time accumulating a penalty that dissuades from
 tearing down the building.  </p>

<p>Density can still be part of the mix, but not in a way that it precludes smart gentle density to be built without
 it. The reality is that much of our building stock will go, the economic drivers are just too strong. And we all know that 
we should not replace those single family homes with yet another single family home that at best the top 5% income households 
can afford. We should allow these houses to be replaced with ground-oriented units a much larger portion of Vancouver 
families can afford.  </p>

<p>Smarter people than me have probably though about this for quite some time now. I would love to hear more ideas how 
character retention can be structured so that it does not get in the way of gentle density for new builts.   </p>

<script>

function stacked_bar_graph(div,shiftAxis,domainFormatter,rangeFormatter,domainLabelFormatter){
    if (!domainFormatter) domainFormatter=d3.format("d")
    if (!rangeFormatter)
     rangeFormatter = function (y) {
        return y;
     };
     if (!domainLabelFormatter) domainLabelFormatter=domainFormatter;

var margin = {top: 20, right: 20, bottom: 40, left: 70},
    width = parseInt(div.style("width")) - margin.left - margin.right,
    height = parseInt(div.style("height")) - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);


var xAxis = d3.svg.axis()
    .scale(x)
    .tickFormat(domainFormatter)
    .orient("bottom");


var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .tickFormat(rangeFormatter)
    .ticks(5, rangeFormatter);

var svg = div.append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var data_url=div[0][0].dataset.url;
var legend=d3.select(div.node().parentNode).select('.legend');


d3.json(data_url, function(error, json) {
  if (error) throw error;
  var graphData=json[0];
  var data=graphData.data;
  var color = d3.scale.ordinal().domain(graphData.colors.map(function(d,i){return i}))
  .range(graphData.colors);
  var domain=data.map(function(d){return d.date;});
  x.domain(domain);

  function graphValueId(i){
      return graphData.class + '_' + i + '_value'
  }

  graphData.labels.forEach(function(text,i){
    var color=graphData.colors[i];
    var html='<i style="background:' + color + '"></i> ' + text + ' <span style="float:right;margin-right:10px;" id="' + graphValueId(i) + '"></span>'
    legend.append('p').html(html);
  });

  data.forEach(function(d) {
      var y0 = 0;
      d.values = color.domain().map(function(i) { return {date: d.date, y0: y0, y1: y0 += +d.count[i]}; });
      d.total = d.values[d.values.length - 1].y1;
  });
  y.domain([0, d3.max(data, function(d) { return d.total; })]);

  var domainTickValues=[];
  var skip=Math.round(40/x.rangeBand());
  if (skip<=0) skip=1;
  for (var i=0;i<x.domain().length;i++) {
    if (i % skip==0) domainTickValues.push(x.domain()[i]);
  }
  if (x.domain().length % 5 !=0) domainTickValues.push(x.domain()[x.domain().length-1]);
  xAxis.tickValues(domainTickValues);

  var xShift=shiftAxis ?  x.rangeBand()/2.0 * 1.1 : 0;

  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(" + xShift + "," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis);
//    .append("text")
//      .attr("transform", "rotate(-90)")
//      .attr("y", 6)
//      .attr("dy", ".71em")
//      .style("text-anchor", "end")
//      .text("Probability");

    function updateTooltip(d,i){
       color.domain().forEach(function(j){
             var value=d && i==j ? (domainLabelFormatter(d.date) + ': ' +rangeFormatter(d.y1-d.y0)) : '';
             d3.select('#'+graphValueId(j)).text( value);
       });
    }

  var year=svg.selectAll(".year")
    .data(data)
        .enter().append("g")
          .attr("class", "g");
  year.selectAll(".color-bar")
      .data(function(d) { return d.values; })
    .enter().append("rect")
      .attr("class", graphData.class + " color-bar")
      .attr("fill", graphData.color)
      .attr("x", function(d) { return x(d.date); })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.y1); })
      .attr("height", function(d) { return Math.max(0, y(d.y0) - y(d.y1)); })
      .attr("fill",function(d,i) {return color(i);})
      .on('mouseover',updateTooltip)
      .on('click',updateTooltip)
      .on('touch',updateTooltip)
      .on('mouseout',function(){updateTooltip(null,i)});


});

}


var priceFormatter2=d3.format("$,");
    var priceFormatter = function (y) {
        return y>=1000000 ? (priceFormatter2(y/1000000) + 'm') : (priceFormatter2(y/1000) + 'k');
    };
    var brackets=[100000,200000,300000,400000,500000,600000,700000,800000,900000,1000000,2000000,10000000,20000000,40000000]

var binFormatter=function(top){
    var bottom=0;
    if (top<=1000000) bottom=top-100000;
    else if (top==2000000) bottom= 1000000;
    else if (top==10000000) bottom= 2000000;
    else if (top=20000000) bottom= 10000000;
    else bottom=20000000;
    return priceFormatter(bottom) + ' - ' + priceFormatter(top);
}
var numberFormatter=d3.format(",");
var numberBinFormatter=function(top){
    var     bins=[0,1,2,4,8,10,16,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,300,400,500,700];
    var index=0;
    while (bins[index]<top && index<bins.length) index ++;
    bottom=bins[index-1]+1;
    return (bottom == top) ? numberFormatter(bottom) : numberFormatter(bottom) + ' - ' + numberFormatter(top);
}
stacked_bar_graph(d3.select("#graph_sfh"));
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What's up with RT?]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/08/04/rt/"/>
    <updated>2016-08-04T13:18:51-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/08/04/rt</id>
    <content type="html"><![CDATA[<p>RT is Vancouver&rsquo;s zoning for duplexes. Over time, various areas have been zoned to allow duplexes. Examples are
Kits Point, much of Point Grey Road reaching up to Broadway, much of Granview-Woodlands, parts of Mount Pleasant and many
other areas.</p>

<p><a href="https://mountainmath.ca/map/assessment?filter=[zone_RT,residential]&zoom=13&lat=49.245&lng=-123.1166&layer=4&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/rt_teardowns.png" style="width:50%;float:right;margin-left:10px;"></a>
Recently I have had some interesting conversations <a href="https://twitter.com/fixbced/status/748196970783637504">on Twitter regarding RM-6</a>
and over BBQ dinner about RT-7. Then the
<a href="http://vancouver.ca/home-property-development/grandview-woodland-community-plan.aspx">Granview-Woodland plan</a>
passed by council, and it contains a curious provision of reducing the outright FSR for the
RT-zoned properties from 0.6 to 0.5.</p>

<p>All of which got me thinking. What is RT supposed to accomplish, how does the diverse RT-zoning rules influence development
and how is RT overall performing?</p>

<!-- more -->


<p>To make things complicated many RT zoning rules contain provisions to allow for higher FSR, and more than two units,
under the &ldquo;heritage preservation&rdquo; program. Most people like the multi-plex developments that come out of this process,
but the road there is quite adurous. Permitting takes a long time and is marred by uncertainty. Those projects are
done by small developers, that are ill-equipped to deal with risky and drawn-out rezoning processes.</p>

<p>These would be great projects carried out by small developers that lead to gentle and affordable density,
as this kind of re-development does not require land-assembly and does not lead
to crazy land-value lifts that require CACs to claw back some of that value rise
<a href="http://doodles.mountainmath.ca/blog/2016/04/01/on-dirt-and-houses/">as we explained before</a>.</p>

<p>We have some ideas what a refreshed RT (and RS) should look like, and there are
<a href="https://pricetags.wordpress.com/2016/07/29/open-letter-on-the-downzoning-of-grandview-woodland-rt/">some efforts to push for changes</a>.
We leave the details of this to the experts and take a little field expedition to
see how the different flavours of RT (and RM) perform in data.</p>

<h3>Why RT?</h3>

<p>One central question to ask is: What is the purpose of RT (or any) zoning? The initial idea was to allow higher density
than RS, with two stratified properties on one lot. But since the creation of RT the RS zoning has undergone significant
changes and allows, in many cases, for higher FSR and higher unit count, although not stratified. In much of RS three
units are allowed on each lot, the main house, a secondary suite and a laneway house.</p>

<p><a href="https://mountainmath.ca/map/assessment?filter=[zone_RT_RS_FSD,residential]&zoom=13&lat=49.245&lng=-123.1166&layer=5&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/rs_rt_land.png" style="width:50%;float:left;margin-right:10px;"></a>
So how does RT perform in the wild? There seems to be little effect RT vs RS zoning has on land values. The
<a href="http://mountainmath.ca/map/assessment?filter=[zone_RT_RS_FSD,residential]&amp;zoom=13&amp;lat=49.245&amp;lng=-123.1166&amp;layer=6&amp;mapBase=2">boundaries between RS and RT</a>
can&rsquo;t be discerned from a
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT_RS_FSD,residential]&amp;zoom=13&amp;lat=49.245&amp;lng=-123.1166&amp;layer=5&amp;mapBase=2">land value map of all residential properties in RS, RT (and First Shaugnessey) zones</a>.</p>

<p>How good are RT lots at realizing &ldquo;zone capacity&rdquo;, that is how many of the historically SFH get turned over into duplexes,
or in some cases multi-plexes (through the heritage presercation program)?
That depends as the following graph shows.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_sfh" style="height:200px;max-width:640px;" data-url="/data/zone_sfh.json"></div>
</div>


<p><a href="https://mountainmath.ca/map/assessment?filter=[zone_RT]&zoom=13&lat=49.2581&lng=-123.1166&layer=20&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/types_rt.png" style="width:50%;float:left;margin-right:10px;"></a>
We can also study this by mapping out
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT]&amp;zoom=13&amp;lat=49.2581&amp;lng=-123.1166&amp;layer=20&amp;mapBase=2">all residential lots in RT zones</a>,
and color them depending if they are single family lots, duplexes or multi-plexes.</p>

<p>Some RT areas are essentially indistinguishable from RS areas, they are almost entirely comprised of single family lots.
A good example is the
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-4,residential]&amp;zoom=14&amp;lat=49.2733&amp;lng=-123.0646&amp;layer=20&amp;mapBase=2">RT-4, most of which is in Grandview-Woodlands</a>.
And being in RT rathern than RS they often don&rsquo;t even get the benefits of a laneway house. Coach houses, which are legal
in many RT areas and could be stratified, come with such onerous side setbacks that they are very hard to build on
regular lots.</p>

<p>There is a lot of untapped &ldquo;zoned capacity&rdquo;. Similarly, some of the
newer RM zones show little appetite for realizing &ldquo;zoned capacity&rdquo;, possibly due to the added difficulty of having to
assemble lots to do so.</p>

<p>One can argue about whether this is good or bad. Or one can look at the variation between the different types of RT and RM
zones to see what causes the difference.</p>

<h3>Downzoning</h3>

<p>To understand how downzoning works we want to consider two different types on analysis. One is to look at what happened
after RS zone was changed to reduce site coverage in the 1980 in response to concerns about &lsquo;monster homes&rsquo;. We ran
an analysis using LIDAR data to understand the effect on the physical form of SFH and found that while site coverage
indeed decreased sharply, the <a href="http://doodles.mountainmath.ca/blog/2016/03/05/physical-sfh-form-over-time/">bulk of the building remained unchanged</a>.
and the rule change seems to have done little in easing concerns about &lsquo;monster homes&rsquo;, it simply changed some of the
parameters determining what &lsquo;monster homes&rsquo; look like.</p>

<p>Another way to look at the matter is to compare how re-development happened in difference zones. This is very hard, since
there are many factors other than zoning that determine if a building gets re-developed. To get some idea we compiled two
numbers for each of the RS, RT and RM zones (ignoring some of the delicacies in the zoning code and lumping together all
sub-zones with the same leading number. So for example we lumped RT-4, RT-4A, RT-4N and RT-4AN into one category we simply
label &ldquo;RT-4&rdquo;.</p>

<p>First we check how many residential properties got re-developed since 2000.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_redevelopment" style="height:200px;max-width:640px;" data-url="/data/zone_redevelopment.json"></div>
</div>


<p>Next we take a look what percentage of the existing residential stock is in immediate danger of being torn down, using the
<a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">methodology developed earlier</a>.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_teardown" style="height:200px;max-width:640px;" data-url="/data/zone_teardowns.json"></div>
</div>


<p>While there is some correspondence between re-development activity and ratio of teardowns, there are a number of notable
exceptions. RT-1 and RM-9 stand out, but these are
<a href="https://mountainmath.ca/map/assessment?filter=[zoneE_RM-9_RT-1]&amp;zoom=14&amp;lat=49.2146&amp;lng=-123.1282&amp;layer=5&amp;mapBase=2">oddball cases with just a few properties</a>.</p>

<p><a href="https://mountainmath.ca/map/assessment?filter=[zone_RT,years_2000]&zoom=13&lat=49.2581&lng=-123.1166&layer=20&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/new_types_rt.png" style="width:50%;float:right;margin-left:10px;"></a>
We can also map just <a href="https://mountainmath.ca/map/assessment?filter=[zone_RT,years_2000]&amp;zoom=13&amp;lat=49.2581&amp;lng=-123.1166&amp;layer=20&amp;mapBase=2">residential properties in RT that were re-developed since 2000</a>
and colour them by SFH, Duplex and Multi-plex status. We can clearly see how some RT areas, like RT-7, don&rsquo;t see much
re-development, others, like RT-6 see mostly multi-plexes being developed. Some zones, like RT-10, 11, 12 are too new
to be judged on development happening since 2000. For example, zooming in on
<a href="http://mountainmath.ca/map/assessment?filter=[zone_RT-10,residential,years_2009]&amp;zoom=15&amp;lat=49.2485&amp;lng=-123.0734&amp;layer=20&amp;mapBase=2">developments in RT-10 since 2009 paints a very different picture</a>.</p>

<p>There is lots of stuff to explore, but for today I want to look at RT-7 and RT-6 in more detail.</p>

<h4>RT-7</h4>

<p><a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-7,residential]&zoom=15&lat=49.2617&lng=-123.1654&layer=20&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/rt-7.png" style="width:50%;float:left;margin-right:10px;"></a>
A much more interesting case is RT-7, with 35% of properties teardowns but only 4.1% of properties re-developed since 2000.
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-7,residential]&amp;zoom=15&amp;lat=49.2617&amp;lng=-123.1654&amp;layer=4&amp;mapBase=2">Looking at the map</a>
we see that RT-7 is comprised of two pockets on the west side, and the teardown candidates stand out in red and orange.
Filtering further to see what
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-7,residential,years_2000]&amp;zoom=15&amp;lat=49.2617&amp;lng=-123.1654&amp;layer=20&amp;mapBase=2">has been re-developed since 2000</a>
we see that, considering the size of the two pockets, re-development favours the eastern pocket near 16th and Arbutus. We
can clearly see that that the eastern pocket has an overall more valuable building stock, although it is not clear if that
is due to higher rates of re-development or for other reasons, for example the presence of some slightly larger lots. We
also note that re-development does produce duplex units and even some multi-plex.</p>

<p>So what is going on here? Looking at the <a href="http://former.vancouver.ca/commsvcs/BYLAWS/zoning/rt-7.pdf">RT-7 zoning</a> we
see that the area has been downzoned to 0.4 FSR as was rencently pointed out to me, which can conditionally be upzoned
to 0.6 FSR. Additionally, there are caps on the number of units per hectar to further restrict density.</p>

<h4>RT-6</h4>

<p><a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-6,residential]&zoom=16&lat=49.2606&lng=-123.1097&layer=20&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/rt-6.png" style="width:50%;float:right;margin-left:10px;"></a>
Another interesting example is RT-6. RT-6 has seen modest levels of re-develpment and has a <a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-6,residential]&amp;zoom=16&amp;lat=49.2606&amp;lng=-123.1097&amp;layer=4&amp;mapBase=2">relatively healthy building
stock</a>.
What&rsquo;s even more interesting is that it contains
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-6,residential]&amp;zoom=16&amp;lat=49.2597&amp;lng=-123.1101&amp;layer=20&amp;mapBase=2">many multi-plexes</a>.
And more importantly, <a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-6,residential,years_2000]&amp;zoom=16&amp;lat=49.2597&amp;lng=-123.1101&amp;layer=20&amp;mapBase=2">developments since 2000 have mostly produced multi-plexes</a>.</p>

<p>Prime examples of the <em>gentle density</em> that we keep hearing about. I would say that something is working very well here.</p>

<p>So what&rsquo;s the difference? In contrast to RT-7 <a href="http://former.vancouver.ca/commsvcs/BYLAWS/zoning/rt-6.pdf">RT-6 zoning</a> allows for 0.6 FSR
outright, conditionally increased to 0.75. But there are other important differences too. The RT-7 lots are typically
smaller with many at about 340m&sup2; compared to the 580m&sup2; typical RT-6 lots.
(Although RT-7 also contains some 580m&sup2; lots).</p>

<h3>Conclusion</h3>

<p>Does downzoning work? It depends what the goal is. Looking at the RT-7 example, downzoning has slowed re-development compared
to other areas in the city, but it also lead to a deterioration in building stock in RT-7. This is a stop-gap measure,
eventually that lower value building stock will get re-developed.</p>

<p>The combination of larger lots and the heritage retention program in the RT-6 zoning seems to work in producing
gentle density, except that the permitting process takes too long and it is not immediately clear what purpose the
heritage preservation has that allows stripping the building down to the studs and possibly moving it to the front and
then building an infill in the back. There must be a better way to deal with concerns behind
heritage preservation (not much is &ldquo;preserved&rdquo; in this process) and at the same time cut down on the time it
takes to jump through all the permit hoops involved.</p>

<p>It is clear that RS and RT (and RM) zoning would hugely profit from a clearer vision what these zonings should accomplish
and by using data to benchmark how these targets are met.</p>

<div><script>

function bar_graph(div,shiftAxis,domainFormatter,rangeFormatter,domainLabelFormatter,rangeLabelFormatter){
    if (!domainFormatter) domainFormatter=d3.format("d");
    if (!rangeLabelFormatter) rangeLabelFormatter=rangeFormatter;
    if (!rangeFormatter)
     rangeFormatter = function (y) {
        return y;
     };
     if (!domainLabelFormatter) domainLabelFormatter=domainFormatter;

var margin = {top: 20, right: 20, bottom: 40, left: 70},
    width = parseInt(div.style("width")) - margin.left - margin.right,
    height = parseInt(div.style("height")) - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);


var xAxis = d3.svg.axis()
    .scale(x)
    .tickFormat(domainFormatter)
    .orient("bottom");


var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .tickFormat(rangeFormatter)
    .ticks(5, rangeFormatter);

var svg = div.append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var data_url=div[0][0].dataset.url;

d3.json(data_url, function(error, json) {
  if (error) throw error;
  var graphData=json[0];
  var data=graphData.data;
  
  var container=d3.select(div.node().parentNode);
  container.selectAll('.legend.no-margin').remove();
  var legend=container.append('div').attr('class',"legend no-margin");
  legend.append('p').html('<i style="background:'+graphData.color + '"></i>' + graphData.label +  '<span style="float:right;margin-right:10px;" id="' + graphData.class+'_value"></span>');
  
  x.domain(data.map(function(d) { return d.date }));
  y.domain([0, d3.max(data, function(d) { return d.count; })]);
  
  var domainTickValues=[];
  var skip=Math.round(40/x.rangeBand());
  if (skip<=0) skip=1;
  for (var i=0;i<x.domain().length;i++) {
    if (i % skip==0) domainTickValues.push(x.domain()[i]);
  }
  if (x.domain().length % 5 !=0) domainTickValues.push(x.domain()[x.domain().length-1]);
  xAxis.tickValues(domainTickValues);

  var xShift=shiftAxis ?  x.rangeBand()/2.0 * 1.1 : 0;
  
  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(" + xShift + "," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis);
//    .append("text")
//      .attr("transform", "rotate(-90)")
//      .attr("y", 6)
//      .attr("dy", ".71em")
//      .style("text-anchor", "end")
//      .text("Probability");

  svg.selectAll(".bar")
      .data(data)
    .enter().append("rect")
      .attr("class", graphData.class + " bar")
      .style("fill", graphData.color)
      .attr("x", function(d) { return x(d.date); })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.count); })
      .attr("height", function(d) { return height - y(d.count); })
      .on('mouseover',function(d){
         d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeLabelFormatter(d.count)) 
      }).on('click',function(d){
       d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeLabelFormatter(d.count)) 
      }).on('touch',function(d){
         d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeLabelFormatter(d.count)) 
      }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});

      
});

}



var percentageFormatter=d3.format(".1%");
var textFormatter=function(d){return d};
var teardownLabelFormatter=function(d){return percentageFormatter(d) + " teardowns"};
var sfhLabelFormatter=function(d){return percentageFormatter(d) + " SFH"};
var redeveopmentLabelFormatter=function(d){return percentageFormatter(d) + " built since 2000"};
bar_graph(d3.select("#graph_redevelopment"),false,textFormatter,percentageFormatter,textFormatter,redeveopmentLabelFormatter);
bar_graph(d3.select("#graph_teardown"),false,textFormatter,percentageFormatter,textFormatter,teardownLabelFormatter);
bar_graph(d3.select("#graph_sfh"),false,textFormatter,percentageFormatter,textFormatter,sfhLabelFormatter);
</script></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mixing Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/07/06/mixing-data/"/>
    <updated>2016-07-06T09:19:04-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/07/06/mixing-data</id>
    <content type="html"><![CDATA[<p>Census data is very rich and with <a href="https://censusmapper.ca">CensusMapper</a> we now have about 1 billion data points right at
our fingertips. And we have managed to open up some of our interfaces for everyone to <a href="http://doodles.mountainmath.ca/blog/2016/05/04/census-mapping-for-everyone/">explore data and make their own maps
and freely share them</a>.</p>

<p>Things really get interesting when one mixes custom data with census data. While, at this point, these are not part of
the freely available CensusMapper functionality we still wanted to share what can be done.</p>

<p>At CensusMapper we have developed three basic ways to rapidly mix custom data with census data. So this is really three
blog posts in one.</p>

<!-- more -->


<h3>1. Overlay Mapping</h3>

<p>First up, advanced users can upload custom datasets and map them on top of census data.
<a href="http://doodles.mountainmath.ca/images/restaurants_2.png"><img  src="http://doodles.mountainmath.ca/images/restaurants_2.png" style="width:50%;float:right;margin-left:10px;"></a>
For example, we took a
<a href="http://data.vancouver.ca/datacatalogue/businessLicence.htm">business license dataset</a> from the City of Vancouver open
data catalogue, filtered it by the <em>BusinessType</em> field to only include businesses starting with &ldquo;Restaurant&rdquo; or
&ldquo;Liquor Establishment&rdquo; and uploaded them to CensusMapper to map it on top of census data. We have used a map
for <em>median age</em> that was <a href="https://twitter.com/jofu_/status/750564269796823041">recently created using CensusMapper&rsquo;s free public interface</a>
(have you made <a href="https://censusmapper.ca/maps/new">your own CensusMapper map</a> yet?), we have faded out areas outside of
Vancouver and coloured <em>Liquor Establishments</em> in red and <em>Restaurants</em> in blue.</p>

<p>Giving a visual impression of your own dataset in relation to census data is the first step to location analysis.</p>

<h3>2. Populate Custom Data with Census Data</h3>

<p>Next
up is to populate your own dataset with census data for further analysis. For the restaurants, we may be interested in
attaching population estimates in 5 minute walking distance from each location. We might also be interested in specific
age brackets, or numbers of recent immigrants
from specific countries that we may want to target with a new restaurant, or maybe even     income data.</p>

<p>This can be a time consuming and painful process,
but we have automated this at CensusMapper.</p>

<p>To showcase how this works we will show an example using elementary school catchment areas in the City of Vancouver. The
areas we have (from the <a href="http://data.vancouver.ca/datacatalogue/publicPlaces.htm">Vancouver Open Data Catalogue</a>) are
quite out of date, but for the purpose of this example they work as we will compare them to 2011 census data. We will look at the
<a href="https://censusmapper.ca/maps/419">school aged population</a> in each catchment.</p>

<!--
The general census release data does not fit our task perfectly, the census splits by age in early May 2011 and not by
year born and it does not have fine enough age brackets to estimating accurate school catchment numbers.
[BC Stats](http://www.bcstats.gov.bc.ca/StatisticsBySubject/Demography/PopulationEstimates.aspx) has finer age brackets
computed to school district (not catchment) boundaries, but their estimates are ridiculously far off of census numbers
when using matching age brackets and years that their usefulness is highly questionable. 
-->


<p><a href="http://doodles.mountainmath.ca/images/e-schools.png"><img  src="http://doodles.mountainmath.ca/images/e-schools.png" style="width:50%;float:left;margin-right:10px;"></a>
Using the visual overlay we notice that the catchment areas do fit boundaries of Dissemination Areas shown on the map
reasonably well, with some exceptions. The same cannot be said for Census Tracts, we can be reasonably confident that
Dissemination Area data is fairly accurate.</p>

<p>Next we use the built-in CensusMapper functionality to automatically populate the catchment areas with the census
data we are interested
in. When Dissemination Areas don&rsquo;t line up with the catchment boundaries we go down to Dissemination Blocks to estimate
how many children live on what side of the catchment boundary. We
<a href="http://doodles.mountainmath.ca/blog/2016/04/06/tod/">previously explained this process in more detail</a>, the result is
a spreadsheet with the population data by age for each catchment area.</p>

<div id='schools'></div>


<div><script src="http://doodles.mountainmath.ca/javascripts/colorbrewer.js" charset="utf-8"></script></div>


<div><script src="http://doodles.mountainmath.ca/javascripts/school_bar_graph.js" charset="utf-8"></script></div>


<p>Here we show the results by school, the select element can be used to select any school of interest. The whole process
of populating the school data with census data just required uploading the catchment boundaries and selecting which
variables to attach.</p>

<h3>3. Custom Census Data Mapping</h3>

<p>Sometimes it is not practical to map custom data on CensusMapper. Maybe the custom data is too sensitive to be uploaded
to CensusMapper servers. Or it is quite complex and is better mapped separately. So we created an API to pull in
census data from CensusMapper to easily show census data on custom maps. And dynamically mix in your own data. As an
example we mix census data with data from BCAssessment, again obtained through the
<a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data Catalogue</a> (and enriched with
<a href="http://www.metrovancouver.org/data">open data from Metro Vancouver</a>).
For demonstration purposes take Dissemination Area
geographies and Dwelling Characteristic data from CensusMapper and mash it up with our
<a href="http://mountainmath.ca/map/assessment?zoom=14&amp;lat=49.2604&amp;lng=-123.1417&amp;layer=14&amp;mapBase=2">processed assessment data</a>
to explore the <a href="http://doodles.mountainmath.ca/blog/2016/06/17/sdh-zoning-and-land-use/">differences in how single family properties are classified</a>.</p>

<p><a href="https://mountainmath.ca/census_mix/map?mapBase=2&layer=0" target="_blank"><img  src="http://doodles.mountainmath.ca/images/sfh_unit_count.png" style="width:50%;float:right;margin-left:10px;"></a>
For example we can <a href="https://mountainmath.ca/census_mix/map?mapBase=2&amp;layer=0">compare BC Assessment single family lot count to Stats Canada unit count</a>
in the dissemination areas
that are exclusively made up of single family lots. This gives an indication of how many suites and laneway houses there
are in those areas. The census is prone to undercount units, but still does a better job at estimating them than other data sources,
like the city or BC Assessment.</p>

<p>One of the reasons why census unit counts come up higher than other methods is that the census also counts illegal units,
which naturally are not part of other official government counts. There are different reasons why a suite may be illegal:
<a href="https://mountainmath.ca/census_mix/map?mapBase=2&layer=5" target="_blank"><img  src="http://doodles.mountainmath.ca/images/illegal_units.png" style="width:50%;float:left;margin-right:10px;"></a>
It could simply be that the owner has not made the effort to register it. Or the suite may not be up to code.
And in some cases, a property may have more than one suite in the main building, which is illegal in the City of Vancouver.
The latter ones we can pick out in census data, since a house with two secondary suites &ndash; so three dwelling units in one building in
total &ndash; is classified as being an &ldquo;Apartment, building that has fewer than five storeys&rdquo;. So, in census dissemination areas that
only have duplex or single family lots as residential land uses based on assessment and land use data, we can look for
how many dwelling units the census places in an &ldquo;Apartment, building that has fewer than five storeys&rdquo;. And
<a href="https://mountainmath.ca/census_mix/map?mapBase=2&amp;layer=5">map them</a>.</p>
]]></content>
  </entry>
  
</feed>
