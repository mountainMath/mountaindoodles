<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Mountain Doodles]]></title>
  <link href="http://doodles.mountainmath.ca/feed.xml" rel="self"/>
  <link href="http://doodles.mountainmath.ca/"/>
  <updated>2017-08-06T22:44:49-07:00</updated>
  <id>http://doodles.mountainmath.ca/</id>
  <author>
    <name><![CDATA[MountainMath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Millennials Redux]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/08/06/millennials-redux/"/>
    <updated>2017-08-06T20:54:48-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/08/06/millennials-redux</id>
    <content type="html"><![CDATA[<p>Catching up with my local news reading last night I stumbled about another
<a href="https://beta.theglobeandmail.com/real-estate/vancouver/priced-out-of-downtown-vancouver-millennials-are-building-hipsturbia/article35884038/">new report on millennials</a>.</p>

<p>The notion that millennials are fleeing Vancouver is a recurring theme in the Vancouver press, and we have addressed
some of the <a href="http://doodles.mountainmath.ca/blog/2017/05/16/lifeblood/">problems in the data used to support that claim before</a>.</p>

<p>Sadly, this new article&rsquo;s use of data is no less problematic, and the topic, as well as the data misrepresentations, are serious enough that
I felt they need addressing so as not do distract from the actual real problems that millennials are facing. Problems that are quite
different from those the 25 to 39 year old age cohort was facing 20 years ago. Groups like Generation Squeeze
have done a good job nailing some of that down in the data.</p>

<h3>The Data Rabbit Hole Trap</h3>

<p>To the data-minded person reading the article there are a number of red flags that go off throughout. Many of these can
be attributed to today&rsquo;s typical data-adverse journalism, but typically the actual hard numbers in the article hold up and are just
misrepresented to varying degrees. What got me stumbling in this article was the data chart at the bottom claiming that the 25 to 39 year old age cohort
in the &ldquo;UEL&rdquo; grew by 5% between 1996 and 2016. The UEL of course is a quasi-municipality that sits wedged between the City of Vancouver and UBC,
but many people less attuned to data and administrative details use the term to refer to various portions of the region west of the Pacific Spirit Park,
sometimes including &ldquo;Little Australia&rdquo; which is west of the Park but an actual part of the UEL and sometimes excluding it.</p>

<p>I took it to mean some version of UEL and UBC/UNA combined, and the 5% number looked suspicious to me. The population in that area more than tripled
during that period, one would expect the change in that age cohort to be much larger. So I started to dig into the numbers.</p>

<p>The first step was to look up the numbers for the City of Vancouver since there are no issues with administrative boundaries between 1996 and 2016,
just to make sure that the data was labelled correctly and it was really representing the percentage change in the number of millennials between 1996 and 2016.
But the number I got was different from the one in the article. The article lists a 10% increase, I calculated an 11% increase. 11.2% to be precise, so
there was no chance that this was a rounding issue.</p>

<p>And the data rabbit hole opens up, sucks me in and the trap closes.</p>

<h3>The Data</h3>

<p>The data is, for the most part, reasonably straight forward. I just grabbed the 1996 count og 25 to 39 year olds, then the 2016 counts and compared them.
One problem is boundary changes. Administrative boundaries don&rsquo;t stay fixed. And boundary changes don&rsquo;t always show up when just looking at non-geographic
data, names or even the uniqe geographic identifiers don&rsquo;t necessarily change when census boundaries change. Looking that the geographic data for both
censuses one immediately notices that the UEL/UBC/UNA area changed a lot (and also got new geographic identifiers), and Coquitlam changed too.
That complicates things a little, the UEL/UBC/UNA part is easy enough to deal with. In 1996 that area was called the &ldquo;University Endowment Area&rdquo;, in 2016
that same area can be obtained by adding to census tracts. Coquitlam is a little trickier and I wasn&rsquo;t interested enough in figuring out the details so
I decided to ignore it.</p>

<p>Step one, trying to reproduce the graph in the newspaper, is below with blue bars, with the graph from the newspaper in red bars for reference.</p>

<div class="half-image"><a href="http://doodles.mountainmath.ca/images/millennial_grap_1.png"><img src="http://doodles.mountainmath.ca/images/millennial_grap_1.png" ><p>News Story</p></a></div>


<div class="half-image"><a href="http://doodles.mountainmath.ca/images/millennial_grap_2.png"><img src="http://doodles.mountainmath.ca/images/millennial_grap_2.png" ><p>Actual Data</p></a></div>


<p>There is definitely a correspondence between the graphs, but the numbers don&rsquo;t quite match up. I have no idea how the &ldquo;UEL&rdquo; numbers were derived for
the article. But I have an explanation for the difference in the other municipality&rsquo;s numbers. Looking at the graphs
suggests that a larger denominator was used in the article, and indeed the numbers match up perfectly if I were to divide the
difference of population in the 25-39 year
old cohort by the 2016 number instead of the 1996 number. An embarrassing data mistake to be sure, but nothing out of the ordinary for today&rsquo;s
news stories.</p>

<p>I can&rsquo;t explain why Surrey, the city with the largest gain in millennials, was dropped from the data used for the story.</p>

<h3>Data Representation</h3>

<p>But these data problems are really only a side show to the real issue.
The most important question is what data to use for what purpose. The article chose to use the change in the total number of millennials
to support the notion that the 25 to 39 year old cohort are shunning the City of Vancouver for some of the more outlying regions.
The obvious issue with that
is that that measure is confounded by population growth. If the population is growing, so will the number of millennials,
even if the share of millennials
in the population did not change. For this story, this is clearly a very poor choice of data representation.</p>

<p>As a first approximation to understanding where that age cohort settles in 2016 compared to 1996 one can look at the respectives shares of the
population in those age cohorts. The only problem, the pretext of the story goes away when one represents the data in this way.
<a href="http://doodles.mountainmath.ca/images/millennial_grap_3.png"><img  src="http://doodles.mountainmath.ca/images/millennial_grap_3.png" style="width:50%;float:right;margin-left:10px;"></a></p>

<p>What stands out is that the share of 25-39 year olds dropped in all areas. Some of that is just part of the
changing makeup of the population in general. And
one sees that the City of Vancouver not only has the highest share of 25-39 year olds, it also experienced the lowest drop.</p>

<h3>Framing of the Data</h3>

<p>The other part of the story that irks me is the deep confusion and free mixing of two different concepts. One is that of migration, that is
(the same) people moving from one area to another over some time period. The other is that of the number or share of (different)
people in a secific age cohort at two distinct points in time. Nathan Lauster has added some very
<a href="https://homefreesociology.wordpress.com/2016/02/12/is-the-lifeblood-of-vancouver-leaving/">good analysis</a> to this topic, and has
followed up with a series of blog posts. And this was picked up in various news articles too.</p>

<p>This article not only lacks appreciation for this important distinction by talking about &ldquo;migration&rdquo; when really comparing age cohorts, but
it takes it to the next level by talking about &ldquo;millennials&rdquo; as being 25-39 year old in 1996 (as well as in 2016), which is
comically absurd.</p>

<p>Not sure what to make of the authors assertion that BC Assessment is in the business of enumerating 25 to 39 year olds between 1996 and 2016,
I wonder how people get stuff like this past their editors.</p>

<p>The larger storyline is still important here, as Vancouver grows up from a city with surrounding suburbs into an integrated metropolitan area.
And a new generation,
spurred on by new challenges, including housing affordability, accelerating that transformation and re-defining what some of these former suburbs into
hip local centres that are tied together by a growing transit system.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lifeblood]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/05/16/lifeblood/"/>
    <updated>2017-05-16T13:52:05-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/05/16/lifeblood</id>
    <content type="html"><![CDATA[<p><a href="https://censusmapper.ca/maps/731"><img src="http://doodles.mountainmath.ca/images/net_animated.gif" style="width:50%;float:right;margin-left:10px;"></a>
Ever since that <a href="https://www.bloomberg.com/news/articles/2016-03-14/millennials-flee-vancouver-for-cities-with-more-affordable-homes">Bloomberg article whose claims nobody could reproduce</a>,
where the author refused to disclose what data was used, but that
got recycled all across the local press there has been a hightened interest
in migration patterns in Vancouver. Nathan Lauster took it upon himself to
dig deeper and look if <a href="https://homefreesociology.wordpress.com/2016/02/12/is-the-lifeblood-of-vancouver-leaving/">Vancouver&rsquo;s lifeblood was really leaving</a>,
which he kept elaborating on as better data became available until the
<a href="https://homefreesociology.wordpress.com/2017/05/05/good-age-specific-net-migration-estimates-come-in-threes/">most recent iteration</a>
that compares Metro Vancouver to other Candian metropolitan areas as well
as the City of Vancouver to other cities within Metro Vancouver using 2016 census data.</p>

<p>This is seriously good work and we thought it would be helpful to reproduce Lauster&rsquo;s methods
in CensusMapper. The result is a series of maps, one for each five-year age cohort, that
visualized net migration of the cohort geographically, while hovering over a
region reproduces Lauster&rsquo;s net migration bar graph for that region.</p>

<!-- more -->


<p>With CensusMapper, we instantly get the ability to dig into areas to observe
finer net migration patterns, and to pan across the country to compare different
regions.</p>

<p>For Metro Vancouver, we can see how we have strong net in-migration in all younger
cohorts, including children. And as Lauster observed, we see a net out-migration
of 50 to 60 year olds.</p>

<p>For the City of Vancouver we see a net in-migration for 5 to 30 year olds, with
basically all other age groups exhibiting net out-migration. This is echoed by
Burnaby and New Westminster, and to a lesser extend Richmond. We can zoom in
further to the Census Tract level to observe spacial patterns of net migration
at the sub-municipal level. There we see that net migration of the under 5 cohort
and the 20 to 24 cohort are almost exact mirror images. The strongest signal
comes from the town centres of the region, that get flooded as the 20 to 24 year
old cohort ages into the 25 to 29 year old cohort. As they form families and have kids,
the flood reverses and as many children that under 5 that lived in these areas don&rsquo;t
live there any more when they advance to the 5 to 9 cohort.</p>

<p>Sadly we don&rsquo;t have common tiles for other censuses that would allow for hassle-free
comparisons to detect changes over time. (It&rsquo;s a surprising amount of work
to get those common tilings right.) But we can compare this pattern to other
population centres in Canada, and this trend seems to hold true universally.</p>

<div class="half-image"><img src="http://doodles.mountainmath.ca/images/net_van.png" ><p>Vancouver</p></div>


<div class="half-image"><img src="http://doodles.mountainmath.ca/images/net_tor.png" ><p>Toronto</p></div>


<div class="half-image"><img src="http://doodles.mountainmath.ca/images/net_mon.png" ><p>Montreal</p></div>


<div class="half-image"><img src="http://doodles.mountainmath.ca/images/net_cal.png" ><p>Calgary</p></div>


<p>Metro Vancouver as a region certainly looks quite healthy in terms of the net migration of it&rsquo;s age groups.
The migration patterns within the region also seem to be consistent with other population centres in Canada.
These alone, at least at the qualitative level that we are mapping, does not seem to explain the
prevalent feeling of millennials &ldquo;fleeing&rdquo; Vancouver. Maybe a more quantitative approach could
dig up more. Or it could be that families moving away from the central regions is generally
perceived as a &ldquo;move up&rdquo; in most cities, but in Vancouver it is viewed as &ldquo;moving down&rdquo;.</p>

<h3>Technical Details</h3>

<h4>Common Geographies</h4>

<p>The first (and biggest) problem is that the 2011 and 2016 census geographies
don&rsquo;t necessarily match up. But we <a href="http://doodles.mountainmath.ca/blog/2017/03/22/comparing-censuses/">solved that problem</a>
for the 2011 and 2016 censuses.</p>

<h4>Easier said than done</h4>

<p>The devil is of course always in the details, and as usual it is not until
one has reproduced something until one can fully appreciate the work that
went into the original piece. There were two main challenges for us to
reproduce this within CensusMapper.</p>

<h4>The basics</h4>

<p>What is net migration? To compute the net migration for an age group,
say the &ldquo;under 5&rdquo; (in 2011) age group, we want to compare them to an age
group in 2016. To do that we need to &ldquo;age them forward&rdquo;. Children that were
under 5 in 2011 were 5 to 9 years old in 2016. But some, in the case of
the under 5 age group only a very small fraction, won&rsquo;t have lived to 2016.
So we need to subtract those out. More formally, we need to apply the
apporpriate mortality rates as we age them forward. Then we compare
the aged-forward group to the 5 to 9 year old cohort that the 2016 census
counted, and we divide by the size of the original under 5 year old cohort
(from 2011) to get the percentage net migration.</p>

<h4>Mortality</h4>

<p>This basic fact of life makes net migration really tricky to get right.
Once we get into the older age groups, and thus higher mortality rates,
the process is extremely sensitive to changes in the mortality rates. In
CensusMapper we are using effective mortality rates of 5 year cohorts from
<a href="http://www5.statcan.gc.ca/cansim/a26?lang=eng&amp;id=1020504">CANSIM</a> for
Canada as a whole and the provinces and territories. A fair amount of
massaging is needed to make sure this works properly, in particular it
is important to interpolate the effective mortality rates as we age a
cohort forward. And one should recognise that the mortality rates are
based on the age at death, but that an age cohort in the census is, on average,
already half way through a given year in their life.</p>

<p>Additionally, there is little reason to believe that there is no geographic
variation in mortality rates within the provinces. To deal with that we
decided to add an uncertainty band to the graph that allows for a 5%
variation in mortality rates.</p>

<h4>Uncertainty</h4>

<p>The next issue is that census data underwent random rounding to an adjacent
number that&rsquo;s divisible by 5, which leads to an expected error of 1.6.
(Actual rounding gives an expected error of 1.2.)
When taking differences between censuses, that yields an expected error of 2.2.
Additionally, we don&rsquo;t know the actual number of people in each age group,
just the one the census found. The &ldquo;census undercount&rdquo; can be quite
sizable. Overall it is around 3%, but it varies by age group and geography.</p>

<p>To deal with this we add an error term of a mis-count of 5 people that
also shows up in our error bars in the graph.</p>

<h4>Fine geographies</h4>

<p>CensusMapper is great for exploring all geographic levels. But that can
lead to problems when our age cohorts become small in size. We divide
by the size of the original 2011 age group cohort, and to avoid running
into small denominator issues we cut our estimates off when there are
fewer than 50 people in a cohort. Our error bars partially take care of
this issue, but at 50 people our error bar already spans 10% points of
net migration on either side of the computed value, showing values with
even larger error bars is likely to do more harm than good.</p>

<script>
function resetImages(){
    $('img').each(function(img){
        imgsrc = $(img).attr('src');
        if (imgsrc.slice(imgsrc.length-4)=='.gif') {
            $(img).attr('src', '');
            $(img).attr('src', imgsrc);

        }
    });
    setTimeout(function(){
        resetImages();
    },35000);
}
setTimeout(function(){
    resetImages();
},35000);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Surprise Maps]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/04/10/surprise/"/>
    <updated>2017-04-10T08:43:58-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/04/10/surprise</id>
    <content type="html"><![CDATA[<p><a href="https://censusmapper.ca/maps/668"><img src="http://doodles.mountainmath.ca/images/surprise.png" style="width:50%;float:right;margin-left:10px;"></a>
At CensusMapper we like building models based on census data. We now
have a <a href="http://doodles.mountainmath.ca/blog/2017/03/22/comparing-censuses/">common tiling for 2011 and 2016 geographies</a>
that allows us to easily model changes over time. After building a model
we often want to see how well the model performs. An easy way to do this is to
simply map the difference of observations and model predictions.</p>

<p>Those maps are great and it is easy to understand what is mapped. But they are difficult to interpret properly. In many cases
a better metric to map is how consistent the observations in each region are with the model. Which brings us to Bayesian surprise maps.</p>

<!-- more -->


<p>There is a <a href="https://medium.com/@uwdata/surprise-maps-showing-the-unexpected-e92b67398865">great post</a> making the rounds on the
web, and when it recently <a href="https://twitter.com/LGeospatial/status/850806648201388032">showed up again in my Twitter feed</a>
I finally decided to get in on the fun.</p>

<h3>The Problem</h3>

<p><a href="https://censusmapper.ca/maps/584#15/49.2089/-123.1141"><img src="http://doodles.mountainmath.ca/images/marine_gateway_nbhd.png" style="width:50%;float:left;margin-right:10px;"></a>
To understand why surprise maps can be so useful, let&rsquo;s look a a concrete example. Suppose we want to understand dwelling units
that are not used as primary residences. (And incidentally it seems that everyone currently living in Vancouver or Toronto wants to do this.) To
that end we can simply consult <a href="https://censusmapper.ca/maps/584">the map of these</a> based on the 2016 census. As we try and
understand better what is happening in each region and we zoom in more and more we start to run into issues. Consider
<a href="https://censusmapper.ca/maps/584#15/49.2089/-123.1141">this example looking at the Marine Gateway neighbourhood in Vancouver</a>. We
see two areas coloured in dark blue, indicating a high rate. The left is the site of the MC2 development that got completed
just before the census and we discussed <a href="http://doodles.mountainmath.ca/blog/2017/04/03/joyce-collingwood/">earlier in detail</a>.
The right one is a large area with exactly one private dwelling unit, which happens not to be a primary residence for anyone. If
someone did use it as primary residence it would show up on the other extreme end of our colour spectrum. Either way, it does not
really give much useful information, it is mostly a distraction that takes attention away from MC2 that has a more important story
to tell.</p>

<p>More generally, areas with very low dwelling counts are much more likely to see high variations of rates of non-permanent residence buildings
purely for statistical reasons. This results in a <a href="https://censusmapper.ca/maps/584#16/49.2425/-123.1777">&ldquo;checkerboard&rdquo; pattern</a>
that is mostly due to statistical noise and hides meaningful variations in the data.</p>

<h3>Surprise maps</h3>

<p><a href="https://censusmapper.ca/maps/669#15/49.2089/-123.1141"><img src="http://doodles.mountainmath.ca/images/marine_gateway_nbhd2.png" style="width:50%;float:right;margin-left:10px;"></a>
That&rsquo;s were surprise maps come in. As a first step, instead of colouring by the rate of non-primary residence dwellings,
let&rsquo;s colour by how this rate differs from our expectations. So we re-interpret our original map as mapping the difference from expectations
where the expectation is that all dwellings are used as primary residences. Alternatively we can also take the regional average
rate of non-primary residence dwellings and map the deviation from the average. Or we could build a
<a href="https://censusmapper.ca/maps/669">more elaborate model</a> using the 2011 rates and the Canada-wide average rates per net new dwelling unit.</p>

<p>Either way, what we are doing here is we make a guess what we think the rate of non-primary residence units should be in each area
and we compare it with observations. The better our guess is, the stronger the &ldquo;checkerboard&rdquo; pattern will become as the residuals
will be reduced to statistical noise. This as can easily be seen when
<a href="https://censusmapper.ca/maps/669#16/49.2425/-123.1777">comparing the more advanced model</a>
to the <a href="https://censusmapper.ca/maps/584#16/49.2425/-123.1777">checkerboard observed using the &ldquo;zero&rdquo; model</a>. In practice that
means that we have to click into each region to see the number of dwelling units, as well as the rate, to understand how we want
to interpret the result. Through this labour intensive process we will then weed out the area with just 1 dwelling unit and ignore
it.</p>

<p>Surprise maps don&rsquo;t map the actual difference of model and observation, they map how consistent the model is with the observation
in each area. In our initial example of an area with only one dwelling unit in it, no matter if that unit is used as
primary residence or not, this cannot be taken as strong evidence that our model is wrong. We should assign this a neutral colour.
The MC2 development on the other hand contained 570 dwelling units in 2016, a large deviation from the model indicates that
our model prediction might have some problems for that region.</p>

<p>In surprise maps, we colour each area by the amount of evidence observations in a that area gives against our model. To have good evidence
against our model, observations should deviate from model prediction, and we should be able to exclude regular statistical noise as
a cause for this.</p>

<p>For this example, we chose a mixture of the &ldquo;base rate&rdquo; and &ldquo;de Moivre funnel&rdquo; models
<a href="http://idl.cs.washington.edu/files/2017-SurpriseMaps-InfoVis.pdf">described in this excellent paper</a> where we essentially modify
the de Moivre funnel model by allowing an arbitrary model to take the place of the average of our variable. We also keep track
of the sign of the evidence we collect against our model, so whether our model underpredicted or overpredicted the rate of
non-primary residence dwelling units.</p>

<p><a href="https://censusmapper.ca/maps/668#15/49.2089/-123.1141"><img src="http://doodles.mountainmath.ca/images/marine_gateway_nbhd3.png" style="width:50%;float:left;margin-right:10px;"></a>
The <a href="https://censusmapper.ca/maps/668">result is a map that makes it much easier to sport &ldquo;surprising&rdquo; areas</a>,
that is areas where observations provide good evidence
that our model does not hold well there.</p>

<p>We can now go back to check the <a href="https://censusmapper.ca/maps/668#16/49.2425/-123.1777">Marine Gateway area</a> we looked at before,
and we see that the only areas that contribute solid evidence against our model are the unexpectedly high rates at the MC2 development,
as well as the unexpectedly low rates at the area to the east between Main and Frasier streets.</p>

<h3>Surprise Maps in CensusMapper</h3>

<p><a href="https://censusmapper.ca/maps/671"><img src="http://doodles.mountainmath.ca/images/child_poverty_surprise.png" style="width:50%;float:right;margin-left:10px;"></a>
We baked these kind of surprise maps into CensusMapper, so now we can easily apply this to any other kind of observable.</p>

<p>The CensusMapper surprise maps that are implemented right now require as input a &ldquo;model&rdquo; , an &ldquo;observation&rdquo; a &ldquo;base&rdquo; variable
(the number of dwelling units, averaged over the 2011 and 2016 censuses in our case) and a standard deviation of the difference
between model and predictions. For now, that standard deviation will have to be entered manually for technical reasons.
Moreover, other parameters like estimates of accuracy of census counts as well ask statistical rounding and other
operations that may have been performed on the data, can be added in to account for the fact that the rate observed in the census
is different from the actual rate. We added the ability to make the standard deviation region dependent, which allows us to
account for estimates of the accuracy of the census data based on region dependent variables.</p>

<p>For a simple example what this might look like consider child poverty. We have
<a href="https://censusmapper.ca/maps/132">mapped this variable in CensusMapper</a>, but at times
it can be hard to distill out the really important areas that have high child poverty rates as well as a high number of
children overall.
We have added a scatter plot of the rate vs the total number of children in poverty to the
map story, so that the user can more easily determine how significant a high child poverty rate in each particular region is.</p>

<p>Reliability of the data can also be tainted by low NHS return rates,
so CensusMapper maps automatically shade to regions that have particularly low NHS return rates
(and similarly for the full census return rates) to give some indication of reliability,
and we display the return rate on hover. But it is cumbersome to keep track of all this information.</p>

<p>Another way to deal with these issues is to make a <a href="https://censusmapper.ca/maps/671">child poverty surprise map</a>.
As model we can simply use the assumption that there
are zero children in poverty in each area. We scale the standard deviation of the child poverty rate linearly by the return rate
to weight down areas with low return rate. The result is a map that colours each region by the amount of evidence they provide
against the model assumption of zero children in poverty.</p>

<p>This makes it easier to filter out regions that have a low number
of children overall, where high rates of child poverty might just be a statistical fluke.</p>

<h3>Where to go from here</h3>

<p>Should all maps be surprise maps? No, there is value in just mapping straight up census variables, or mapping plain differences
of observations from the model. But there are many good reasons why such maps should be complemented,
or in some cases even replaced by, surprise maps.</p>

<p>Some kind of automation to aid the selection of appropriate standard deviation for the surprise model is needed before we can
open this up to a wider user base. Or CensusMapper server has an R server running that ties directly into the database and
communicates with the web server, so we need to build the appropriate scripts that can automate this task.</p>

<p>Another logical extension is to include proximity data into the Bayesian estimates. If one Census Tract provides good evidence
against our model, but that evidence is distributed uniformly across Dissemination Areas within it, then our surprise map
will show weaker evidence at the dissemination area level simply because each area has a smaller base population. But we could
check if the deviation from the model is localized in just one Dissemination Area, or also present in neighbouring
Dissemination Areas, and include that information in our estimates. This approach would also help with the problem if the evidence
is concentrated in an area that is split between to Census Tracts and drowned out by outher data in these respective tracts. So
it won&rsquo;t show up at the Census Tract level because of the particular ways the boundaries were drawn (MAUP), and it will get
diluted out at the Dissemination Area level because of low base population counts. Adding in proximity measures could recover
this evidence at the Dissemination Area level.</p>

<p>There are lots of other interesting possibilities of using surprise maps while leveraging the dynamic nature of CensusMapper.
One may allow for several models and let
relative importance of the models self-adjust with the map view. The output would then be for each map view a linear combination
of the models that provides the best fit for the current map view, as well as the evidence each region in the current map view provides
against the model.</p>

<p>We will keep experimenting with surprise models and at some point open up some of it&rsquo;s capabilities to a wider group
of CensusMapper users.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Marine Gateway and Joyce-Collingwood]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/04/03/joyce-collingwood/"/>
    <updated>2017-04-03T12:46:47-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/04/03/joyce-collingwood</id>
    <content type="html"><![CDATA[<p><a href="https://censusmapper.ca/maps/584#11/49.2389/-123.0977"><img src="http://doodles.mountainmath.ca/images/non-primary.png" style="width:50%;float:right;margin-left:10px;"></a>
There has been some <a href="http://www.theglobeandmail.com/real-estate/vancouver/bcs-empty-home-problem-moving-beyond-just-vancouver/article34130746/">recent confusion</a>
that got <a href="http://vancouversun.com/business/local-business/three-transit-oriented-communities-in-metro-rival-coal-harbour-for-empty-or-underused-housing">confounded further</a>
about transit-oriented development
in Vancouver harbouring a large number of non-primary residence homes. Good data is
important in moving forward in Vancouver&rsquo;s crazy housing market. Without proper
context, a couple of data points can serve to paint a very misleading picture
of what is happening. So I decided to fill in some gaps on the very narrow
question of understanding the CT level numbers that get tossed around.
No deep analysis, just looking into the CTs in question to see where the numbers that the census
picked up came from.</p>

<h3>TL;DR</h3>

<p>To understand the overall rate of 24.4% non-primary residence dwelling units at the Joyce census tract,
one should split the area into the Wall Centre Central Park development (99.2% non primary residence units)
and the rest of the CT (3.4% non-primary residence units).</p>

<p>To understand the Marine Gateway CT (24% non-primary residence dwellings),
it should be split inte the block with Marine Gateway development (13.7%),
the block containing the MC2 development (67.4%), and the rest (10.1%).</p>

<p>Comparing any of these very recent developments to the much older Coal Harbour makes no sense.
Coal Harbor is still &ldquo;filling in&rdquo; although at a stubbornly slow rate. It will be interesting
to see if the new vacancy tax can help speed that up.</p>

<!-- more -->


<h3>Marine Gateway</h3>

<p><a href="https://www.google.ca/maps/@49.2114587,-123.1170704,3a,75y,94.33h,114.49t/data=!3m6!1e1!3m4!1sdPusgUyW9gIfMiDEYVcduw!2e0!7i13312!8i6656!6m1!1e1"><img src="http://doodles.mountainmath.ca/images/ec2_streetview.png" style="width:50%;float:left;margin-right:10px;"></a></p>

<p>To understand the 24% non-primary residence units in the census tract containing the
Marine Gateway development, we split the area into three parts.
The block with the Marine Gateway
development has in 460 dwelling units, 63 (13.7%) of which were found not to be
primary residences. The block north of that containing MC2 had 570 dwelling units
394 (67.4%) of which were not used as primary dwellings. The remaining part of the
CT as 1,507 dwelling units, 152 (10.1%) of which were not used as primary dwellings.</p>

<p>What we see here quite nicely is how the rates of non-primary residence units
are changing as the buildings they are in get older. Both Marine Gateway and MC2 are fairly
recent projects, but Marine Gatway itself got completed about a year before MC2,
and have significantly lower rates of non-primary residence homes.</p>

<p>What needs further sleuthing is the rate of 10.1% for the rest of the CT,
which is significantly higher than the baseline of 5.4% in 2011. There are no
other larger pockets of increase of non-primary residence units in that area.
This is might be best analysed in conjunction with similar increases in non-primary
residence dwellings in some other single
family neighbourhoods and goes beyond the scope of this short note.</p>

<h4>Data:</h4>

<ul>
<li><a href="https://censusmapper.ca/maps/584#16/49.2117/-123.1146">Non-primary residences</a></li>
<li><a href="https://censusmapper.ca/maps/586#15/49.2155/-123.1105">Change in non-primary residence dwellings</a></li>
<li><a href="https://censusmapper.ca/maps/650#15/49.2155/-123.1105">Total change in non-primary residence dwellings</a></li>
</ul>


<h3>Joyce-Collingwood</h3>

<p><a href="https://www.google.ca/maps/@49.2342503,-123.0248672,3a,75y,147.43h,109.85t/data=!3m6!1e1!3m4!1s22enJ14Z6DaYnXcsXcqYqw!2e0!7i13312!8i6656!6m1!1e1"><img src="http://doodles.mountainmath.ca/images/wall_centre.png" style="width:50%;float:right;margin-left:10px;"></a>
The story for Joyce-Collingwood is quite a bit cleaner (but holds some data complexities that
manifest themselves in geocoding changes between censuses that can mess with fine-grained data).</p>

<p>The articles focus on the increase by 609 CT-level non-primary residence dwelling units at Joyce. The articles
hypothesise that &ldquo;Speculation is one of our prime suspects&rdquo; and
&ldquo;We don’t know why, but we know it’s concentrated in a few neighbourhoods.&rdquo;
without making any effort to look beyond CT-level data to find out.</p>

<p>Diving in we see immediately that the Dissemination Block at the north-west
corner of Kingsway and Boundary had 707 non-primary residence dwelling units in 2016,
vs 3 in 2011. That&rsquo;s a net change of 704 non-primary residence dwelling units
in just that block. It
block contains the Wall Centre Central Park development, that completed
shortly before the census. The census counted 713 units, only 6 of
which it found occupied.</p>

<p>At the same time, population dropped by 161 people in this block, as the
previous single family homes made way to the new development, easily
explaining the drop in population by 57 in the CT level data that
<a href="https://twitter.com/Ayan604/status/848952053435047936">has people scratching their heads</a>.</p>

<p>So what&rsquo;s the rate of non-primary residence buildings once we take out the
Wall Centre Central Park that got completed just before the census?</p>

<p>It&rsquo;s 3.4%. Which explains why Jennifer Gray-Grant, executive director at Collingwood Neighbourhood House,
was quoted finding the stat of 24% non-primary residence home &ldquo;perplexing&rdquo;.</p>

<p>It is also interesting to look back through time to see
<a href="https://censusmapper.ca/maps/400#16/49.2348/-123.0310">the rates in 2006</a>, when
the area around Aberdeen Park had 18.1% non-primary residence units. I am guessing
that some major building projects got completed in the year before the census,
by 2011 the number of non-primary residence units dropped dramatically.</p>

<h4>Data:</h4>

<ul>
<li><a href="https://censusmapper.ca/maps/584#16/49.2358/-123.0287">Non-primary residences</a></li>
<li><a href="https://censusmapper.ca/maps/586#16/49.2358/-123.0287">Change in non-primary residence dwellings</a></li>
<li><a href="https://censusmapper.ca/maps/650#16/49.2358/-123.0287">Total change in non-primary residence dwellings</a></li>
</ul>


<h3>Olympic Village</h3>

<p>The high rate of non-primary resident units in 2011 in the Olympic Village should
be seen in the context of the City&rsquo;s decision not to sell the units below cost
but hold onto the units and sell them
at a later point when the market recovers, which eventually paid off.</p>

<p>To understand the current overall rate of 9.4% non-primary residence units, it is
instructional to simply zoom into the block level data and observe how the
rates change depending on the completion time of the building.</p>

<h4>Data:</h4>

<ul>
<li><a href="https://censusmapper.ca/maps/584#15/49.2665/-123.1159">Non-primary residences</a></li>
<li><a href="https://censusmapper.ca/maps/586#15/49.2665/-123.1159">Change in non-primary residence dwellings</a></li>
<li><a href="https://censusmapper.ca/maps/650#15/49.2665/-123.1159">Total change in non-primary residence dwellings</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Comparing Censuses]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/03/22/comparing-censuses/"/>
    <updated>2017-03-22T13:28:51-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/03/22/comparing-censuses</id>
    <content type="html"><![CDATA[<p><a href="https://censusmapper.ca/maps/624#14/49.2608/-123.1556"><img src="http://doodles.mountainmath.ca/images/pop_change.png" style="width:50%;float:right;margin-left:10px;"></a>
It&rsquo;s great to have fresh census data to play with. Right now we only have
three variables, population, dwellings and households. There is still lots
of <a href="http://doodles.mountainmath.ca/blog/2017/02/10/2016-census-data/">interesting information that can be extracted</a>.</p>

<p>So we started exploring in our <a href="http://doodles.mountainmath.ca/blog/2017/03/06/rs-population-change/">last post</a>,
things get really interesting when looking at change between censuses. But as we noted, there are
several technical difficulties that need to be overcome.</p>

<p>So we at CensusMapper took that as and invitation to do what we love most: breaking down barriers.</p>

<!-- more -->


<p>The biggest difficulty in comparing censuses is that census geographies change over time to adapt to new
demographic realities on the ground. Whenever that happens, there is no easy way to compare data across
censuses.</p>

<h3>Custom Tabulations</h3>

<p>The best way to compare data across censuses is to pull a custom tabulation for data on a common geography,
there are a couple of things to keep in mind though:</p>

<p>1) It costs money (or at least time if part of a specific research project with access to a StatCan data centre),
2) it takes time to complete a custom tabulation, and
3) custom tabulations are typically not available at fine geographic levels.</p>

<p>For these reasons, comparing data across censuses is usually confined to select university research projects
or well-resourced private interest. And to make matters worse, custom tabulations are rarely shared. I have
asked university research groups if they were willing to share custom tabulations through CensusMapper, but got turned down
every time so far. While the
<a href="http://www.science.gc.ca/eic/site/063.nsf/eng/h_F6765465.html?OpenDocument">Tri-Agency Open Access Policy on Publications</a>,
postulates that research needs to be published in open access journals and the data also needs to be made available, in practice
research groups often keep their data locked up long after publication.</p>

<p>But some public agencies do make their custom tabulations available, for example the City of Vancouver has custom tabulations
for neighbourhood boundaries <a href="http://data.vancouver.ca/datacatalogue/index.htm">available on their open data catalogue</a>.</p>

<h3>Comparable Regions</h3>

<p>But even without custom tabulations we can compare regions if they have not changed across censuses. For example, we can compare
how the City of Vancouver or Metro Vancouver have change over time by tracing them through censuses, as their geography has not
changed. Census regions are tracked through their unique region identifier, which allows people to establish
timelines for regions. When doing so one quickly runs into issues with this approach though.</p>

<p>Just because a region has the same name and unique geographic identifier does not guarantee that the region has not changed. Simple
examples of this are Census Metropolitan Areas that can change quite dramatically over time without this being reflected in the name
or region identifier. For example, Ottawa, Montreal or Abbotsford were all expanded, some substantially) from the 2011 to the 2016 census.</p>

<p>For example, by just comparing Census Metropolitan Area data, Montreal&rsquo;s population grew by 7.2% between 2011 and 2016. But a good
chunk of that was simply due to the geographic region of the CMA being expanded, taking the 2016 expanded region as a base and
including all people that lived in that region in 2011, including the ones that lived outside of the 2011 CMA of Montreal but
that are now included in the 2016 CMA, the region&rsquo;s population only grew by 4.2%.</p>

<p>On top of large changes like this, many smaller regions have undergone slight boundary adjustments while keeping their unique identifier,
which move some people from one geographic region to another. And as one drills down into finer geographies one can easily
find geocoding errors in the data, where people were moved from one region to another between censuses without any region
adjustment taking place. These geocoding problems may persists in custom tabulations, but they are likely to even out as one
moves to higher aggregation levels.</p>

<h3>CensusMapper comparable regions</h3>

<p><a href="https://censusmapper.ca/maps/648"><img src="http://doodles.mountainmath.ca/images/pop_adjusted.png" style="width:50%;float:left;margin-right:10px;"></a>
At CensusMapper we decided that it would be in the general interest to be able to compare standard census variables
across all aggregation levels. So we wrote some scripts to create a &ldquo;least common denominator&rdquo; tiling for all of
Canada for all aggregation levels,
ranging from Census Blocks to all of Canada. The idea is to keep regions that have not changed and, for regions that did change,
join regions together in a way that the joins match again across censuses. While this sounds simple in principle, there are lots
of issues and pitfalls along the way. Hopefully we have properly dealt with all of them now. We have done some testing, and
there are still have 285 regions (36 CTs, 245 CSDs and 4 CMAs) where the adjusted and computed 2011 population counts don&rsquo;t
quite add up. For example, 348 people were moved between Coquitlam and Port Coquitlam in this fashion, most of it due to an
adjustment between CTs 9330287.01 and 9330291.01 that is not due to a change in geographic boundary. In the example of Airdrie
trading 705 people with Rocky view this is associated with a change of the census geography, but no change in name or
geographic identifier. In most cases the adjustment is much smaller than this. Even more extreme than Airdrie is the case
of CTs 5350512.00 and 5350511.01 in Mississauga that swapped 1074 people without any apparent boundary change.</p>

<p>We have a <a href="https://censusmapper.ca/maps/648">map that highlights all these difference at or above the CT zoom level</a>, and in all maps we shade areas with adjusted
population counts and we indicate the actual population adjustment on hover.</p>

<h3>The Good, the Bad and the Ugly</h3>

<p><a href="https://censusmapper.ca/maps/588#14/49.2613/-123.1568"><img src="http://doodles.mountainmath.ca/images/dw_change.png" style="width:50%;float:right;margin-left:10px;"></a>
So what does all of this buy us? For now, we get comprehensive maps comparing population, dwellings and households across the
2011 and 2016 censuses at all geographic levels, including Dissemination Blocks. As new census data rolls in, we will be
able to compare data down to the Dissemination Area level. That&rsquo;s pretty cool, we think.</p>

<p>What could be bad about this? Census data is far from perfect. And mapping differences tends to bring out the problems in
Census Data. Incorrectly enumerated dwellings, households or population, geocoding issues, as well as changes in geographies.</p>

<p>And things start to get really ugly when the rest of the census data comes in where statistical rounding was applied. Even if
everything was perfectly enumerated and geocoded, statistical rounding means that inevitably some areas will have census data
that has been rounded up in one year and rounded down in the next. And taking differences will bring this out very strongly,
especially at the Dissemination Area level. It is hard to overstate the effect of this, statistical rounding means that when
mapping change at small aggregation levels we will map lots of statistical noise.</p>

<p>There is however still value in showing and
working with differences of census variables at small aggregation levels, but the results are much harder to interpret.
One very simple workflow that we use regularly is to generally use Census Tract level data, but zoom in to Dissemination Areas
when we want more context for particular Census Tract values. Doing this, salted and peppered with a good dose of caution and
ideally some ground truthing, we can greatly refine our insights in what is going on in that Census Tract.</p>

<p>For this
reason we are decided to, at this point, keep fine grained data publicly available. We are aware that this will lead to
some people misinterpreting census data, but this is already happening at any aggregation level and is generally not a
sufficient argument to lock up data in our view. We may add more visual aids to warn the user of unreliable results, like adding
pattern shading to results that could be dramatically effected by small difference errors due to statistical rounding or
sampling/enumeration issues.</p>

<h3>Maps</h3>

<p>We opened up a number of maps on CensusMapper that explore changes between the 2011 and 2016 censuses down to the block level.</p>

<ul>
<li><a href="https://censusmapper.ca/maps/624">Population Change</a></li>
<li><a href="https://censusmapper.ca/maps/588">Dwelling Change</a></li>
<li><a href="https://censusmapper.ca/maps/586">Change in Non-Primary Residence Dwellings</a></li>
<li><a href="https://censusmapper.ca/maps/590">Change in Household Size</a></li>
<li><a href="https://censusmapper.ca/maps/596">Components of Population Change</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RS Population Change]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/03/06/rs-population-change/"/>
    <updated>2017-03-06T11:06:57-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/03/06/rs-population-change</id>
    <content type="html"><![CDATA[<p><a href="https://censusmapper.ca/maps/583"><img src="http://doodles.mountainmath.ca/images/pop_change_ct.png" style="width:50%;float:right;margin-left:10px;"></a>
With reporting on the new census numbers gaining traction, and now
Mayor Robertson
<a href="http://www.cbc.ca/news/canada/british-columbia/gregor-robertson-statement-vancouver-character-homes-review-1.4011100">picking up on single family neighbourhoods losing population</a>
we thought it is time to crunch some numbers.</p>

<p>Why does it need number crunching? All the reporting so far is based on looking at CT (Census Tract) aggregates, like e.g. in the
map shown and linked to the right. But there is actually no
single CT in the City of Vancouver that only contains RS zoning. Deducing results by just looking on CT aggregates can lead
to misleading reporting, like we have seen with unoccupied dwellings in the &ldquo;Marine Gateway Neighbourhood&rdquo;. Given how prominent
this topic has become it is high time to dig into the details.</p>

<h3>TL;DR</h3>

<p>In summary, we can confirm that RS (single family), RT (duplex) and FSD neighbourhoods have been dropping population.
Slightly. Looking separately at the
east and the west side, we notice that population in these neighbourhoods dropped by about 1% on the west side and increased
slightly on the east side.</p>

<p>In all groupings that we looked at the household size dropped and the rate of unoccupied dwellings increased. This was counter-acted
by a growth in dwelling units, mostly confined to RS zones where laneway houses and suites were added (or newly discovered
in the 2016 census).</p>

<p>We split the analysis into <em>core</em> regions, blocks that lie completely within RS, RT and FSD zoning, and <em>fringe</em> regions,
blocks that have RS, RT or FSD zoning as well as other zoning. Fringe regions grew in population and had overall lower rates
of unoccupied units when compared to core regions.</p>

<!-- more -->


<h3>Comparing Censuses</h3>

<p>Comparing data across censuses is hard. For one, definitions change from one census to the next
and thus variables aren&rsquo;t always comparable. Four our immediate goal of comparing
population, private dwellings, and private households between the 2011 and 2016 censuses
that is not a concern.</p>

<p>Comparing data is relatively easy when census geographies are large (i.e. CT, CSD level or higher)
and the census geography matches exactly the area that we are interested in. For CSDs (Municipalities)
this is often the case, but at the sub-municipal level, the CTs (Census Tracts) or other sub-municipal
aggregation levels rarely line up with the regions one is interested in.</p>

<p>For example, if one is interested in changes in population in RS (single family zoned) neighbourhoods in
Vancouver, looking at selected CTs will only give some initial indication. The reason is that
there is actually no CT in the City of Vancouver that is entirely RS zoned. There are several that
come close (the closest one is CT 9330015.01 around 41st and Thyne, which actually
increased population from 5,364 in 2011 to 5,485 in 2016) but it shows how tricky
it is to answer the really simple question how the population changed in RS neighbourhoods.</p>

<p>So how to deal with this issue? The cleanest way is a custom tabulation from StatCan, but that takes
time, costs money, and may still
<a href="https://twitter.com/vb_jens/status/838561779970011136">result in problems when data was not geocoded correctly</a>,
which is next to impossible to detect in custom tabulations.</p>

<p>An alternative way is to compare censuses at finer aggregation levels, that is at DAs (Dissemination Areas) or
DBs (Dissemination Blocks).</p>

<h3>Comparing Censuses at DA or DB levels</h3>

<p>For our concrete example, that means we look for DBs within RS zoning and work with these.</p>

<p>There are several difficulties with this approach. The most important is that the finer data we look at,
the more likely we pick up problems with Census data (yes, there are problems) and mistake them for real world data.
For our case through, we can avoid some of that by aggregating over all DBs within RS zoning to even out some
of these issues. Morever, we can visually inspect the data to look for any particular DB that seems to be problematic
and do some ground-truthing to see if the issues are only in the data or actually on the ground.</p>

<p>The next difficulty is technical in nature. Census geographies, including DBs and DAs, can change from one census to
another and thus may not be comparable. In order to get reliable results we need to make sure that we work with
a common set of geographies for both the 2011 and 2016 censuses. Luckily this is only a technical problem that can be
overcome as census geographies don&rsquo;t just change randomly but still retain some basic comparability.</p>

<p>And it&rsquo;s perfect timing, since we just created a &ldquo;least common denominator&rdquo; tiling derived from 2011 and 2016 DBs and DAs. At
CensusMapper we work with &ldquo;cartographic&rdquo; DBs (so we clip our major bodies of water), which leads to a minor issue where
between 2011 and 2016 things were clipped slightly differently which yielded two DBs in 2011 (with 16 and 17 people in it)
being clipped out (and having no people in it) in 2017, likely a combination of adjusting some tolerances in the StatCan
algorithms as well as some minor changes in geocoding that moved the population into an adjacent DB
(like e.g. happened with 59150971009 and 59150971008 south of King Edward split along Carnarvon). Apart from that, the
result is a tiling of Canada by DB and DA-based geographies that allow for consistent comparisons across the two censuses.</p>

<p>In numbers there were 493,192 cartographic DBs in 2016, 489,676 in 2016, and these resulted in a least common denominator tiling
by 445,953 DB-based geographies.</p>

<h3>RS, RT and FSD Zones and Population Change</h3>

<p><img src="http://doodles.mountainmath.ca/images/filter_all.png" style="width:50%;float:right;margin-left:10px;">
Back to our original question, how did population change in RS zoning. Before we go there, we think it makes more sense to expand
the question to ask for RS, RT (duplex) and FSD (First Shaughnessy) combined as these are about equally restrictive in what
we allow there.</p>

<p>Grabbing the <a href="http://data.vancouver.ca/datacatalogue/zoning.htm">latest available zoning data</a> and uploading it to
CensusMapper makes it easy to download the 2011 and 2016 dissemination blocks that intersect RS, RT and FSD zoning. We removed the RS part
that snakes along the downtown beaches and covers Stanley Park, as well as the sliver creeping up False Creek and covering the marinas there.</p>

<p>When we intersect the census data with the zones, we also compute the overlap each DB has with the zoning and disregard any region
with less than 10% overlap. Moreover we divide the dissemination blocks into <em>core</em> blocks where
the overlap is greater than 99% and <em>fringe</em> blocks, where the overlap is less than 99%.</p>

<p>One should remember that a significant
portion (a majority actually) of RS, RT and FSD dwellings are contained in &ldquo;fringe&rdquo; areas.
So it is best to focus on the rates of change, we would expect the total number of population decline
of all RS, RT and FSD zoned properties to be higher.</p>

<p>Here are the results:</p>

<pre><code>RS, RT, FSD
core: 236 DB, fringe: 475 DB
Population 2011 - 2016
core pop change: -69, fringe pop change: 6673, core total pop: 124916, fringe total pop: 284231
Dwellings 2011 - 2016
core dw change: 2812, fringe dw change: 7142, core total dw: 47008, fringe total dw: 121488
Households 2011 - 2016
core hh change: 1194, fringe hh change: 5127, core total hh: 42298, fringe total hh: 111801
</code></pre>

<p><img src="http://doodles.mountainmath.ca/images/filter_core.png" style="width:50%;float:right;margin-left:10px;">
What we see is that the population in the &ldquo;core&rdquo; DBs did drop. Slightly. At the same time the number of dwellings
increased quite noticeably by 6.3% in the core, with essentially all of the dwelling growth located within RS zones
(as opposed to RT and FSD). So most of that dwelling
growth is due to suites and laneway houses. Note that we only capture 47008 dwellings in the &ldquo;core&rdquo; RS, RT, FSD areas,
which is less
than half of the dwelling units in RS, RT and FSD with the remaining dwellings are located within the &ldquo;fringe&rdquo; regions.</p>

<p><a href="https://censusmapper.ca/maps/596"><img src="http://doodles.mountainmath.ca/images/van_pop_comp.png" style="width:50%;float:left;margin-right:10px;"></a>
Interestingly, the number of households grew much slower than the number of dwellings in the &ldquo;core&rdquo; regions, increasing the rate
of unoccupied units from 7% to 10%. Following our decomposition of population growth <a href="https://censusmapper.ca/maps/596">mapped here</a>
and <a href="http://doodles.mountainmath.ca/blog/2017/02/10/2016-census-data/">explained in more detail in a previous post</a>, we see
that the population growth of -69 in the &ldquo;core&rdquo; regions can be decomposed into:</p>

<ul>
<li>-3,699 due to declining household size,</li>
<li>-4,321 due to increase in unoccupied dwellings, and</li>
<li>7,952 due to increase in dwelling units.</li>
</ul>


<p><img src="http://doodles.mountainmath.ca/images/filter_fringe.png" style="width:50%;float:right;margin-left:10px;">
We can do the same analysis for the &ldquo;fringe&rdquo; areas, where RS, RT and FSD zoning mixes with other zones. Here we get a population
increase by 2.4%, driven by an increase
in dwelling units by 6.2%, and dampened by shrinking household size and a
more modest increase in the rate of unoccupied units from 6.7% in 2011 to 8% in 2016. We note that
the rate of unoccupied units increased significantly less on the fringe when compared to the core.</p>

<p>Breaking up the population growth of 6,673 people as before we have:</p>

<ul>
<li>-6,667 due to declining household size,</li>
<li>-3,996 due to increase in unoccupied dwellings, and</li>
<li>17,336 due to increase in dwelling units.</li>
</ul>


<h4>The West Side</h4>

<p>Lastly, we probably can&rsquo;t talk about this without running a separate analysis for the west side. So here we go.</p>

<pre><code>RS, RT, FSD
core: 88 DB, fringe: 184 DB
Population 2011 - 2016
core pop change: -510, fringe pop change: 1631, core total pop: 42878, fringe total pop: 105309
Dwellings 2011 - 2016
core dw change: 572, fringe dw change: 2426, core total dw: 16735, fringe total dw: 49210
Households 2011 - 2016
core hh change: -18, fringe hh change: 1226, core total hh: 15066, fringe total hh: 44907
</code></pre>

<p>We see that this confirms conventional wisdom that the population decline in the core areas of RS, RT, FSD is stronger
on the west side (and in fact population did increase overall in the core areas on the east side). The rate of unoccupied (by usual residents)
units was quite similar to the overall RS, RT, FSD, climbing from 6.7% in 2011 to 10% in 2016.</p>

<p>Again splitting up the population change of -510 people in the &ldquo;core&rdquo; area into components we get:</p>

<ul>
<li>-458 due to declining household size,</li>
<li>-1,587 due to increase in unoccupied dwellings, and</li>
<li>1,535 due to increase in dwelling units.</li>
</ul>


<p>For the &ldquo;fringe&rdquo; west side areas we again observe that population increased at 1.5%, dwellings by 5.1% and
the rate of unoccupied units grew slower from 6.6% to 8.7%. Splitting up the population change of 1,631 people:</p>

<ul>
<li>-1,278 due to declining household size,</li>
<li>-2,466 due to increase in unoccupied dwellings, and</li>
<li>5,376 due to increase in dwelling units.</li>
</ul>


<h3>Glossary</h3>

<p>We are a little loose with our use of language. In this post &ldquo;unoccupied&rdquo; is always short for &ldquo;not occupied by usual residents&rdquo;,
so in simpler terms &ldquo;not used as primary residence&rdquo;. &ldquo;Occupied&rdquo; refers to &ldquo;used as primary residence&rdquo;.</p>

<p>West side and east side were divided along the longitude for Ontario road.</p>

<p>Core regions are Dissemination Blocks that have at least 99% overlap with RS, RT or FSD zoning.</p>

<p>Fringe regions are Dissemination Blocks that have between 10% and 99% overlap with RS, RT or FSD zoning.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Transit Explorer]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/03/01/transit-explorer/"/>
    <updated>2017-03-01T12:10:29-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/03/01/transit-explorer</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/transit/map"><img src="http://doodles.mountainmath.ca/images/transit_vancouver.png" style="width:50%;float:right;margin-left:10px;"></a>
I have played with <a href="https://mapzen.com/documentation/mobility/isochrone/api-reference/#isochrone-service-api-reference">Mapzen&rsquo;s Isochrone serivce</a>
in the past with a simple <a href="http://doodles.mountainmath.ca/blog/2016/11/18/interactive-isochrones/">visualization of walksheds</a>.</p>

<p>Recently Mazen <a href="https://mapzen.com/blog/exclusion/">updated the isochrone API</a> to allow for a more
fine-grained selection of exactly what transit services to include
or exclude in transit routing, and they created an <a href="https://mapzen.com/mobility/explorer/">amazing mobility explorer</a>
based on that.</p>

<p>Partially motivated by chatting with two TransLink planners I decided to riff off of that and
<a href="https://mountainmath.ca/transit/map">take a look at how well TransLink serves different parts of Vancouver</a>.
At different times of day.
And how susceptible TransLink&rsquo;s network is
to Skytrain service disruptions.</p>

<!-- more -->


<p>To do this I decided to allow users to drag a location pin around that sets the start location, allow to change the time of day,
and call Mapzen&rsquo;s API to compute transit isochrones to visualize what areas can be reached
from the start location in 15, 30, 45 and 60 minutes.</p>

<p>To add some fun I made the Skytrain stations clickable, allowing the user to toggle the station status from open to closed,
so users can explore how mobility options change if a station is closed for boarding and no Skytrains pass through
any more. Essentially this cuts the transit network.</p>

<p>This does neglect bottlenecks that will emerge when alternative routes become overcrowded in the event of a skytrain failure,
and it does not take into account countermeasures by TransLink to deploy parallel buses, but it nonetheless gives
interesting conclusions about how crucial certain nodes are to the overall network.</p>

<p>Do you feel that your area is not served well enough by transit? Or under served in the evenings? Or are you worried about
what happens if the Skytrain breaks down somewhere? Just
<a href="https://mountainmath.ca/transit/map" target="_blank" class='btn btn-default'>launch the Vancouver Transit Explorer</a>
and play around to see how transit serves your needs.</p>

<p>Don&rsquo;t live in Vancouver and want to explore transit in your region? Not a problem. Use the search bar to jump to whatever city you
are interested in and click on the map to move the start location there. Then drag it around to explore that region.</p>

<p>If transit services in your city are already part of the <a href="https://transit.land/feed-registry/">TransitLand feed registry</a> that is.
If not, this visualization won&rsquo;t do you much good right now. If you are keen to use it to explore transit in your city,
just help TransitLand <a href="https://transit.land/news/2016/02/19/get-started-add-feeds.html">add your local transit agency to their feed registry</a>.</p>

<h3>Details</h3>

<p>I used settings that assume we have some happy walkers that are willing to walk quite a bit to get to and from transit, as
well as walking between stations. It seemed to me that the visualization is already overloaded with options that I did not want
to throw in another leaver.</p>

<p>The Mapzen Isochrone API also allows for routes or operators to be excluded from the calculations, so one can build more
complex &ldquo;what if&rdquo; type simulations.</p>

<p>And the service does not include bike share, which really is another piece in the whole mobility puzzle that can
significantly shorten travel time (or increase travel distance).</p>

<h3>Issues and Caveats</h3>

<p>Times after midnight may run into some issues, in some places, like e.g. Vancouver or Toronto, the early morning hour
isochrones won&rsquo;t work properly using this visualization. The technical reason seems to be that some GTFS used times past
24 hours, so 25:01 for one minute past 1AM the next day. And that breaks things somewhere. The good news is it&rsquo;s just a matter
of time for this to get fixed one way or another. But for now it&rsquo;s broken. :-(</p>

<p>Also, the tools this is built on are quite fresh. So there might be some glitches and opportunities to improve. Exciting times
when services like the isochrone API by Mapzen become publicly, and freely, available.</p>

<h3>5pm transit sheds around the world</h3>

<p><img src="http://doodles.mountainmath.ca/images/transit_vancouver.png" style="display:inline-block; width:30%; padding:1%">
<img src="http://doodles.mountainmath.ca/images/transit_toronto.png" style="display:inline-block; width:30%; padding:1%">
<img src="http://doodles.mountainmath.ca/images/transit_calgary.png" style="display:inline-block; width:30%; padding:1%">
<img src="http://doodles.mountainmath.ca/images/transit_seattle.png" style="display:inline-block; width:30%; padding:1%">
<img src="http://doodles.mountainmath.ca/images/transit_san_francisco.png" style="display:inline-block; width:30%; padding:1%">
<img src="http://doodles.mountainmath.ca/images/transit_new_york.png" style="display:inline-block; width:30%; padding:1%">
<img src="http://doodles.mountainmath.ca/images/transit_london.png" style="display:inline-block; width:30%; padding:1%">
<img src="http://doodles.mountainmath.ca/images/transit_paris.png" style="display:inline-block; width:30%; padding:1%">
<img src="http://doodles.mountainmath.ca/images/transit_melbourne.png" style="display:inline-block; width:30%; padding:1%"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[More on Teardowns]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/02/21/more-on-teardowns/"/>
    <updated>2017-02-21T10:59:27-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/02/21/more-on-teardowns</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/teardowns"><img src="http://doodles.mountainmath.ca/images/teardowns_animated.gif" style="width:50%;float:right;margin-left:10px;"></a>
A little over a year ago we ran some analysis
<a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">on teardowns</a>
of single family homes in the City of Vancouver. We used the City of Vancouver
open data to understand why some single family homes got torn down and other&rsquo;s don&rsquo;t.</p>

<p>Relying entirely on open data, there were some important questions that could not
be answered. So together with Joe Dahmen at UBC&rsquo;s School Of Architecture And Landscape Architecture
we came back to the question
and folded in transaction data from BC Assessment to add some more details and rigor.</p>

<p>The result turned out quite similar to what our initial cruder methods came
up with, but it lead to some important refinements.</p>

<p>We won&rsquo;t go into the details of the findings here, you can
<a href="https://mountainmath.ca/teardowns" target="_blank" class='btn btn-default'>read the online data story</a>
if you are interested. Instead we will go into a little more details how
the analysis was done and what is still missing.</p>

<!-- more -->


<p>The most critical piece that we added was transaction data, that is
which properties got sold in what year. Almost all properties that got
torn down were associated with a property transaction in the 4 years
around it getting torn down rebuilt.</p>

<p>This allowed us to refine the question from &ldquo;why did building A get torn
own and building B did not&rdquo; to ask the same question only considering
transacted buildings.</p>

<p>Conditioning on the most important determinant of a building getting torn down,
the transaction, we could focus in much better on what building-specific
parameters are driving teardowns.</p>

<h3>Variables</h3>

<p>We had annual assessment data pegged at July 2005 through July 2016, although
we excluded the July 2016 data for some parts of the analysis as the value
gains that year where
<a href="http://doodles.mountainmath.ca/blog/2017/01/16/2017-assessment-data/">quite extraordinary</a>
and prices have come down
a bit since then. We felt that this most recent assessment may not be a good
launching point to project the future from.</p>

<p>Unfortunately, the number of variables for teardowns that we have is
quite limited. We only have good data on assessed land values, assessed
building values and lot area. For a very small subset of about 500 buildings
we also have the building age of the building that got torn down. We have
GFA estimates for buildings that got torn down after 2009 through the
<a href="http://doodles.mountainmath.ca/blog/2016/03/05/physical-sfh-form-over-time/">analysis of LIDAR data</a>
that we did, but those estimates are quite crude and again only cover a portion of our
time frame.</p>

<p>A crucial variable that we are still missing is the actual time of the building
demolition. We inferred this from the time a new building got completed on that
property, but this inevitably introduces noise to the data. It makes it
difficult to pick the right time to calculate the relative building value. Moreover,
there may be the occasional property that got built on vacant land, so nothing got torn down.
This was less an issue for the analysis part, where we had ways to filter out such properties,
but it did cause some problems with the visualization part of the project. We did filter out
some properties manually that we could identify as being built on vacant land within
the timeframe of the visualization, namely some properties on Deering Island.</p>

<p>On top of that, the decision to demolish the building was often made long
before the building got torn down. Waiting times on demolition permits can be quite long, depending
on the property. Having access to building permit data would help sharpen
this variable. The word from the friendly open data folks is that the
City of Vancouver is working on making these public, maybe an FOI request
can help them speed up the process.</p>

<h3>Noise</h3>

<p>The most important source of noise in our data is that fact that assessment
data is only accurate <em>on average</em>. For particular buildings it can be substantially
off. We suspect that this is one of the reasons why for
buildings that are assessed to be essentially worthless,
the teardown probability tops out at a little above 60%. So someone
paying $2.5 million for a house that is worth only $10,000 to move in and live
in that house makes absolutely no sense. If the building like this did not get torn down,
we hypothesize one of three scenarios:</p>

<ol>
<li>The building was purchased as a pure investment vehicle and rented out
until an opportune time to re-develop or sell the property.</li>
<li>The assessment grossly undervalued the building.</li>
<li>The building was extensively renovated.</li>
</ol>


<p>We have looked through the data and have found little evidence that scenario 3 is
playing out in significant numbers. Extensive renovations show up in assessment
data via building value gains and the &ldquo;year improved&rdquo;. We don&rsquo;t have
data to assess the other two hypotheses.</p>

<h3>Model</h3>

<p>Given that limited variables we trained a handful of models on our data to see
how to best predict future teardowns. In all models we used, the relative
building value was the single most predictive variable, accounting for well over
80% of explanatory power no matter what methods we used. Moreover, the
performance of more complex machine learning models was not markedly better
that using a simple logistic regression. Similarly, dropping all other variables
except the relative building value only slightly decreased the skill of our
model.</p>

<p>One way to improve on our model is to use a proper survival analysis that
can better account for data that is only available for certain time frames.
For example, teardown early in our time frame suffer from the shortcoming that
we don&rsquo;t have transaction data reaching back far enough to link the teardown
to a transaction. Or more to the point, be able to compare it to other
transacted properties that didn&rsquo;t get torn down. Similar problems occur
at the end of our time frame, and with variables that are only available
in certain sub time frames.</p>

<script>
function resetImages(){
    $('img').each(function(img){
        imgsrc = $(img).attr('src');
        if (imgsrc.slice(imgsrc.length-4)=='.gif') {
            $(img).attr('src', '');
            $(img).attr('src', imgsrc);

        }
    });
    setTimeout(function(){
        resetImages();
    },25000);
}
setTimeout(function(){
    resetImages();
},25000);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2016 Census Data - Part 1]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/02/10/2016-census-data/"/>
    <updated>2017-02-10T20:27:10-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/02/10/2016-census-data</id>
    <content type="html"><![CDATA[<p><a href="https://censusmapper.ca/maps/583"><img src="http://doodles.mountainmath.ca/images/van_pop_change.png" style="width:50%;float:right;margin-left:10px;"></a>
Finally the first batch of 2016 census data has arrived on Tuesday AM and
CensusMapper was updated with the new census numbers by mid-morning.</p>

<p>Dissemination Block data was a little harder to find, but with the help
of some friendly StatCan people I finally managed to locate the data and
add that too this afternoon.</p>

<p>Time for writing up some observations. I am hoping to find time to do
this regularly as more data gets released.</p>

<!-- more -->


<p>The first batch of data only comes with three variables: Population, dwellings and households.
And two more, a data quality flag indicating areas with incomplete enumeration and the area of
each geographic region. Area itself is more tricky than it seems at first, one has
to make decisions what to count. Lakes? Rivers? National or Regional Parks?</p>

<p>Next to these variables, we also got the corresponding 2011 population for all geographic
regions at the CT/CSD level and above to aid the comparison, but these
were not available at the DA or DB level.</p>

<p>The variables seem straight forward, but nothing is simple when it comes to big projects like a census,
so it might we worthwhile to spend a little bit of time looking at them.</p>

<h3>Population</h3>

<p><a href="https://censusmapper.ca/maps/591"><img src="http://doodles.mountainmath.ca/images/van_pop_density.png" style="width:50%;float:right;margin-left:10px;"></a>
The quintessential census variable counts the number of people in each census region. With the exception
of people that have a primary residence elsewhere in Canada or abroad. There are several reasons for this,
the simples one is to avoid the double-counting of people.</p>

<h3>Dwellings</h3>

<p>&ldquo;Dwellings&rdquo; is short for &ldquo;private dwellings&rdquo;, this does not count collective dwellings
like prisons, student dorms, hospitals or nursery homes or even some coops. This can lead
to interesting situation where there are zero private dwellings (and zero households) but non-zero population.
Metro Vancouver has 45 such dissemination blocks. But these people are
only counted if they don&rsquo;t have a primary residence somewhere else, which
often is the case for students in dorms. But not for all students, as the
Dissemination Block on UBC campus that&rsquo;s wedged between NW Marine, University,
Lower Mall and Agronomy shows.
It has zero dwellings, zero households, but a population of 890 people.</p>

<h3>Households</h3>

<p>The &ldquo;dwellings, occupied by usual residents&rdquo; is also called &ldquo;households&rdquo;. It
refers to a private dwelling that is used as a primary residence, and the inhabitants
make up a household. If inhabitants of a dwelling only live there temporarily and/or has
a primary residence somewhere else in Canada or abroad, then they are not
counted in the population nor the household counts.</p>

<h3>Unoccupied Dwellings</h3>

<p><a href="https://censusmapper.ca/maps/584"><img src="http://doodles.mountainmath.ca/images/van_unoccupied.png" style="width:50%;float:left;margin-right:10px;"></a>
The difference between dwellings and households is &ldquo;dwellings, not occupied by usual residents&rdquo;. Essentially, that&rsquo;s
dwellings that are not used as primary residences. It&rsquo;s a fun variable to look at, and it
is available at the very fine Census Block level. In some cases, a Census Block can contain just one apartment building.</p>

<h3>Mixing Variables</h3>

<p>The fun starts when we mix these variables. And compare them to the previous
censuses. And there are lots of ways to do that, here are a few:</p>

<h4>Population Change</h4>

<p><a href="https://censusmapper.ca/maps/583"><img src="http://doodles.mountainmath.ca/images/van_pop_change.png" style="width:50%;float:right;margin-left:10px;"></a>
The most immediate variable to look at is population change. We have
complete Canada-wide data at the CT level or above, but if we are really
interested we can also view this data for DAs and DBs that stayed the same
across the census years. The issue with doing that is that some regions will have
not data, and one has to be very careful not to read too much into an incomplete
dataset. Which is why I usually don&rsquo;t like giving out maps that contain incomplete data like this.
But this is great for diving into specific regions to get more information.</p>

<h4>Dwelling Change</h4>

<p><a href="https://censusmapper.ca/maps/588"><img src="http://doodles.mountainmath.ca/images/van_dw_change.png" style="width:50%;float:left;margin-right:10px;"></a>
This is a great way to see where more dwellings got built. This only measures
net changes, so even if a region shows zero dwelling increase, there could have
been quite a bit of construction in the area. In particular, if houses with secondary
suites get torn down and replaced by houses without secondary suites, it will
show up as a decline in the number of dwelling units.</p>

<h4>Non-primary residence dwellings</h4>

<p><a href="https://censusmapper.ca/maps/586"><img src="http://doodles.mountainmath.ca/images/van_uo_change.png" style="width:50%;float:right;margin-left:10px;"></a>
More formally, these are &ldquo;dwellings not occupied by usual residents&rdquo;, it&rsquo;s
the difference between dwellings and households and the object of continued
scrutiny in Vancouver. And a good portion of these are the target of the
upcoming empty home tax to either monetize non-primary resident homes or
nudge them into the rental market.</p>

<h4>Household size change</h4>

<p><a href="https://censusmapper.ca/maps/590"><img src="http://doodles.mountainmath.ca/images/van_hs_change.png" style="width:50%;float:left;margin-right:10px;"></a>
A big part of population change is the change in household size. Canada wide the
trend to smaller households is continuing. That means that if the number of dwellings
and the rate of unoccupied dwellings in a region remain unchanged, then the
population in the area will decline if it follows the national trend to
smaller household size.</p>

<h3>Population Change Null Sum Game</h3>

<p>From looking at CensusMapper, the variable that got by far the most attention
nationally in the past three days is Population Change. It&rsquo;s great to
see where population grew or declined. But almost immediately people wonder
why population change was different in one region compared to another.</p>

<p>The first step to this is the Population Null Sum Game. You need the</p>

<ul>
<li>Dwelling Change</li>
<li>Change in Unoccupied Dwellings</li>
<li>Change in Household size</li>
</ul>


<p>to play. The game is then to express Population Change in terms of those three,
and thus &ldquo;explaining&rdquo; population change in terms of these variables. There are
of course other ways to split up the Population Change variable, but I find
this a useful one.</p>

<p>Nathaniel Lauster kicked the game off
<a href="https://homefreesociology.wordpress.com/2017/02/10/if-you-build-it-will-they-come/">with this great post</a>.
Let&rsquo;s follow up his inter-municipal level analysis with some intra-municipal numbers.</p>

<p>We will focus on the City of Vancouver only. People are welcome to use CensusMapper
to repeat this for whatever region they are interested in. Vancouver has the
nice advantage that at the dissemination area geography only on change was made.
A 2011 downtown DA got split into two for 2016. That makes it very easy to carry
this out at the DA level.</p>

<h3>Components of Population Change</h3>

<p>First we refine our variables a little bit it seems more interesting to use the rate of unoccupied builings as a variable,
rather than the number of these. We want to express population change <code>Δpop</code> as a
linear combination of</p>

<ul>
<li>Dwelling change <code>Δdw</code></li>
<li>Change in ratio of unoccupied dwellings <code>Δur</code></li>
<li>Change in household size <code>Δhs</code></li>
</ul>


<p>More formally:</p>

<pre><code>Δpop = hs₁₁ * (1-ur₁₁) * Δdw - hs₁₁ * dw₁₆ * Δur +  hh₁₆ * Δhs
</code></pre>

<p>where <code>hs₁₁=pop₁₁/hh₁₁</code> is the household size in 2011 as computed by dividing the population by the number of households,
<code>ur₁₁</code> is the rate of unoccupied dwellings in 2011, <code>dw₁₆</code> is the number of dwellings in 2016 and <code>hh₁₆</code> is the number of households in 2016.</p>

<p>It is simple algebra to check that the identity holds.</p>

<p><a href="https://censusmapper.ca/maps/596"><img src="http://doodles.mountainmath.ca/images/van_pop_comp.png" style="width:50%;float:right;margin-left:10px;"></a>
The first term gives the contribution to population growth due to the growth in dwellings, assuming that household size and the rate of unoccupied dwellings
are unchanged from 2011.</p>

<p>The second term give the population growth due to the change in the rate of unoccupied dwellings and the third term gives the
population growth due to a change in household size.</p>

<p>Great, all that&rsquo;s left to do is to type the formula into CensusMapper and graph the relative contribution of each of these terms. To make this work
we will have to make due with using only the magnitude of each term, but we can visualize the sign in the bar graph widget when
we hover over an area.</p>

<p>This makes it very easy to compare different areas and see how the different components contribute to the change in
population in each area.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jane Jacobs' Vancouver]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/01/25/jane-jacobs-vancouver/"/>
    <updated>2017-01-25T21:57:05-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/01/25/jane-jacobs-vancouver</id>
    <content type="html"><![CDATA[<p>Some time ago I saw <a href="https://twitter.com/gboeing/status/816331801266262017">Geoff Boeing&rsquo;s excellent package</a>
to generate Jane Jacobs style street grid images. It&rsquo;s lots of fun to compare different cities that way.</p>

<p>It can be hard to represent one city by one square mile, so I thought it would be neat to use this
to compare different parts of Vancouver. Some common themes emerge for the central parts,
the more outlying areas display very differnet patterns.</p>

<!-- more -->


<p>So I <a href="http://bl.ocks.org/d/88803d79ab2a3e637e2cce7fc151423d">dropped a couple of points on a map</a>, downloaded
the geojson and ran the script below. These are the results:</p>

<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/Downtown.png" ><p>Downtown</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/West%20End.png" ><p>West End</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/Grandview%20Woodlands.png" ><p>Grandview Woodlands</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/Kitsilano.png" ><p>Kitsilano</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/North%20Vancouver.png" ><p>North Vancouver</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/New%20West.png" ><p>New West</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/Surrey.png" ><p>Surrey</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/Metrotown.png" ><p>Metrotown</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/Richmond.png" ><p>Richmond</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/West%20Vancouver.png" ><p>West Vancouver</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/Langley.png" ><p>Langley</p></div>


<div class="jacobs"><img src="http://doodles.mountainmath.ca/images/jacobs/Port%20Moody.png" ><p>Port Moody</p></div>


<p>If you want to make your own, just grab the <a href="https://github.com/gboeing/osmnx/blob/master/examples/09-example-figure-ground.ipynb">lightly adapted</a> code below.
Yes, it is that easy.</p>

<h4>Code to generate the images</h4>

<pre><code># jane_jacobs.py
import geojson
import osmnx as ox
from IPython.display import Image
ox.config(log_file=True, log_console=True, use_cache=True)

file="data/van_cities.geojson"
img_folder = 'images'
extension = 'png'
size = 350
dpi = 90

cities=geojson.loads(open(file,"r").read())
for city in cities.features:
    place = city.properties['name']
    point = (city.geometry.coordinates[1],city.geometry.coordinates[0])
    fig, ax = ox.plot_figure_ground(point=point, filename=place)
    Image('{}/{}.{}'.format(img_folder, place, extension), height=size, width=size)
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bumper Year for Thumb Twiddlers]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/01/18/bumper-year-for-thumb-twiddlers/"/>
    <updated>2017-01-18T11:10:43-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/01/18/bumper-year-for-thumb-twiddlers</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/assessment?zoom=12&amp;lat=49.2494&amp;lng=-123.1241&amp;layer=12"><img src="http://doodles.mountainmath.ca/images/twiddling_thumbs_2006_2017.png" style="width:50%;float:right;margin-left:10px;"></a>
Almost a year has passed since we first noticed how
<a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">sitting on single family homes and twiddling thumbs generates more income than working</a>.
And not just at the level of individual single family households. In the
City of Vancouver, the cumulative
land value gains of just the single family homes eclipsed the cumulative taxable earnings
reported to the CRA for the entire population.</p>

<p>With the new assessment data available now, it is time to run the numbers
and see how our thumb-twiddlers fared vs workers this year. If you thought
last year&rsquo;s twiddling thumbs returns were crazy high, you better hold onto
your hats!</p>

<!-- more -->


<h3>Work</h3>

<p>Not much changed in terms of the income people earned. CANSIM does not make the
taxfiler data freely available at the municipal level, we we will again estimate
the cumulative income for residents of the City of Vancouver by using 2010
Census data and extrapolating by applying the Metro Vancouver rate of income
growth.</p>

<p>This way we get a cumulative $22.3bn pre-tax or $18.6bn after-tax income in 2010,
<a href="http://www5.statcan.gc.ca/cansim/a26?lang=eng&amp;retrLang=eng&amp;id=1110009&amp;&amp;pattern=&amp;stByVal=1&amp;p1=1&amp;p2=37&amp;tabMode=dataTable&amp;csid=">which grew around 13% between 2010 and 2014</a>.
Extrapolating this for another two years to 2016 we estimate an income growth of around 20% since 2010,
which then pegs the cumulative income for the City of Vancouver at
$26.8bn pre-tax or $22.3bn after-tax (ignoring financial drag and other issues).</p>

<h3>Twiddling Thumbs</h3>

<p>Let&rsquo;s compare this to how much Vancouver home owner households &ldquo;earned&rdquo; last year by twiddling thumbs while sitting on their
property. To keep things simple and consistent with last year, we again focus on just the single family homes (SFH).
And again, we only consider land value changes, after all changing the
building value because of renovation or rebuilding certainly does not happen by twiddling thumbs.</p>

<p>The median SFH land value increased was $435,000 (the average was $594,005), for a cumulative land value increase
of $46.7bn, accounting for about half of the
<a href="http://doodles.mountainmath.ca/blog/2017/01/16/2017-assessment-data/">total land value increase of the City of Vancouver</a>
or two thirds of the land value increase for residential (and mixed) land uses.</p>

<p>So while last year the twiddling thumbs return were still comparable to the cumulative income in the City of Vancouver,
this year the thumb twiddlers blow the income earners out of the water.
<span style="font-weight:bolder;">Just by by twiddling their thumbs, the SFH property owners alone earned twice as much
as the entire population of the
City of Vancouver did by actually working</span>. And in most cases homeowners won&rsquo;t pay taxes on
their thumb-twiddle earnings, although the
<a href="http://doodles.mountainmath.ca/blog/2016/10/04/secondary-suites-and-taxes/">CRA recently made it harder with their new reporting rules</a>.</p>

<p>We are glossing over a couple of details here, for example the numbers are not adjusted for inflation, and costs like
property taxes and property transfer tax are not taken into account.</p>

<p>Of course, comparing income from work to income from twiddling thumbs is not entirely fair. The income from twiddling thumbs
is frozen in the property until the owner sells. But gains accumulate over the years, and eventually everyone sells and realizes
the gains (or passes them on to the next generation). And it&rsquo;s always good to remember that
&ldquo;<a href="https://twitter.com/GRIDSVancouver/status/813516103490015232">house-rich cash-poor homeowners are one transaction away from simply being rich renters</a>&rdquo;.</p>

<h3>The Long Term</h3>

<p><a href="https://mountainmath.ca/map/values?filter=sfh&zoom=13&lat=49.2482&lng=-123.1213&layer=25&mapBase=2&year=2016"><img src="http://doodles.mountainmath.ca/images/twiddling_thumbs_animated_2017.gif" style="width:50%;float:left;margin-right:10px;"></a>
Most people don&rsquo;t trade houses like stocks, so what really matters is the long term gains, not the year to year changes. The 11 year
timeframe for which we have data roughly matches the average holding time for a single family house. The year over year
changes in land value vary dramatically, as the image shows and can be explored further using the
<a href="https://mountainmath.ca/map/values?filter=sfh&amp;zoom=13&amp;lat=49.2482&amp;lng=-123.1213&amp;layer=25&amp;mapBase=2&amp;year=2016">interactive version</a>.</p>

<p>Over 11 years, the single family home land value quadrupled. The median (nominal) single family home land value increase over that timespan
was $1,233,000, or $112,000 per year. Broken down further, that&rsquo;s $2,339,000 ($213,000 per year) for the median west side home
and $1,031,000 ($94,000 per year) for the median east side home.</p>

<p><a href="https://mountainmath.ca/map/assessment?filter=sfh&amp;zoom=13&amp;lat=49.2494&amp;lng=-123.1241&amp;layer=12"><img  src="http://doodles.mountainmath.ca/images/twiddling_thumbs_2006_2017.png" style="width:50%;float:right;"></a>
We <a href="https://mountainmath.ca/map/assessment?filter=sfh&amp;zoom=13&amp;lat=49.2494&amp;lng=-123.1241&amp;layer=12">mapped the land value gain averaged over 11 years</a>.
We can observe that the average yearly increase depends on the square footage of the property as well as the location. The rates are mostly uniform
throughout the city (with some notable exceptions), but properties starting out with a higher value will see higher total value
increases. It&rsquo;s probably fair to say that even using the 11 year average, most homeowners &ldquo;earned&rdquo; more money twiddling thumbs
than pursuing a more traditional employment.</p>

<h3>Hourly Rate for Twiddling Thumbs</h3>

<p>Using the <a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">same methods as last year</a>
we can compute the hourly earnings of thumb twiddlers. For the July 2016 to July 2017 timeframe, thumb twiddlers
in the City of Vancouver averaged a tidy $239 per hour.
Using the 11 year averaged numbers instead of focusing on just the last year we still get a healthy average
thumb twiddling rate of $62 per hour.</p>

<p>Another bumper year for thumb twiddlers!
Considering to change your line work to thumb twiddling? To bad thumb twiddling is so unaffordable!</p>

<h3>Data Dump</h3>

<p>Here is the raw output of the stats run on the single family properties for anyone interested.</p>

<h5>SFH Land Value Rise (2016 - 2017)</h5>

<ul>
<li>City Wide: Average $594,005 Median $435,000, Count: 78648, Total: $46,717,325,799, Hourly: $239</li>
<li>Eastside: Average $373,306 Median $358,000, Count: 47988, Total: $17,914,233,199, Hourly: $150</li>
<li>Westside: Average $939,435 Median $866,000, Count: 30660, Total: $28,803,092,600, Hourly: $378</li>
</ul>


<h5>SFH Land Value Rise (2006 - 2017)</h5>

<ul>
<li>City Wide: Average $153,777 Median $112,090, Count: 77809, Total: $11,965,271,272, Hourly: $62</li>
<li>Eastside: Average $96,892 Median $93,727, Count: 47545, Total: $4,606,748,985, Hourly: $39</li>
<li>Westside: Average $243,144 Median $212,636, Count: 30264, Total: $7,358,522,287, Hourly: $98</li>
</ul>


<p>The total number of units for the 2006 to 2017 analysis are a little lower since not all properties can be traced over that time
period without resorting to more tedious analysis.</p>

<script>
function resetImages(){
    $('img').each(function(img){
        imgsrc = $(img).attr('src');
        if (imgsrc.slice(imgsrc.length-4)=='.gif') {
            $(img).attr('src', '');
            $(img).attr('src', imgsrc);

        }
    });
    setTimeout(function(){
        resetImages();
    },25000);
}
setTimeout(function(){
    resetImages();
},25000);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Coveted $1.2m - $1.6m Vote]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/01/17/the-coveted-2m-6m-vote/"/>
    <updated>2017-01-17T16:19:03-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/01/17/the-coveted-2m-6m-vote</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/special/81?layer=101&amp;mapBase=2"><img  src="http://doodles.mountainmath.ca/images/coveted_vote.png" style="width:50%;float:right;margin-left:10px;"></a>
Earlier this month the province increased the threshold for the homeowner grant from $1.2 million to $1.6 million dollars.
It&rsquo;s an election year, and with the BC Assessment data for the City of Vancouver now being available via their
<a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">open data catalogue</a>
we can ask who exactly this move was targeting.</p>

<p>Restricted to the City of Vancouver, the answer is quite simple. There are about 24,000 single family homes, 1,200 duplex units and 4,000 condo units
in that bracket.</p>

<p>Let&rsquo;s take a closer look.</p>

<!-- more -->


<p><a href="https://mountainmath.ca/map/assessment?filter=[sfh,total_1200000_1600000]&amp;layer=101&amp;mapBase=2"><img  src="http://doodles.mountainmath.ca/images/coveted_sfh_vote.png" style="width:50%;float:left;margin-right:10px;"></a>
If we focus in on just the
<a href="https://mountainmath.ca/map/assessment?filter=[sfh,total_1200000_1600000]&amp;layer=101&amp;mapBase=2">single family homes</a>,
which make up the vast majority of the units in that bracket, we see that they
are (almost) entirely located in East Vancouver. In fact, we can see how Main Street delineates these properties quite neatly.
In fact, there are fewer than 500 single family homes west of Main in that bracket.</p>

<p>One could be lead to think that Main Street is the &ldquo;$1.6m line&rdquo;. But of the single family homes
east of Main, these only make up just over half of the properties there,
there are almost as many that are
<a href="https://mountainmath.ca/map/assessment?filter=[sfh,total_1600000]&amp;layer=101&amp;mapBase=2">assessed over $1.6m</a>
and <a href="https://mountainmath.ca/map/assessment?filter=[sfh,total__1200000]&amp;layer=101&amp;mapBase=2">almost 2,000 assessed under $1.2m</a>.</p>

<h4>Changing Homeowner Grant Status</h4>

<p>There are a number of single family homes where the homeowner grant status changed between 2016 and 2017.
There are about 3,100 single family homes who did not qualify for the homeowner
grant in 2016, but do now. And about 1000 that did qualify in 2016 but won&rsquo;t this year.</p>

<p>Yes, that&rsquo;s right, there are
1000 single family homes with 2016 assessment below $1.2m and 2017 assessment over $1.6m. I pity them,
having to pay an extra $50/month in property taxes just because the province did not care about them
enough to set the new threshold higher than $1.6m.</p>

<p>To round things up, there were a little under 500 duplex and condo units that did not qualify for the homeowner
grant in 2016 and do in 2017, and under 200 that did qualify in 2016 and don&rsquo;t now.</p>

<h3>How to get the grant if you are renting?</h3>

<p>You can&rsquo;t. And your landlord can&rsquo;t either. No matter how much your unit is worth, the province won&rsquo;t be cutting
any checks to lower your rent by $50/month.
The homeowner grant is just one of the many perks exclusively available home owners.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2017 Vancouver Assessment Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2017/01/16/2017-assessment-data/"/>
    <updated>2017-01-16T19:04:19-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2017/01/16/2017-assessment-data</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/values?filter=[sfh]&amp;zoom=13&amp;layer=9"><img  src="http://doodles.mountainmath.ca/images/value_animated.gif" style="width:50%;float:right"></a>
The friendly folks at
<a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data</a> just
<a href="https://twitter.com/VanOpenData/status/688060388190097408">updated their property assessment data</a> with the fresh 2017
property tax assessments. Time to run the script to update the <a href="https://mountainmath.ca/map/assessment">Vancouver Assessment Map</a>
with the new data, just <a href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/">like we did last year</a>.</p>

<p>For now we just updated our standard visuals and computed some overall statistics. We will take a closer look at the data over the coming days.</p>

<h4>Maps</h4>

<!-- more -->


<p>By now we have a variety of maps that highlight different aspects of Vancouver real estate, and after running the import scripts they
automatically serve the newest data. Our basic
<a href="https://mountainmath.ca/map/assessment">interactive assessment map</a> offers a variety of ways to slice and display the data. It&rsquo;s
mostly focused on functionality, some of which we described in
<a href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/">last year&rsquo;s post</a>, as well as other posts like
the one on <a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">twiddling thumbs vs working</a>.</p>

<p>We also have interactive views focusing on how real estate prices varied over time, for example the <a href="http://doodles.mountainmath.ca/blog/2016/04/01/on-dirt-and-houses/">houses and dirt map</a>
that separates out (inflation adjusted) values of the structures and the land, and also allows to filter by type of housing, to animate changes over time.</p>

<p>For people that like simpler maps we also have a <a href="https://mountainmath.ca/map/values?filter=[sfh]&amp;zoom=13&amp;layer=9">plain total (nominal) value over time map</a>
that allows to interactively step through the years and see how single family house values in Vancouver changed over time. Here we also
added the ability to visualize year-over-year value changes, which also hints at how BC Assessment changed their valuation
algorithm over the years.</p>

<h4>The Data</h4>

<p>The data originates with BC Assessment, which estimates land and building values of each property based on recent sales
of comparable properties. The values are pegged at July 1 of each year, with the the most recent available now being July 1 2016.
The estimates for the values do not reflect changes in the market since then. Moreover, the estimates can be quite off on an individual
property level, but are unbiased. That means that any statistics derived from a large subsample should fairly accurately
reflect actual market conditions for July 1st. Lastly, the assessed values will still change a bit as some will be successfully appealed.</p>

<p>City of Vancouver, as well as the City of Surrey, make this data available for general use through their open data portal, which
allows us to create these maps. The format of the data the municipalities are giving out through their open data portal is
different, so lazy me is only importing data from City of Vancouver. Sadly, BC Assessment does not make this data
generally available province wide for us to make province wide maps.</p>

<p>While BC Assessment makes this data available on their <a href="https://evaluebc.bcassessment.ca">eValue website</a> for browsing individual
properties and also provides it in bulk to researchers, the attached license does not allow the thematic mapping of individual properties.</p>

<p>The motivation behind the map was to understand the building stock, so in the maps as well as the summary statistics below
we filter out parks and some other properties.</p>

<p>The new city dataset does not include the 2017 tax levy, so our maps still only show the 2016 tax levies until CoV updated their dataset.</p>

<h4>History</h4>

<p>In the spirit of <a href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/">last year&rsquo;s post</a> we ran some
quick summary statistics to break down the numbers by neighbourhood. Instead of listing the most recent land and building value
increases by neighbourhood we stuck everything into an interactive graph for the entire time span between 2006 and 2017 tax years.
Use the dropdown menus to drill down
into city neighbourhoods, view values for all properties or just residential properties and display as total value or year-over-year
percentage change.</p>

<p>The last year again
saw an huge increase in property values. For the City of Vancouver land values were up 35% and building values 10%, with the land
value increase setting a record for the timeframe for which we have data. The increases become even more pronounced when we zero in
on residential property only.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<select id="nbhd-select"></select>
<select id="keys-select"></select>
<select id="type-select">
<option value='total'>Total Value</option>
<option value='percent'>Percentage Change</option>
</select>
<div id="graph" style="height:200px;max-width:640px;" data-lines="/data/detail_history.json"></div>
<div class="legend no-margin">
</div>
</div>




<script>
var ready_for_graph = function() {
    var d3lines=[];
    var padding = {top: 20, right: 20, bottom: 30, left: 90};
    var prevChartWidth = 0, prevChartHeight = 0;
    var updateTransistionMS = 750; // milliseconds
    var jsonData, xScale, yScale, line,neighbourhoods;

    var currentValue='City of Vancouver';
    var currentKeys=['land','building'];
    var currentModePercentage=false;

    var hash={
            land: {label:"Land Value", color:"green"},
              building: {label:"Building Value", color:"blue"},
              res_land: {label:"Residential Land Value", color:"darkgreen"},
              res_building: {label:"Residential Building Value", color:"darkblue"},
              land_p: {label:"Land Value Increase", color:"green"},
              building_p: {label:"Building Value Increase", color:"blue"},
              res_land_p: {label:"Residential Land Value Increase", color:"darkgreen"},
              res_building_p: {label:"Residential Building Value Increase", color:"darkblue"}
                };
    var keys=Object.keys(hash);

    var symbol;
    var prefix;
    var numberFormatter = function (y) {
        return '$' + Math.round(prefix.scale(y*10))/10.0 + symbol;
    };

    var graphs=d3.select("#graph");
    var div=graphs[0][0];
    if (div==null|| div.childElementCount!=0) {return;}
    var data_url=div.dataset.url;

    // create svg and g to contain the chart contents
    var baseSvg = graphs.append("svg");
    var chartSvg=baseSvg
        .append("g")
        .attr("class", "chartContainer")
        .attr("transform", "translate(" + padding.left + "," + padding.top + ")");

    // create the x axis container
    chartSvg.append("g")
        .attr("class", "x axis");

    // create the y axis container
    chartSvg.append("g")
        .attr("class", "y axis");
    var line;
    var largest=null;
    var lineData;
    if (div.dataset.lines) {
        d3.json(div.dataset.lines,function(error,json){
        jsonData=json;
        neighbourhoods=Object.keys(jsonData);
        var keyHash={all:{label: 'All Properties',data:['land','building']},res:{label: 'Residential Properties',data:['res_land','res_building']}};
        var keyOptions=Object.keys(keyHash);
        d3.select('#nbhd-select').selectAll('option').data(neighbourhoods).enter().append('option').attr('value',function(d){return d}).text(function(d){return d});
        $('#nbhd-select').on('change',function(d){
            currentValue=this.value;
            updateChart({init:true,keys:currentKeys,data:jsonData[currentValue],percentage:currentModePercentage})
        });
        d3.select('#keys-select').selectAll('option').data(keyOptions).enter().append('option').attr('value',function(d){return d}).text(function(d){return keyHash[d].label});
        $('#keys-select').on('change',function(d){
            currentKeys=keyHash[this.value].data;
            updateChart({init:true,keys:currentKeys,data:jsonData[currentValue],percentage:currentModePercentage})
        });
        $('#type-select').on('change',function(d){
            currentModePercentage=this.value=='percent';
            updateChart({init:true,keys:currentKeys,data:jsonData[currentValue],percentage:currentModePercentage})
        });
        lineData=json[currentValue];
        var domain=[null,null];
        var range=[null,null];
        lineData.forEach(function(d) {
             d.date = +d.date;
             if (domain[0]==null || domain[0]> d.date) domain[0]= d.date;
             if (domain[1]==null || domain[1]< d.date) domain[1]= d.date;
             keys.forEach(function(k){
                d[k]=+d[k];
                if (range[0]==null || range[0]> d[k]) range[0]= d[k];
                if (range[1]==null || range[1]< d[k]) range[1]= d[k];
            });
        });
        xScale=d3.scale.linear().domain(domain);
        var toAdd=(range[1]-range[0])/10;
        range[0]-=toAdd;
        range[1]+=toAdd;
        yScale=d3.scale.linear()
            .domain(range);

        line = d3.svg.line()
            .x(function(d) { return xScale(d.date); })
            .y(function(d) { return yScale(0); })
            .interpolate("linear");
        xAxis = d3.svg.axis()
            .scale(xScale)
            .orient("bottom")
            .tickFormat(d3.format("d"));
            //.ticks(5);
            //.tickValues(domain);

        yAxis = d3.svg.axis()
            .scale(yScale)
            .orient("left")
            .tickFormat(numberFormatter)
            .ticks(5);

        prefix = d3.formatPrefix(range[1]);
        if (prefix.symbol=='K') {
            symbol='k'
        } else if (prefix.symbol=='M') {
                symbol='m'
        } else if (prefix.symbol=='G') {
            symbol='bn'
        } else if (prefix.symbol=='T') {
            symbol='tn'
        }
        updateChart({init:true,keys:currentKeys,data:jsonData[currentValue],percentage:currentModePercentage});
        });

    }


    function updateChart(options)
    {
        var lineData=options.data;
        var init=options.init;
        var keys=options.keys;

        lineData.forEach(function(d,i) {
             keys.forEach(function(k){
                d[k]=+d[k];
                if (i>0) d[k+'_p']=d[k]/lineData[i-1][k]-1;
             });
        });

        if (options.percentage) {
            keys=keys.map(function(k){return k+'_p'});
            lineData=lineData.slice(1);
        }

        var domain=[null,null];
        var range=[null,null];
        lineData.forEach(function(d,i) {
             d.date = +d.date;
             if (domain[0]==null || domain[0]> d.date) domain[0]= d.date;
             if (domain[1]==null || domain[1]< d.date) domain[1]= d.date;
             keys.forEach(function(k){
                if (range[0]==null || range[0]> d[k]) range[0]= d[k];
                if (range[1]==null || range[1]< d[k]) range[1]= d[k];
            });
        });

        //if (options.percentage) domain[0]+=1;

        var toAdd=(range[1]-range[0])/10;
        range[0]-=toAdd;
        if (!options.percentage) range[0]=Math.max(0,range[0]);
        range[1]+=toAdd;
        yScale.domain(range);
        xScale.domain(domain);
        var formatter;
        if (options.percentage) {
            formatter=d3.format('.1%');
        } else {
            formatter=numberFormatter;
        }
        yAxis.tickFormat(formatter);

        var legend=d3.select('.legend');
        legend.empty();
        legend.selectAll('.item').data(keys)
            .enter().append('p').attr('class','item')
            .html(function(k){return '<i style="background:'+hash[k].color+'"></i> '+hash[k].label+' <span style="float:right;margin-right:10px;" id="'+k+'_value"></span>'});
        //legend.selectAll('.item').exit().remove();


        // get the height and width subtracting the padding
//    var innerHeight = window.innerHeight - 20;
        var innerWidth = window.innerWidth - 20;
        var divWidth=$(div).width();
        if (divWidth==0) divWidth=$(div.parentElement.parentElement).width();
        var maxWidth=parseInt($(div).css('max-width'));
        if (divWidth==0) divWidth=innerWidth*0.8;
        if (divWidth>maxWidth) divWidth=maxWidth;
        var chartWidth = divWidth-padding.left-padding.right;//960 - margin.left - margin.right,
        var chartHeight = $(div).height()-padding.top-padding.bottom;//500 - margin.top - margin.bottom;


        // only update if chart size has changed
        if (true || (prevChartWidth != chartWidth) || (prevChartHeight != chartHeight)) {
            prevChartWidth = chartWidth;
            prevChartHeight = chartHeight;

            //set the width and height of the SVG element
            chartSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);
            baseSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);

            // ranges are based on the width and height available so reset
            xScale.range([0, chartWidth]);
            yScale.range([chartHeight, 0]);




            if (init) {
                // if first run then just display axis with no transition
                chartSvg.select(".x")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                chartSvg.select(".y")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .call(yAxis);

                chartSvg.select('.x.axis path').style('display','inherit');
            }
            else {
                // for subsequent updates use a transistion to animate the axis to the new position
                var t = chartSvg.transition().duration(updateTransistionMS);

                t.select(".x")
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                t.select(".y")
                    .call(yAxis);
            }

            var sourceData=lineData;

            function addSeries(key){
                var g=d3.select(this);
                var color=hash[key].color;
                var label=hash[key].label;
                var className=key;

                // bind up the data to the line
                var lines = g.selectAll("path.line")
                    .data([sourceData]); // needs to be an array (size of 1 for our data) of arrays

                var valueFunction=function(d){return d[key]};
                var yFunction=function(d){return yScale(valueFunction(d))};
                var ff=key[key.length-1]=='p' ? d3.format('.1%') : numberFormatter;
                var formatFunction=function(d){return ff(valueFunction(d))};

                function tooltipFunction(d,el){
                  var key=d3.select(el.parentElement).datum();
                  var ff=key[key.length-1]=='p' ? d3.format('.1%') : numberFormatter;
                  return d.date + ': ' + ff(d[key]);
                }

                 var line=d3.svg.line()
                      .x(function(d) { return xScale(d.date); })
                      .y(yFunction)
                      .interpolate("linear");

               // transistion to new position if already exists
                lines.transition()
                    .duration(updateTransistionMS)
                    .attr("d", line);


                // add line if not already existing
                lines.enter().append("path")
                    .attr("class", "line")
                    .attr("stroke", color)
                    .attr("stroke-width", 2)
                    .attr('fill','none')
                    .attr("d", line);

                lines.exit().remove();

                // bind up the data to an array of circles
                var circles = g.selectAll("circle")
                    .data(sourceData);

                // if already existing then transition each circle to its new position
                circles.transition()
                    .duration(updateTransistionMS)
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", yFunction);

                // if new circle then just display
                circles.enter().append("circle")
                    .attr("class", className)
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", yFunction)
                    .attr("r", 4)
                    .attr('fill', 'transparent')
                    .style("stroke", color)
                    .style("stroke-width", 8)
                    .on('mouseover',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(tooltipFunction(d,this))
                    }).on('click',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(tooltipFunction(d,this))
                    }).on('touch',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(tooltipFunction(d,this))
                    }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});
                circles.exit().remove();
                }
            }

            var selection=chartSvg.selectAll('g.series').data(keys);
            selection.exit().remove();
            selection.enter().append('g').attr('class','series');
            chartSvg.selectAll('g.series').each(addSeries);
    }

    // look for resize but use timer to only call the update script when a resize stops
    var resizeTimer;
    window.onresize = function(event) {
        clearTimeout(resizeTimer);
        resizeTimer = setTimeout(function()
        {
                updateChart({init:false,keys:currentKeys,data:jsonData[currentValue],percentage:currentModePercentage});
        }, 100);
    }


};
ready_for_graph();



function resetImages(){
    $('img').each(function(img){
        imgsrc = $(img).attr('src');
        if (imgsrc.slice(imgsrc.length-4)=='.gif') {
            $(img).attr('src', '');
            $(img).attr('src', imgsrc);

        }
    });
    setTimeout(function(){
        resetImages();
    },25000);
}
setTimeout(function(){
    resetImages();
},25000);

</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updated Property Tax Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/12/13/updated-property-tax-data/"/>
    <updated>2016-12-13T20:10:22-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/12/13/updated-property-tax-data</id>
    <content type="html"><![CDATA[<p>The property tax data for the City of Vancouver has been available for a while now, and with new assessment data becoming
available soon everyone&rsquo;s worried about what their property taxes will look like. The City just passed a 3.9% increase
in their budget, so on average everyone will pay 3.9% more taxes than they did last year.</p>

<p>The exact change in property taxes varies from property to property. There is a <a href="https://patrickjohnstone.ca/2013/01/on-assessments-and-mil-rates.html">nice overview</a>
on how this works in general, for the City of Vancouver there is an added complication of land value averaging meant to
soften sudden land value increases, that effectively serves to lower taxes for single family homeowners in a rising market.</p>

<p>If that&rsquo;s all to abstract for you, keep reading.</p>

<!-- more -->


<p><a href="https://mountainmath.ca/assessment_gl/map?zoom=15&lat=49.2672&lng=-123.1449" target="_blank"><img  src="http://doodles.mountainmath.ca/images/pt_animated.gif" style="width:50%;float:left;margin-right:10px;"></a>
To make the change in property taxes a little more transparent I have added a time slider to my
<a href="https://mountainmath.ca/assessment_gl/map?zoom=15&amp;lat=49.2672&amp;lng=-123.1449">Tax Density by Land Use Map</a> that I have
<a href="http://doodles.mountainmath.ca/blog/2016/03/02/property-taxes-and-land-use/">described previously</a>. So now people can
go back in time and see how property taxes changed and compare it to their neighbours. At the same time I
have updated the data on my <a href="">regular assessment data maps</a> to be based on the 2016 tax data, more background on the
tax data is in <a href="http://doodles.mountainmath.ca/blog/2015/05/31/density-in-vancouver/">this post</a>.</p>

<p>Check out the <a class='btn' href="https://mountainmath.ca/assessment_gl/map?zoom=14&lat=49.2814&lng=-123.1312" target="_blank">interactive map</a>.</p>

<p>This map also serves as a good reality check on the tax productivity of the land.</p>

<p>Some caveats: I am missing data for some years or some
properties, and this map aggregates property taxes for
all strata lots in a stratified property, you will have to dive into the data yourself if you want to see how it changed
on individual strata lots. Zoning and land use data stay at 2016 and don&rsquo;t animate back in time because of availability.</p>

<p>Special thanks to <a href="https://mapzen.com">Mapzen</a> for making it so ridiculously easy to make these maps and for
<a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data</a> and
<a href="http://www.metrovancouver.org/data">Metro Vancouver Open Data</a> for making that data available.</p>

<script>
function resetImages(){
    $('img').each(function(img){
        imgsrc = $(img).attr('src');
        if (imgsrc.slice(imgsrc.length-4)=='.gif') {
            $(img).attr('src', '');
            $(img).attr('src', imgsrc);

        }
    });
    setTimeout(function(){
        resetImages();
    },25000);
}
setTimeout(function(){
    resetImages();
},25000);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Character Retention]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/11/26/character-retention/"/>
    <updated>2016-11-26T20:28:45-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/11/26/character-retention</id>
    <content type="html"><![CDATA[<p>Today I went to the City of Vancouver Character Retention open house. It was quite informative, city staff were helpful and knowledgeable,
and the display board and feedback form asked many good questions. But I came away with a couple of points that I think need to be addressed further:  </p>

<ul class="indented">
<li>Faux character retention vs character design guidelines. </li>
<li>Understanding economic drivers of teardown decisions.</li>
<li>Evaluation of RT character retention policies.</li>
<li>Need to separate character retention from gentle ground-oriented density. </li>
<li>New Carrots</li>
</ul>


<h3>TL;DR</h3>

<p>It gets a little wonky, so here the very short version:</p>

<ul class="indented bordered">
<li>
Current and proposed &#8220;character retention&#8221; is hollow, just retains shell. Should be handled in design guidelines.
</li>
<li>
Real character (or heritage) retention should take closer look at underlying economic drivers to become more effective.
</li>
<li>
Gentle, ground-oriented density like 4-plexes is desperately needed in RS and RT, should be decoupled from &#8220;character retention&#8221;.
</li>
</ul>


<!-- more -->


<h3>Faux Retention  </h3>

<p><img  src="http://doodles.mountainmath.ca/images/4-plex.png" style="width:50%;float:right;margin-left:10px;"> 
The existing character retention guidelines in RT, which seem to serve as a model for the expansion of character retention 
to RS, often don&rsquo;t &ldquo;retain&rdquo; all that much. </p>

<p>What the existing character retention process entails is essentially a &ldquo;character home&#8221; 
being gutted down to the studs (in most cases), potentially moved to a corner of the property and a foundation added. Then 
the entire house, including structural elements, gets build up again and infill added to the back.  </p>

<p>The result of the process is a main house with exterior form resembling a character home, and infill in the back matching
 the style. This process is almost indistinguishable from a new built to &ldquo;character design guidelines&rdquo;. The difference in 
landfill waste is minimal (and much better addressed through recycing policies) and there is no difference to the eye, 
as can easily be seen when comparing the new built infill to the &ldquo;retained&rdquo; front house, like in the Mt Pleasant 4-plex pictured above. </p>

<h3>Economics</h3>

<p>  Character retention is hard to do. It takes incentives to make it happen. The more the character retention process 
can leverage some underlying economic drivers, the more effective it will be.</p>

<p>  A while back <a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">we ran some analysis</a>
on 11 years of property-level assessment data in the City of Vancouver that was made available 
through the open data catalogue. The upshot is that the single most important factor that predicts if a building gets
 torn down is the (assessed) value of the building alone relative to the total value of the property. We call this 
quotient the &ldquo;teardown coefficient&rdquo; and found that if it is below 5%, the building has a 1/6 chance to get torn down 
within 8 to 10 years. More details and background on this
 <a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">can be found here</a>, a refinement of this analysis is works in progress.  
Currently, 34% of the SFH building stock in Vancouver fall into that category. Often people assume that the risk of a 
building getting torn down simply depends on the building age. But the relationship is not that simple as the following graph shows.  </p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;"> 
<div id="graph_sfh" style="height:200px;max-width:640px;" data-url="/data/sfh_age.json"></div> 
<div class="legend no-margin"></div> 
</div>


<p>  </p>

<p>The blue denotes properties facing little teardown pressure, the red is for properties with high teardown pressure. 
We can see that the pre-1920 building stock holds up reasonably well. In fact, 33% of the pre-1920 building stock with
 high teardown risk. For the 1920 to 1935 stock we have 59% of buildings in teardown territory, and for 1935 to 1965
 buildings that number jumps to 78%. This makes the particular choice of the character cutoff year 1940 a bit of a head-scratcher.</p>

<p>  <a href="https://mountainmath.ca/map/assessment?filter=[sfh,teardowns]&layer=100&zoom=13&lat=49.2489&lng=-123.1081&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/teardowns2.png" style="width:50%;float:right;margin-left:10px;"></a> 
This informs the scale of the teardown pressure the building stock of a particular vintage is facing. We can similarly 
map the individual properties to understand their geographic distribution. One should note that assessment data, while 
unbiased in aggregate, can be quite off when looking at specific buildings. So when mapping for example
 <a href="https://mountainmath.ca/map/assessment?filter=[sfh,teardowns]&amp;layer=4">all single family homes with high teardown pressure</a> 
it might mis-identify some buildings as teardown candidates, or miss others. However, in aggregate the data will be
 quite robust and one can easily see that there are no obvious geographic biases in teardown risk. To quantify some of 
this, 31% of eastside SFH vs 39% of westside SFH face high teardown risk.  </p>

<p>Understanding the teardown pressure, as well as the geographic distribution and the pressures by vintage, gives an 
important baseline to the heritage and the character discussions. Houses far above the 5% threshold face little
 teardown pressure, and they need little, if any, policy protection to retain them. On the other hand, for houses
 below the teardown cutoff it makes very little economic sense to retain a building, and will require a 
lot of effort and policy protection to try and retain them. And even with these protections, there is a good chance
 that the building will eventually go or face &ldquo;demolition by neglect&rdquo;.  </p>

<p><img  src="http://doodles.mountainmath.ca/images/demo_by_neglect.png" style="width:40%;float:left;margin-right:10px;">  
One example of this is the property at 4755 Belmont Ave, which the city lists as heritage building of primary significance. </p>

<p>Effective retention policies are likely going to aim at the building stock around and somewhat above the teardown 
threshold of 5%. Ideally character retention policies should aim to strengthen the economic viability of buildings with
character merit so that they don’t fall into the range where the teardown pressure becomes overwhelming.</p>

<p>With this in mind, and setting the age cutoff to 1935 instead of 1940, we can investigate the existing pre-1935 building
 stock by teardown pressure, separating out the stock that faces 
<a href="https://mountainmath.ca/map/assessment?filter=[sfh,years_1935,tdc__0.05]&amp;layer=100&amp;zoom=13&amp;lat=49.2489&amp;lng=-123.1081&amp;mapBase=2">high teardown pressure</a> 
from the <a href="https://mountainmath.ca/map/assessment?filter=[sfh,years__1935,tdc_0.05_0.1]&amp;layer=101&amp;zoom=13&amp;lat=49.2489&amp;lng=-123.1081&amp;mapBase=2">stock with moderate teardown pressure</a> 
and the <a href="https://mountainmath.ca/map/assessment?filter=[sfh,years__1935,tdc_0.1]&amp;layer=102&amp;zoom=13&amp;lat=49.2489&amp;lng=-123.1081&amp;mapBase=2">stock with low teardown pressure</a>. 
  <a href="https://mountainmath.ca/map/assessment?filter=[sfh,years__1935]&layer=110&zoom=13&lat=49.2489&lng=-123.1148&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/teardown_pressure.png" style="width:50%;float:right;margin-left:10px;"></a> 
And we can view <a href="https://mountainmath.ca/map/assessment?filter=[sfh,years__1935]&amp;layer=110&amp;zoom=13&amp;lat=49.2489&amp;lng=-123.1148&amp;mapBase=2">all three at the same time</a>.   </p>

<p>Without taking these underlying economic factors into consideration, character (as well as heritage) retention policies are 
likely to be less effective at retention than they otherwise would be. And have broader adverse effects on buildings
 without significant heritage or character merit.</p>

<p>The city’s approach to deal with heritage retention seems to be to target all properties built before 1940,
indiscriminate of what shape the buildings are in. In the next days the consultant report the city ordered will become
available, maybe there are some nuggets in there that make this whole endeavour sound reasonable.</p>

<h3>RT </h3>

<p>The current character retention effort is entirely focused on RS.  This seems to be motivated by the fact that RT already
 has character retention guidelines. The way it works is that much of RT is downzoned, with extra density conditional on
 character retention. This seems to be the basic blueprint of the proposal for RS, so let&rsquo;s take a close look how this works.  </p>

<p><a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-6]&layer=20&zoom=16&lat=49.2587&lng=-123.1063&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/rt-6.png" style="width:50%;float:right;margin-left:10px;"></a> 
Mount Pleasant gives a good example of what this can look like. In 
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-6]&amp;layer=20&amp;zoom=16&amp;lat=49.2587&amp;lng=-123.1063&amp;mapBase=2">RT-6 zoning</a> 
we have allowed stratified 4 and 5-plexes on single family lots under the character retention policies. We have 
<a href="http://doodles.mountainmath.ca/blog/2016/08/04/rt/">looked at this before</a>, this results in the property getting sliced up and stratified into
 3 to 5 family-sized ground-oriented units. In terms of prices, the assessments pegged at July 2015 
(which became surprisingly accurate again) puts RT-6 single family lots between $1m and $6m (median $1.9m), and units in those
 multiplexes between $300k and $1.8m (median $800k).</p>

<p>  <a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-7]&layer=20&zoom=15&lat=49.2636&lng=-123.1706&mapBase=2" target="_blank"><img  src="http://doodles.mountainmath.ca/images/rt-7.png" style="width:50%;float:left;margin-right:10px;"></a> 
This kind of development is great. We need more of that, much more. 
But this same model can&rsquo;t be immediately taken and expanded across the city. One look across town to 
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-7]&amp;layer=20&amp;zoom=15&amp;lat=49.2636&amp;lng=-123.1706&amp;mapBase=2">RT-7</a>, 
where similar guidelines are in place, shows that multi-plexes only appear in the pocket at 16th and Arbutus. That&#8217;s 
mainly because the Mt Pleasant model of character &ldquo;retention&rdquo; requires large lots, and most of RT-7 is on smaller lots
 that make the process of &ldquo;retention&rdquo; into multi-plexes according to current city rules unattractive. The rules need to 
be amended to take lots size (and other parameters) into account to unlock this kind of development across the city.  </p>

<p>This resulted in RT-6 only having 12% of the building stock currently facing high teardown pressure, compared to 34% of
 RT-7. In the pocket of RT-7 east of MacDonald, where lots are larger and more properties underwent the character retention
 process, only 17% of the building stock faces high teardown pressure. And yes, RT-6 started out with buildings in a
better state than RT-7. Which is another part of the reason why the character retention program was more successful in RT-6.  </p>

<p>We can try to understand better where the character retention program was utilized and where it wasn&rsquo;t by looking at 
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT,years_2000,nzone_RT-11]&amp;layer=20&amp;zoom=13&amp;lat=49.2497&amp;lng=-123.1232&amp;mapBase=2">all the properties that have been built since 2000 in RT</a>, 
keyed by whether it was a single family, duplex or multi-plex that got built. We removed RT-11 from the map, it is too
 new to be meaningful. It&rsquo;s amazing to me how in some pockets almost no multi-plexes get built. We should try and fine-tune 
this process before transplanting it to RS.  RT-11 offers a cautionary tale. Looking at what got
<a href="https://mountainmath.ca/map/assessment?filter=[zone_RT-11,years_2000]&amp;layer=20&amp;zoom=15&amp;lat=49.2393&amp;lng=-123.0509&amp;mapBase=2">built in current RT-11 after 2000</a> 
we see lots of single family homes. A huge missed opportunity. If we had upzoned that area a decade earlier we could 
have allowed for homes that suit our families much better. </p>

<p>In summary, we should look carefully at how character retention performed in RT and going forward devise policies for RS and RT.
It’s time to drop that artificial divide that has been overtaken by the reality that RS already allows 3 units on each property, more than RT.</p>

<h3>Hostage Taking</h3>

<p>The display boards suggests that the current plan for &ldquo;character retention&rdquo; in RS is essentailly to copy over the RT
blueprint with some fine-tuning. Downzone RS and give density for undergoing &ldquo;character retention&rdquo;.</p>

<p>  In essence that&rsquo;s holding gentle, ground-oriented density hostage to character retention, and it&rsquo;s a terrible thing to do. Whatever the 
question is, downzoning can&rsquo;t be part of the answer any more. Vancouver is starved for gentle, sensible, ground-oriented 
family-friendly development.</p>

<p>  Originally, when the character retention guidelines for RT were made, this kind of gentle density came in the Trojan horse 
of character retention. And even if much of that was faux character, I don&rsquo;t mind. I take that kind of density any way I can.  
But we are quite a bit further along now. Unaffordability skyrocketed. Families are starved for housing options. </p>

<p>We need to scale this process up. And just expanding  the area where we deploy it to also cover RS won&rsquo;t do the trick.
We average about 10 RT character retention projects 
a year. Out of 11k RT properties. If we scale that up to all 68k RS properties, we will increase that number to about 70 
a year. Just for perspective to see how inadequate that is, we tear down around 1000 single family homes a year. 
And replace them with single family homes.  </p>

<p>What we need is a program that brings this type of density to RS and RT independent of the character retention program.
And let’s drop the &ldquo;retention&rdquo; pretence. If the character home look is what Vancouverites want in return for gentle
density, then let&rsquo;s prescribe the exterior look of new-builts and let the design review process  handle it.  </p>

<h3>New Carrots  </h3>

<p>If density won&rsquo;t be the main carrot (or a stick) for character retention, than what can pull up the slack? Property taxes is an obvious one.
 The owner could be allowed to re-claim property taxes against improvements and maintenance that aim to maintain or
 underline the character merit of the home. And these re-claimed taxes become cumulatively payable, with interest, in the
 event of demolition. Of course this would raise everyone else&rsquo;s property taxes, but I think that&rsquo;s only fair in return
 for the community dictating character-homeowners the exterior look of their home.</p>

<p>  The idea behind this is that this aims to directly strengthen the economic viability of the building, thus removing the 
economic drivers that favour tearing down the building. And over time accumulating a penalty that dissuades from
 tearing down the building.  </p>

<p>Density can still be part of the mix, but not in a way that it precludes smart gentle density to be built without
 it. The reality is that much of our building stock will go, the economic drivers are just too strong. And we all know that 
we should not replace those single family homes with yet another single family home that at best the top 5% income households 
can afford. We should allow these houses to be replaced with ground-oriented units a much larger portion of Vancouver 
families can afford.  </p>

<p>Smarter people than me have probably though about this for quite some time now. I would love to hear more ideas how 
character retention can be structured so that it does not get in the way of gentle density for new builts.   </p>

<script>

function stacked_bar_graph(div,shiftAxis,domainFormatter,rangeFormatter,domainLabelFormatter){
    if (!domainFormatter) domainFormatter=d3.format("d")
    if (!rangeFormatter)
     rangeFormatter = function (y) {
        return y;
     };
     if (!domainLabelFormatter) domainLabelFormatter=domainFormatter;

var margin = {top: 20, right: 20, bottom: 40, left: 70},
    width = parseInt(div.style("width")) - margin.left - margin.right,
    height = parseInt(div.style("height")) - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);


var xAxis = d3.svg.axis()
    .scale(x)
    .tickFormat(domainFormatter)
    .orient("bottom");


var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .tickFormat(rangeFormatter)
    .ticks(5, rangeFormatter);

var svg = div.append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var data_url=div[0][0].dataset.url;
var legend=d3.select(div.node().parentNode).select('.legend');


d3.json(data_url, function(error, json) {
  if (error) throw error;
  var graphData=json[0];
  var data=graphData.data;
  var color = d3.scale.ordinal().domain(graphData.colors.map(function(d,i){return i}))
  .range(graphData.colors);
  var domain=data.map(function(d){return d.date;});
  x.domain(domain);

  function graphValueId(i){
      return graphData.class + '_' + i + '_value'
  }

  graphData.labels.forEach(function(text,i){
    var color=graphData.colors[i];
    var html='<i style="background:' + color + '"></i> ' + text + ' <span style="float:right;margin-right:10px;" id="' + graphValueId(i) + '"></span>'
    legend.append('p').html(html);
  });

  data.forEach(function(d) {
      var y0 = 0;
      d.values = color.domain().map(function(i) { return {date: d.date, y0: y0, y1: y0 += +d.count[i]}; });
      d.total = d.values[d.values.length - 1].y1;
  });
  y.domain([0, d3.max(data, function(d) { return d.total; })]);

  var domainTickValues=[];
  var skip=Math.round(40/x.rangeBand());
  if (skip<=0) skip=1;
  for (var i=0;i<x.domain().length;i++) {
    if (i % skip==0) domainTickValues.push(x.domain()[i]);
  }
  if (x.domain().length % 5 !=0) domainTickValues.push(x.domain()[x.domain().length-1]);
  xAxis.tickValues(domainTickValues);

  var xShift=shiftAxis ?  x.rangeBand()/2.0 * 1.1 : 0;

  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(" + xShift + "," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis);
//    .append("text")
//      .attr("transform", "rotate(-90)")
//      .attr("y", 6)
//      .attr("dy", ".71em")
//      .style("text-anchor", "end")
//      .text("Probability");

    function updateTooltip(d,i){
       color.domain().forEach(function(j){
             var value=d && i==j ? (domainLabelFormatter(d.date) + ': ' +rangeFormatter(d.y1-d.y0)) : '';
             d3.select('#'+graphValueId(j)).text( value);
       });
    }

  var year=svg.selectAll(".year")
    .data(data)
        .enter().append("g")
          .attr("class", "g");
  year.selectAll(".color-bar")
      .data(function(d) { return d.values; })
    .enter().append("rect")
      .attr("class", graphData.class + " color-bar")
      .attr("fill", graphData.color)
      .attr("x", function(d) { return x(d.date); })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.y1); })
      .attr("height", function(d) { return Math.max(0, y(d.y0) - y(d.y1)); })
      .attr("fill",function(d,i) {return color(i);})
      .on('mouseover',updateTooltip)
      .on('click',updateTooltip)
      .on('touch',updateTooltip)
      .on('mouseout',function(){updateTooltip(null,i)});


});

}


var priceFormatter2=d3.format("$,");
    var priceFormatter = function (y) {
        return y>=1000000 ? (priceFormatter2(y/1000000) + 'm') : (priceFormatter2(y/1000) + 'k');
    };
    var brackets=[100000,200000,300000,400000,500000,600000,700000,800000,900000,1000000,2000000,10000000,20000000,40000000]

var binFormatter=function(top){
    var bottom=0;
    if (top<=1000000) bottom=top-100000;
    else if (top==2000000) bottom= 1000000;
    else if (top==10000000) bottom= 2000000;
    else if (top=20000000) bottom= 10000000;
    else bottom=20000000;
    return priceFormatter(bottom) + ' - ' + priceFormatter(top);
}
var numberFormatter=d3.format(",");
var numberBinFormatter=function(top){
    var     bins=[0,1,2,4,8,10,16,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,300,400,500,700];
    var index=0;
    while (bins[index]<top && index<bins.length) index ++;
    bottom=bins[index-1]+1;
    return (bottom == top) ? numberFormatter(bottom) : numberFormatter(bottom) + ' - ' + numberFormatter(top);
}
stacked_bar_graph(d3.select("#graph_sfh"));
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interactive Isochrones]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/11/18/interactive-isochrones/"/>
    <updated>2016-11-18T10:22:29-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/11/18/interactive-isochrones</id>
    <content type="html"><![CDATA[<p><img  src="http://doodles.mountainmath.ca/images/isochrone.png" style="width:50%;float:right;margin-left:10px;">
Mapzen again upped their game by publishing their <a href="https://mapzen.com/documentation/mobility">Mobility API</a>. This is
super exciting for anyone interested in a whole range of mobility questions. A <a href="https://twitter.com/dnproulx/status/799644235720900608">question I have seen</a> is how to adapt that
to specific needs. So here is a quick example how to customize walksheds.</p>

<!-- more -->


<p>All we do is set up a quick map that computes the 5 and 10 minute
walksheds when the user clicks on the map.</p>

<iframe src="http://doodles.mountainmath.ca/isochrone.html#14/49.2775/-123.1292" width="80%" height="450" style="margin: 5px 10%;"></iframe>


<p>To get a better view you can also
<a href="http://doodles.mountainmath.ca/isochrone.html" target="_blank" class='btn btn-default'>take the map full-screen</a>.</p>

<p>Feel free to just <a href="http://doodles.mountainmath.ca/isochrone.html" download>grab the html</a> and adjust it for your needs. Please go and
register for your <a href="https://mapzen.com/developers/sign_in">free Mapzen API key</a> and replace the key in the downloaded
html file with yours. Refer to the <a href="https://mapzen.com/documentation/mobility">Mobility API</a> to customize this for your
needs.</p>

<p>Here is the relevant code to generate the isochrones:</p>

<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var mapzenApiKey="&lt;your api key&gt;";
</span><span class='line'>var marker,isochrone;
</span><span class='line'>
</span><span class='line'>function httpGetAsync(theUrl, callback)
</span><span class='line'>{
</span><span class='line'>    var xmlHttp = new XMLHttpRequest();
</span><span class='line'>    xmlHttp.onreadystatechange = function() {
</span><span class='line'>        if (xmlHttp.readyState == 4 && xmlHttp.status == 200)
</span><span class='line'>            callback(xmlHttp.responseText);
</span><span class='line'>    };
</span><span class='line'>    xmlHttp.open("GET", theUrl, true); // true for asynchronous
</span><span class='line'>    xmlHttp.send(null);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>map.on('click',function(e) {
</span><span class='line'>    if (marker) marker.removeFrom(map);
</span><span class='line'>    marker = L.marker(e.latlng).addTo(map);
</span><span class='line'>    var json={locations:[{lat:e.latlng.lat,lon:e.latlng.lng}],costing:"pedestrian",contours:[{time:5,color:"006400"},{time:10,color:"006400"},{time:15,color:"006400"}]};
</span><span class='line'>    var url='http://matrix.mapzen.com/isochrone?json='+JSON.stringify(json)+'&api_key='+mapzenApiKey;
</span><span class='line'>    httpGetAsync(url,function(data){
</span><span class='line'>        var geojsonFeatures=JSON.parse(data);
</span><span class='line'>        geojsonFeatures.features.forEach(function(f){
</span><span class='line'>            f.geometry.type="Polygon";
</span><span class='line'>            f.geometry.coordinates=[f.geometry.coordinates];
</span><span class='line'>        });
</span><span class='line'>        if (isochrone) isochrone.removeFrom(map);
</span><span class='line'>        isochrone=L.geoJSON(geojsonFeatures, {style: function(feature){return {color:'#'+feature.properties.color, opacity:feature.properties.opacity}}}).addTo(map);
</span><span class='line'>    });
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure></notextile></div>


<p>Happy mapping.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Trick-or-Treat 2016]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/10/21/trick-or-treat-2016/"/>
    <updated>2016-10-21T20:38:14-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/10/21/trick-or-treat-2016</id>
    <content type="html"><![CDATA[<p><a href="https://censusmapper.ca/maps/529" target="_blank"><img  src="http://doodles.mountainmath.ca/images/tot_density.png" style="width:50%;float:right;margin-left:10px;"></a>
A year ago, as were were just getting CensusMapper up and running, we put out three Halloween-themed census maps. Those
maps almost broke our servers when they went viral. At least as viral as census data goes. They were viewed by over
150,000 Canadians over the course of three days. And many of those came back to view the maps more than once.</p>

<p>Lots of things have happened at CensusMapper since last year, and we heeded the call and put some of CensusMapper&rsquo;s prowess
to use to make some important improvements for this Halloween.</p>

<!-- more -->


<p>We are stuck in a weired place this year. We just completed the
<a href="https://twitter.com/NavdeepSBains/status/770281404848566273">most successful census in the Canadian history</a> this May,
but the results are still undergoing quality control and will only get released starting this coming spring. The 2011
census, that last year&rsquo;s maps are based on, is now 5 years old. Some children that were of prime trick-or-treating age
(which we take as 5 to 14 years old) in 2011 are now in College. And some that weren&rsquo;t even borne in 2011 will be out
knocking on doors this Halloween.</p>

<p>As people move around, the composition of neighbourhoods does not change all that much over time. Young kids grow up to take
the place of the older kids in the neighbourhood. That was our take last year, as we were still mapping 2011 data to
give people an idea how many kids would show up on their door or where to expect the most foot traffic.</p>

<h3>The maps</h3>

<p>Last year we made three maps, the
<a href="https://censusmapper.ca/maps/137">Trick-or-Treat Density map</a> that visualized the expected trick-or-treat foot traffic,
the <a href="https://censusmapper.ca/maps/136">Trick-or-Treat Onslaught Map</a> that visualized how many kids to expect at your
door and the <a href="https://censusmapper.ca/maps/138">Haunted House Map</a>
that&rsquo;s mapping homes occupied by ghosts and other &ldquo;unusual residents&rdquo;.</p>

<p>This year we improved on those maps by building a dynamic estimator that estimates conditions on the ground this Halloween. We
have:</p>

<table>
<tr>
<td style="width:25%;padding:3%">
<a href="https://censusmapper.ca/maps/529" target="_blank"><img  src="http://doodles.mountainmath.ca/images/tot_density.png" style="width:100%"></a>
</td>
<td style="width:25%;padding:3%">
<a href="https://censusmapper.ca/maps/528" target="_blank"><img  src="http://doodles.mountainmath.ca/images/tot_onslaught.png" style="width:100%"></a>
</td>
<td style="width:25%;padding:3%">
<a href="https://censusmapper.ca/maps/138" target="_blank"><img  src="http://doodles.mountainmath.ca/images/tot_haunted_houses.png" style="width:100%"></a>
</td>
<tr> 
</table>


<ul>
<li><p><a class='btn btn-success' href="https://censusmapper.ca/maps/529" target="_blank">Trick-or-Treat Density Estimator</a>
that estimates the number of children of prime trick-or-treat age per area, so the expected trick-or-treat foot traffic
in each area. Use this map if you plan on taking your kids out trick-or-treating and want to make sure you find the area
in your neighbourhood with the most kids out on the street.</p></li>
<li><p><a class='btn btn-success' href="https://censusmapper.ca/maps/528" target="_blank">Trick-or-Treat Onslaught Estimator</a>
that estimates the number of children of prime trick-or-treat age per doorbell. Use this map if you are guarding the door
and want to know how much candy you should have ready.</p></li>
<li><p><a class='btn btn-success' href="https://censusmapper.ca/maps/528" target="_blank">Haunted Houses Map</a> that shows
the houses that were occupied only by ghosts or <em>unusual residents</em> in May 2011. As ghosts and <em>unusual residents</em> tend
to be restless we were not comfortable making estimates where they may be now,
even with all of CensusMapper&rsquo;s power behind it. So we only have the 2011 map to give some inspiration where the best
chances might be to run into ghosts or cross paths with <em>unusual residents</em>.</p></li>
</ul>


<h3>How did we estimate things for 2016 Halloween?</h3>

<p>An skillful estimator strikes a balance between simplicity and effectiveness. We decided to make use of the
following data:</p>

<ul>
<li>The number of children in the age brackets 0-4, 5-9 and 10-14 in each area in 2011.</li>
<li>The number of children in the age brackets 0-4, 5-9 and 10-14 in each area in 2006.</li>
<li>The percentage of the population in each area in 2011 that did not move between 2006 and 2011.</li>
</ul>


<p>Based on this, we split the estimate of the number of children in each area in 2016 into two groups. Children that
already lived in the are in 2011 and still live there in 2016, and children that moved into the area between 2011 and 2016.
We estimate the proportion by using the proportion of people that did not move between 2006 and 2011 as a proxy.</p>

<p>Then estimating the number of children that did not move is easy, we simply age the children we saw in 2011 forward by
five years and weight them by the proportion of non-movers.</p>

<p>To estimate the number of children moving into the area take into account at the trends from 2006 to 2011 in each area
and extrapolate from there to 2016. And weight the number by the proportion of the population that did move and also
allow for some population growth according to the 2006 to 2011 trend.</p>

<p>I will spare you the exact formula. It is possible that refining the model and adding in more variables could make some
improvements to the estimates, but overcomplicating the model also leads to risks of exaggeratig biases. We feel that
our simplistic model strikes a good balance between simplicity and effectiveness. It will be interesting to see how we
did when 2016 data comes in.</p>

<h3>The Super Nerdy Details</h3>

<p>For the data nerds still reading, the CensusMapper servers actually don&rsquo;t do any of that work. They just server straight-up
census data. Plus the instructions of how to use the data to make the map. All the computations and drawings are then
done locally on each user&rsquo;s browser. That&rsquo;s what keeps our servers
lean, responsive, and most importantly maintains maximum flexibility. With the same process we can easily map any function
built from census variables. Canada-wide and across all the entire geographical census hierarchy, from all of Canada down
to Dissemination Areas and, for some data, even Dissemination Blocks. Bwetween the 2006 and 2011 census
variables and all the different geographic regions we have about 1 billion fields in our database that we can map
dynamically. Still a far cry from &ldquo;big data&rdquo;, but enough to require some careful architecture choices.</p>

<h3>Census Mapping for Everyone</h3>

<p>Census data can be very complex and mixing various variables is powerful yet prone to produce meaningless maps due to
misinterpretation of the census variables. Since last Halloween and now we are proud to have been able to open up some
of CensusMapper&rsquo;s capabilities to the general public. Everyone can now make Canada-wide interactive maps based on single
2011 census variables via a couple of mouse clicks and share the freely. You can read more about this
<a href="http://doodles.mountainmath.ca/blog/2016/05/04/census-mapping-for-everyone/">in a previous blog post</a> or
<a href="https://censusmapper.ca/maps/new">jump right in and make your own map</a>.</p>

<h3>FAQ</h3>

<p>Here are some of the questions we got most last year:</p>

<h5>Are the maps only for Vancouver?</h5>

<p>No. All CensusMapper maps are fully interactive and by default Canada-wide. Use the search function or geo-location
button to switch to any place in Canada that tickles your interest. Pan and zoom around to explore. You can always share
the current view of the map with your friends by grabbing the URL in your browser address bar or using the build in share buttons.</p>

<h5>What&rsquo;s the difference between the Trick-or-Treat Density and the Trick-or-Treat Onslaught map?</h5>

<p>The maps are related, both map the number of children of prime trick-or-treat age, but they are normalized differently.
The Density map considers the number of children per area. This estimates how many you will see out on the street in the area.
The Onslaught map considers the number of children per door, so you can estimate how many will come knocking.
The difference can be seen in high density areas like Yaletown in Vancouver. There are a lot of kids living in Yaletown
in a fairly small area, so the expected foot traffic is high. But these kids distribute over the units in the highrises
there and on a per household (or per door) basis, the number of kids per household does not stand out.</p>

<h5>How accurate are the predictions?</h5>

<p>Accuracy of predictions vary. Some parents will drive their kids halfway across town to find the best area for
trick-or-treating. Some areas change too fast for our estimator to catch on. And some neighbourhoods have traditions and
local areas where kids congregate that we don&rsquo;t have data for. So these maps are best use in conjunction with local data.</p>

<h5>Won&rsquo;t your maps lead to more people trick-or-treating outside of their neighbourhood?</h5>

<p>While our maps can be used to facilitate driving kids across town to trick-or-treat, we encourage everyone to also
use the map to find areas near
where they live, and where the kids can walk to. We believe an important part of the Halloween experience is for everyone
to get to know their own neighbourhood better, see things in a new light and experience the
joys of running into friends and acquaintances, and making new friends, along the way.</p>

<h5>I am running out of candy / I got way too much candy!</h5>

<p>The Onslaught map just estimates the average number of children that knock on doors in each area. Some doors will see a
lot more kids than others. Take the Yaletown example from above. Most
of the action happens on the street. So while on average people can only expect a moderate number of trick-or-treaters
at their door, the ones at ground level will be <em>very</em> busy, while the ones higher up will likely just see one or two
neighbour kids from the same floor.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Secondary Suites and Taxes]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/10/04/secondary-suites-and-taxes/"/>
    <updated>2016-10-04T14:59:20-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/10/04/secondary-suites-and-taxes</id>
    <content type="html"><![CDATA[<p><a href="https://censusmapper.ca/maps/519#12/49.2597/-123.1243" target="_blank"><img  src="http://doodles.mountainmath.ca/images/secondary_suites.png" style="width:50%;float:right;margin-left:10px;"></a>
A couple of weeks ago I started thinking about secondary suites, laneway houses and taxes in the City of Vancouver. The number
of secondary suites and laneway houses has been continuously growing. Rental income
is probably one of the main reasons people choose to activate a secondary suite. What are the tax implications?</p>

<!-- more -->


<h2>Secondary Suites</h2>

<p>How many secondary suites are out there? Nobody really knows, but we have some estimates. Comparing single family properties
from the assessment roll to census counts provides one estimate. This is likely a lower bound as the census is prone to
undercount suites. Using this method, we arrive at a lower bound of 30,000 single family properties with a suite in 2011,
up from about 27000 in 2006. The <a href="https://censusmapper.ca/maps/519#12/49.2597/-123.1243">above map</a> gives a rough idea
of their location, although it also includes the roughly 2000 stratified duplex properties.</p>

<p>Of these we estimate that 7,000 had more than one suite in 2011, with a similar number in 2006. The number of laneway houses was
quite small in 2011, likely less than 400. But by now the number has grown to over 2,000.</p>

<p>In total that adds up to an estimated around 37,000 suites in the city of Vancouver in 2011. Probably more in the
recent 5 years, maybe fewer in the years between 2006 and 2011. Most of these will be operated as suites, with the main
unit serving as a principal residence of the property owner. But this also includes investment properties where each
unit, including the main unit in the house, are rented out.</p>

<p>For this post we will focus entirely on the suites. We might expand on this analysis at some later point and separate
out owner-occupied homes from investment properties. Moreover we will use the more conservative estimate of 30,000
suites. Feel free to adjust that estimate in the interactive at the bottom see how this effects the estimates we make.</p>

<h2>Income Tax</h2>

<p>Secondary suite rental income needs to be recorded on
<a href="http://www.cra-arc.gc.ca/E/pbg/tf/t776/t776-15e.pdf">CRA form T776</a> and, after claiming deductions, taxed.</p>

<p>Estimating the average suite rental income at $1,000, claiming $200 in deductions leaves us with $800 or $9,600 in
monthly taxable income. With single family homeowners generally above the median income we use a marginal tax rate of 35%
to estimate $3,360 of taxes per secondary suite, so cumulative $100,800,000 in income taxes payed on secondary suite
rental income in the City of Vancouver.</p>

<h2>Capital Gains Tax</h2>

<p>Assessing the capital gains implications is more complex. The capital gains exemption applies to a <em>housing unit</em>. If a
property with several housing units is sold, only the capital gains attributed to the unit used as principal residence
is exempt from taxation. Typically the portion of taxable gains is determined by the relative areas of the rented and
principal residence parts of the home.</p>

<p>So the question whether the portion of a house taken up by a secondary suites is exempt from capital gains essentially
boils down to the question whether or not the secondary suite counts as a &ldquo;self–contained domestic establishment
for earning rental income&rdquo; or as &ldquo;one or more rooms in the home&rdquo;. I have asked the CRA to clarify how this would apply
to someone converting a portion to a house to secondary suite or building a laneway house, to which I got the following
response.</p>

<blockquote><p>The conversion of a portion of a house that otherwise is the taxpayer’s principal residence for the purpose of earning income or the creation of a separate building or structure on land that forms part of the taxpayer’s principal residence for the purpose of earning income are examples of structural changes that would result in a deemed disposition and reacquisition of a portion of the property.</p></blockquote>

<p>Similarly, when a homeowner deactivate a suite and absorbes it into their main residence that would trigger another
<a href="http://www.cra-arc.gc.ca/tx/tchncl/ncmtx/fls/s1/f3/s1-f3-c2-eng.html#N10BCD">&ldquo;deemed disposition (and reacquisition) thereof at fair market value&rdquo;</a>.
Which then would trigger a &ldquo;taxable capital
gain attributable to the period of use of such portion of the property for income–producing purposes&rdquo;, just like it
would at a sale of the property.</p>

<p>In the case when the homeowner rents out their entire home and later moves back in again, the homeowner can, under
certain circumstances, make an election that he can maintain the primary residence status for most recent 4 years during
which the home was rented out. I have asked the CRA to clarify if a similar election could be made for a secondary suite,
when for example after four years of renting the suite is deactivated and again absorbed in the main house and the response
was that &ldquo;such an election cannot be made where there is only a partial change in use of the property.&rdquo;</p>

<p>Based on this I am fairly convinced that the portion of a home used as a secondary suite or laneway house is not exempt
from capital gains tax. Despite my best efforts I might still be reading this wrong, in which case I would love to have
an expert weight in on this.</p>

<h3>Estimating Capital Gains Generated by Suites</h3>

<p>To estimate the amount of capital gains tax based on secondary suites expected to accrue annually in the City of Vancouver
we use the inflation-adjusted land value gain of a home as a
proxy for the portion of the appreciation of the property that is not due to improvements the owner made to the home, i.e.
the portion that would be subject to capital gains tax if this wasn&rsquo;t a primary residence.</p>

<p>To compute the taxable gains for an individual property we take the difference between the current land value and the
(inflation-adjusted) present land value at the time the property was bought. To simplify things a little we will consider the
current average (over SFH with suite) land value, the average holding period (years between buy and sell), and the
average inflation-adjusted land value gains. This can be refined, but should suffice for our purposes.</p>

<p>Going back to our estimate, the current (July 2015) average land value of single family homes was $1,640,000. Suites skew toward
the east, where average land value was $1,080,000. We roughly average these and take $1,300,000 as our base value. As
average holing period we take 10 years, and we compute the average inflation-adjusted land value gain between 2005
and 2015 at 10.4% (9.7% on east side). So let&rsquo;s use 10%.</p>

<p>That gives an estimate of the annual land value gain of $850,000 over 10 years, or $85,000 on average per year.
As a sanity check, this is in line with estimates of average annual land value gain for single family properties
<a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">we have done before</a>, although these weren&rsquo;t
inflation-adjusted.</p>

<p>Roughly estimating that on
average a secondary suite takes up about a quarter of the area of a
house, we attribute $21,250 of the $85,000 annual land value gains to the secondary suite for tax purposes.</p>

<p>Of course capital gains tax are only payable when the property sells, but the yearly average gains accumulate and eventually
properties will sell. In which case we expect capital gains taxes to be payed on $21,250 per year the suite was rented.
So if a person sells a home with a suite that was rented for the last 10 years, the person will have to pay capital gains
taxes on $212,500. On average, we expect $21,250 of taxable capital gains income per house with suite per year.</p>

<p>The way capital gains tax works is that half of the gains are taxed at the marginal tax rate. As these tend to be large
sums of money at a time, it is safe to assume that the the effective tax rate on the gains is relatively high, say 35% or
more. Some of the taxes could be deferred by e.g. transferring the gains into RRSPs and later taxed at a lower rate. But
we will ignore this, it has the effect of lowering the effective tax rate on the gains and move the tax payments into the future,
but will not effect the overall amount of capital gains declared.</p>

<p>Assuming an effective tax rate of 35% on the gains we expect on average $3,700 of taxes payed per home with suite per
year through this mechanism. With
conservatively estimated 30,000 secondary suites in Vancouver that adds up to a tidy sum of $111,000,000 for capital gains
taxes paid due to secondary suites in the City of Vancouver. Per year.</p>

<h2>To Rent or Not to Rent Out a Suite</h2>

<p>In the case of rental suites with owner-occupied main portion the suite incurs both income and capital gains tax obligations.
This begs the question what minimum rent is required
to offset the capital gains obligations in a particular year. Typically homeowners don&rsquo;t know the annual value value gains
ahead of time, so this is more of an academic exercise than a practical questions. The important takeaway is that the
capital gains exemption has the effect of dramatically lowering the cap rate for converting part of a principal residence
into a rental unit.</p>

<p>The math is quite simple, we need to compare the net rental income (rent minus deductions) to the capital gains taxes
payable due to the rental. A more comprehensive way to look at this is to look at the
CAP rate based on land value investment, in this case the net rental income (after-tax rental income minus deductions)
per land value attributed to the suite. With our standard assumptions this sits at 1.9%.</p>

<p>This we can compare to the effective tax rate on the capital gains, which is given by multiplying half of the tax rate
(as only half the gains are taxed at that rate) with the rate of land value rise, which comes in at 1.75% with our standard assumptions.</p>

<p>To conclude this calculation we define the net cap rate as the difference between the cap rate and the effective tax rate
on the capital gains. If this is positive, it is the advantageous to rent. If it is negative, leaving the property vacant
will yield higher returns.</p>

<p>More specifically if we want to understand if it was advantageous to rent out a suite in a particular year for a particular
property we can adjust the assumptions in the interactive at the bottom accordingly. For example, in the year 2015 we
saw an average land value gain of about 20%. Adjusting the rate in the interactive, and setting the holding period to 1 year,
we get a net CAP rate of -1.6%. The break-even rent, above which it is advantageous to rent out the suite, was $1,820.</p>

<h2>Taxes Collected</h2>

<p>It would be interesting to compare these estimates with actual taxes collected. But this won&rsquo;t be easy without internal
CRA data. The census for example does not break out rental income. But it does break out capital gains income. Not all
capital gains income stems from selling homes with secondary suites of course and again, the census does not break out
the sources of the capital gains income. But capital gains income is rare enough, especially on the suite-heavy east side,
to yield some information.</p>

<p><a href="https://censusmapper.ca/maps/480#11/49.2360/-123.1361" target="_blank"><img  src="http://doodles.mountainmath.ca/images/capital_gains.png" style="width:50%;float:left;margin-right:10px;"></a>
In the City of Vancouver, a total of
$830 million of net capital gains and losses was reported to the CRA in 2010,
most of which was claimed by people living on the west side. When comparing
to 2010 taxes we should adjust our calculation to the average east side single family land value of $620,000 in 2010 and
might want to assume a very conservative inflation-adjusted land value gain of 5% per year (I have no data before 2005,
but one could use sales data to estimate this better). This would yield an expected taxable annual capital gains of $10,000
per suite in each area (with expected annual taxes generated of $1,750 per suite), adding up to over one third of all
capital gains reported in the City of Vancouver that year. And that&rsquo;s not counting taxable capital gains from selling
investment properties that were not used as principal residence at all and where all the gains are taxable. Or any other
source of capital gains income.</p>

<p><a href="https://censusmapper.ca/maps/480">Eyeballing the total capital gains reported</a>  suggests that tax compliance could be improved. It seems that the
changes in requiring the reporting of a sale of a principal residence that were announced yesterday will do just that.</p>

<h2>Interactive</h2>

<p>For people interested in adjusting the assumptions made for these estimates I made a quick interactive to facilitate this.
Adjust to fit your own assumptions.</p>

<h4>Assumptions</h4>

<table style="border-spacing:1em 0;border-collapse:separate;margin-bottom:20px;">
<tr><td>Secondary Suites</td><td><input type="range" id="suite"></td><td id="suiteValue"></td></tr>
<tr><td>Average Portion of House Taken by Suite</td><td> <input type="range" id="area"></td><td id="areaValue"></td></tr>
<tr><td>Average Rent</td><td> <input type="range" id="rent"></td><td id="rentValue"></td></tr>
<tr><td>Average Rent Deductions</td><td> <input type="range" id="deductions"></td><td id="deductionsValue"></td></tr>
<tr><td>Average Marginal Tax Rate</td><td><input type="range" id="tax"></td><td id="taxValue"></td></tr>
<tr><td>Average SFH Land Value</td><td><input type="range" id="land"></td><td id="landValue"></td></tr>
<tr><td>Average SFH Holding Period</td><td><input type="range" id="years"></td><td id="yearsValue"></td></tr>
<tr><td>Average Inflation-Adjusted Land Value Gain</td><td><input type="range" id="gain"></td><td id="gainValue"></td></tr>
</table>


<h4>Computed Quantities of Interest</h4>

<table style="border-spacing:1em 0;border-collapse:separate;margin-bottom:20px;">
<tr><td>CAP Rate</td><td id="cap_rate"></td></tr>
<tr><td>Effective Cap Gains Tax Rate</td><td id="effective_tax_rate"></td></tr>
<tr><td>Net CAP Rate</td><td id="net_cap_rate"></td></tr>
<tr><td>Break-Even Rent</td><td id="minimal_rent"></td></tr>
</table>


<h4>Annual Taxes Generated</h4>

<table style="border-spacing:1em 0;border-collapse:separate;margin-bottom:20px;">
<tr><td>Annual Income Tax Generated</td><td id="income_tax"></td></tr>
<tr style="display:none;"><td>Average Annual Adjusted Total Land Value Gain</td><td id="av_lv_gain"></td></tr>
<tr><td>Annual Cap Gains Tax Generated</td><td id="cap_gains_tax"></td></tr>
<tr><td>Annual Total Tax Generated</td><td id="total_tax"></td></tr>
</table>




<div><script>
var percentageFormatter=d3.format(".1%");
var currencyFormatter=d3.format("$,.3r");
var numberFormatter=d3.format(",.0f");

var suiteValue=30000,
    rentValue=1000,
    deductionsValue=0.2,
    landValue=1300000,
    gainValue=0.1,
    yearsValue=10.0,
    areaValue=0.25,
    minimalRentValue=1000,
    taxValue=0.35;
document.getElementById('suite').value=(suiteValue-20000.0)/400.0;
document.getElementById('rent').value=(rentValue-500)/20;
document.getElementById('deductions').value=deductionsValue*100;
document.getElementById('area').value=areaValue*100;
document.getElementById('tax').value=taxValue*100;
document.getElementById('gain').value=(gainValue+0.1)/0.6*100;
document.getElementById('land').value=(landValue-200000)/20000;
document.getElementById('years').value=(yearsValue-1)*5;

function updateResults(){
   $('#suiteValue').html(numberFormatter(suiteValue));
   $('#rentValue').html(currencyFormatter(rentValue));
   $('#deductionsValue').html(percentageFormatter(deductionsValue));
   $('#areaValue').html(percentageFormatter(areaValue));
   $('#taxValue').html(percentageFormatter(taxValue));
   $('#landValue').html(currencyFormatter(landValue));
   $('#gainValue').html(percentageFormatter(gainValue));
   $('#yearsValue').html(numberFormatter(yearsValue));

   var value1=suiteValue*rentValue*(1-deductionsValue)*taxValue*12.0;
   $('#income_tax').html(currencyFormatter(value1));
   var lvGain=landValue*(1-Math.pow(1-gainValue,yearsValue))/yearsValue;
   var value2=suiteValue*taxValue*areaValue*lvGain/2.0;
   $('#av_lv_gain').html(currencyFormatter(lvGain));
   $('#cap_gains_tax').html(currencyFormatter(value2));
   $('#total_tax').html(currencyFormatter(value1+value2));
   var minimalRent=landValue*areaValue*gainValue/2*taxValue/(1-taxValue)/(1-deductionsValue)/12.0;
   $('#minimal_rent').html(currencyFormatter(minimalRent));
   var capRate=rentValue*12*(1-deductionsValue)*(1-taxValue)/(landValue*areaValue);
   $('#cap_rate').html(percentageFormatter(capRate));
   var effectiveRate=taxValue/2.0*gainValue;
   $('#effective_tax_rate').html(percentageFormatter(effectiveRate));
   $('#net_cap_rate').html(percentageFormatter(capRate-effectiveRate));
}

$('#suite').on('change',function(){
    suiteValue=parseFloat(this.value)*400.0+20000.0;
    updateResults();
});
$('#area').on('change',function(){
    areaValue=parseFloat(this.value)/100.0;
    updateResults();
});
$('#rent').on('change',function(){
    rentValue=parseFloat(this.value)*20+500;
    updateResults();
});
$('#land').on('change',function(){
    landValue=parseFloat(this.value)*20000+200000;
    updateResults();
});
$('#deductions').on('change',function(){
    deductionsValue=parseFloat(this.value)/100.0;
    updateResults();
});
$('#gain').on('change',function(){
    gainValue=parseFloat(this.value)/100.0*0.6-0.1;
    updateResults();
});
$('#years').on('change',function(){
    yearsValue=Math.round(parseFloat(this.value)/5.0 + 1.0);
    document.getElementById('years').value=(yearsValue-1)*5;
    updateResults();
});
$('#tax').on('change',function(){
    taxValue=parseFloat(this.value)/100.0;
    updateResults();
});

updateResults();
</script></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sea Level Rise]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/10/02/sea-level-rise/"/>
    <updated>2016-10-02T21:22:25-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/10/02/sea-level-rise</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/elevation/map?sea_level=10&zoom=12&lat=49.2629&lng=-123.1176" target="_blank"><img  src="http://doodles.mountainmath.ca/images/van_sea_level.png" style="width:50%;float:right;margin-left:10px;"></a>
Sea level rise comes up regularly in coastal cities. And nowadays every costal area,
<a href="http://www.env.gov.bc.ca/wsd/public_safety/flood/pdfs_word/cost_of_adaptation-final_report_oct2012.pdf">including Vancouver</a>
has their flood assessment and detailed plans on how to deal with sea level rise, although some plans are
<a href="http://www.sfu.ca/rise/entries/Prescribe-mountains.html">more interesting than others</a>.</p>

<p>The web is awash with sea level rise maps, some static, some interactive, <a href="http://geology.com/sea-level-rise/">some global ones</a>
and some <a href="https://coast.noaa.gov/slr/">national maps</a>. Data sources range from high detail LIDAR data to satellite data
and other datasets. When Mapzen put out their <a href="https://mapzen.com/blog/elevation/">global elevation tiles</a> I gave it a quick
test drive to check it out and then forgot about it. When the topic inevitably
<a href="https://twitter.com/toddsmithdesign/status/782273909265620992">bubbled up on my twitter feed</a> I decided to do some
minimal styling using Mapzen&rsquo;s <a href="https://mapzen.com/products/tangram/">ridiculously easy to use Tangram map engine</a> and
added a search bar to make it easier to jump to different locations on the globe.</p>

<p>That&rsquo;s all there is to this. Check out <a class="btn btn-default" href="https://mountainmath.ca/elevation/map?sea_level=10&zoom=12&lat=49.2629&lng=-123.1176">yet another interactive global seal level rise map</a>.</p>

<p>The preset for the sea level rise is 10m, which is a little excessive. Current predictions hover around 1m until 2100.
Adjust the slider to see how the flooded areas change depending on the simulated sea level rise.</p>

<p>The map is global, and it&rsquo;s worthwhile to compare Vancouver&rsquo;s sea level threat to that of other places, for example
<a href="https://mountainmath.ca/elevation/map?sea_level=0&amp;zoom=9&amp;lat=52.8774&amp;lng=5.5701">Holland where 21% of the country is below current sea level</a>,
not counting any future sea level rise.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Measuring Housing Affordability]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/09/14/measuring-housing-affordability/"/>
    <updated>2016-09-14T12:06:06-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/09/14/measuring-housing-affordability</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/assessment/split_map" target="_blank"><img  src="http://doodles.mountainmath.ca/images/sfh_price_2015.png" style="width:50%;float:right;margin-left:10px;"></a>
Housing affordability is a serious issue that deserves attention. Affordability is generally defined comparing incomes
to housing costs. And ideally also factoring in transportation cost, although that&rsquo;s seldom done and we will not attempt
today.</p>

<p>Recently we <a href="http://doodles.mountainmath.ca/blog/2016/08/31/incomes/">took a detailed look at income data</a>. Using what we
learned, together with our
<a href="https://mountainmath.ca/assessment/split_map">detailed data on the development of prices of single family properties in Vancouver</a>,
we turn to the never dying question of affordability of single family homes.</p>

<!-- more -->


<p><a href="https://mountainmath.ca/assessment/split_map?year=2010" target="_blank"><img  src="http://doodles.mountainmath.ca/images/sfh_price_2010.png" style="width:50%;float:left;margin-right:10px;"></a>
Let&rsquo;s go through the motions of how different choices in income data can lead to different stories. Some general choices of
what income to use is often pretty obvious. Others are more subtle and a matter of judgement. For simplicity we focus
entirely on the situation in 2010, because that&rsquo;s the last year we have detailed income data for the City of Vancouver
and we also have <a href="https://mountainmath.ca/assessment/split_map?year=2010">detailed publicly available single family housing price data</a>.</p>

<p>When talking about housing, the starting point is to look at income of (private) households (as opposed to individual income).
We may also want to consider the owner-renter split in the region we are interested in. For simplicity we will skip this.</p>

<h3>Household Income</h3>

<p>Households are as diverse as
dwelling types. If we write a story about single family housing, household income of all households is too broad to be
relevant. For example,
in the city of Vancouver there are about 75,500 single family lots, but 264,573 households. So at most 28% of households
can live in the main house on a single family lot. Taking the median household income makes no sense in this situation.
We need to narrow down the target group. In Calgary on the other hand,
59% of the dwelling stock is single detached (and there are more properties if we count the ones with suites like we
did for Vancouver), so in that case the assumption that the median household is looking to buy a single family home is
more reasonable. Although the median household probably won&rsquo;t be looking to buy the median single family home but a lower-priced one.</p>

<h3>Family Income</h3>

<p>So how should we measure the affordability of single family properties? That&rsquo;s not an easy question. A natural choice may
be to focus in on the 151,330 census families in Vancouver. Again, there are more than twice as many candidates as single
family properties. So maybe narrow it down even further an only look at the 88,515 census
families with children at home. Or the 63,790 couple families with children at home. Realistically speaking, even
going with couple families with children at home there just aren&rsquo;t enough single family properties to go around. Unless we
are willing to force out all of the roughly 45% of the occupants of the main house on single family lots that currently only have
1 or 2 people living in there. But overall, couple families with children is probably a better metric for affordability
for single family homes. Others will of course also buy single family homes, but maybe we will be less worried if
they can&rsquo;t afford one.</p>

<h3>Family with Children Income</h3>

<p>To complete the example, in 2010 the median single family property cost $848,000 (average $1,133,000) and the median
couple family with children income in private households was $92,068 (average $123,252).
That gives a dwelling value to income ratio of around 9.2, showing that the median single family property is not
affordable for the median couple family with children. By affordable we mean that with a 20% down payment the shelter
costs stay below 30% of pre-tax income, which requires a ratio of at most 6.6 assuming an interest rate of 3% and 25
years amortization.</p>

<p>If we were to more realistically assume that the median income couple family with children were to look to buy a single
family property priced at the 10 percentile level of $606,100 we get a dwelling value to income ratio of 6.6, barely
squeezing in below our affordability cutoff.</p>

<h3>Matching Income Quantiles with Housing Quantiles</h3>

<p>Another way to slice the data is to look at the household income distribution. By definition there are 37,750
single family homes more expensive than the median single family home. From the income distribution there are 44,215
households with income over $125,000. Taking $125,000 as our income we get a median dwelling value to income ratio of 6.8.
Only when throwing in some secondary suite rental income can we get to the
&lsquo;barely affordable&rsquo; 6.6 ratio. Adding rental income is double-dipping though, rental income should already be
included in our total income numbers. Assuming secondary suite rental income is correctly reported.</p>

<h3>Using Median Household Income</h3>

<p>Using the household income instead we would have gotten a ratio of 13.4, which is ridiculously unaffordable. But then
again, the notion that the median household should be able to purchase the median single family property
(or even a low-value single family property) is also ridiculous.</p>

<p><a href="https://censusmapper.ca/maps/37" target="_blank"><img  src="http://doodles.mountainmath.ca/images/local_affordability_index.png" style="width:50%;float:right;margin-left:10px;"></a>
Somewhat more reasonable is to <a href="https://censusmapper.ca/maps/37">compare household income to all housing</a>,
not just single family. This still ignores that
half of the population in Vancouver is renting, but still can yield some interesting insights if one is interested in
more detailed spatial distribution of affordability and compare it to other cities.</p>

<p>Adding a scatter plot of local affordability index by proportion of owner households shows how in Vancouver, unlike
in comparable cities, the percentage of owner households is a weak determinant of local affordability.</p>

<h3>What about Affordability Today?</h3>

<p>This analysis assumes that all these households are comfortable to max out when buying a home and that there are no other
competing significant financial obligations like heavy student loans. And, just as a reminder, we were looking at 2010.
As of 2015 the price of the median single family home has increased 67%. We don&rsquo;t have 2015 income numbers for the City of
Vancouver (yet), but Couple Family median income for Metro Vancouver has increased 13% between 2010 and 2014, the last
year with available data. Extrapolating out that trend to 2015, and assuming that whatever other income metrics we were
looking at, we get an estimated income increase by 16%. So any affordability ratio is about 1.4 times worse in 2015 compared
to the 2010 ratio. What may have been &lsquo;barely affordable&rsquo; in 2010 is clearly unaffordable in 2015.</p>

<p>So we used the detailed income data available for 2010 to get very good understanding of the affordability situation in 2010.
Then we used regional income trends to extrapolate that data to 2015 and matched it with 2015 housing data to get more up-to-date
estimates. This is generally an effective method that yields good results.</p>
]]></content>
  </entry>
  
</feed>
