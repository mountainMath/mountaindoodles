<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Mountain Doodles]]></title>
  <link href="http://doodles.mountainmath.ca/feed.xml" rel="self"/>
  <link href="http://doodles.mountainmath.ca/"/>
  <updated>2016-04-02T17:26:36-07:00</updated>
  <id>http://doodles.mountainmath.ca/</id>
  <author>
    <name><![CDATA[MountainMath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On Houses and Dirt]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/04/01/on-dirt-and-houses/"/>
    <updated>2016-04-01T22:30:43-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/04/01/on-dirt-and-houses</id>
    <content type="html"><![CDATA[<p>The story of Vancouver real estate is mostly a story of dirt. After spending a bit of time to collect relevant data I
am now wondering how to make better visualizations to make that data more accessible.</p>

<!-- more -->


<p>Looking at my <a href="http://www.mountainmath.ca/map/assessment">old maps based around assessment data</a>, there are a couple of things that bother me. They are not very
good at showing changes over time. And they are not very good at highlighting the different role of dirt and the
houses on top. Yes, they do have separate views for building value, land value and various changes over time.
But the visuals aren&rsquo;t catchy and intuitive. When mapping building values I show the properties the buildings sit
on, instead of the building footprint. When mapping value increases I just map a some kind of ratio of values between
specific years. I use colours to visualize values, highlighting large differences but losing the smaller differences
within the colour brackets.</p>

<p>What would a better visual look like? I want to better explain the different roles played by dirt and houses. But
retain interactivity and the ability to zoom and pan around.</p>

<h3>Better Visualizations</h3>

<p>The idea is to map buildings and properties on the same map. Using 3D mapping we can uses height as an intuitive
representation for value, adding colour brackets for additional reference as values push through emotionally relevant
brackets, like the $1m barrier. To show change over time we just add a time slider and animate the whole thing.
To keep things simple and comparable we will focus on single family homes for now.
<a href="http://www.mountainmath.ca/assessment/split_map" target="_blank"><img  src="http://doodles.mountainmath.ca/images/vanre.gif" style="margin:10px;"></a></p>

<p>We build three maps, one showing land and buildings on the same map, one just showing land values and one just showing building
values. All three can be animated over time.</p>

<p>We normalized all dollar amounts to 2015 by adjusting for inflation. There are good reasons for going either
way, adjusting for inflation or keeping it in original dollar amounts. For this animation we felt the it was better to show
real (inflation adjusted) value growth. Keep in mind that this means is that if a property cost $1,000,000 in 2005 dollars we will
show the value for 2005 to be $1,175,686 in 2015 dollars.</p>

<h3>The Rub</h3>

<p>I find these maps much more intuitive and instructive than earlier maps I did. But it comes at a price &ndash; these maps
will give your GPU a run for it&rsquo;s money. And they are only visible on modern computers and browsers. They can be viewed
on mobile too, but don&rsquo;t try to zoom out. They eat up quite a bit of memory and may crash your mobile browser.</p>

<p>Go test how &ldquo;modern&rdquo; your computer is. It&rsquo;s kind of fun to view the houses in different areas evolve over time. Try out
the <a href="http://www.mountainmath.ca/assessment/split_map">interactive animations</a> with just the houses or with both, houses and land.</p>

<h3>Local Stories</h3>

<p><a href="http://mountainmath.ca/assessment/split_map?zoom=17&lat=49.2362&lng=-123.1365&type=1&use3D=1&year=2015" target="_blank"><img  src="http://doodles.mountainmath.ca/images/s_vanre.gif" style="width:50%;float:left;margin-right:10px;"></a>
Zooming into these maps we can start to see the local stories of the Vancouver single family homes during the last 11 years.
Over time one sees where the land value is doing almost all of the lifting, but occasionally one sees re-development or
extensive re-modeling activity that pushes up the building value as buildings try to catch up with the sharp rise in land
value.</p>

<p>And this is really the essence of the Vancouver real estate story when viewing dirt vs buildings. The land values are rising
so fast that the building values don&rsquo;t have time to catch up. In fact, during the last year alone the aggregate land values of all single family
homes <a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">exceeded the combined pre-tax income of all Vancouverites</a>.</p>

<p>The speed of the rise in land values is also driving the rapid re-development
of buildings as explained <a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">in an earlier post</a>.</p>

<h3>Unoffordable Dirt</h3>

<p><a href="http://mountainmath.ca/assessment/split_map?zoom=17&lat=49.2362&lng=-123.1365&type=3&use3D=1&year=2006" target="_blank"><img  src="http://doodles.mountainmath.ca/images/b_vanre.gif" style="width:50%;float:right;margin-left:10px;"></a>
The visualizations show that Vancouver&rsquo;s &ldquo;unaffordable housing&rdquo; is really all about &ldquo;unaffordable dirt&rdquo; and not &ldquo;unaffordable houses&rdquo;.
Looking at only the houses, we see that while there are some multi-million dollar houses, most houses are very affordable. It&rsquo;s just the
dirt they sit on that is really expensive.</p>

<p>This distinction does not matter much if one is buying a house. But it does matter when making policy decisions. One way
to make housing more affordable is to reduce the impact of the land value. Converting a single detached house into a
duplex immediately cuts the land value in half. If we replace it with a 4-plex, land values go down by a quarter for each
housing unit.</p>

<h3>Next Steps</h3>

<p>This points us toward investigating how the evolution of the single detached property value compares to that of duplexes,
4-plexes or, more generally, the &ldquo;missing middle&rdquo;. We should add stratified units into the visualization and see how
they fair (taking the average over all units in each building) in terms of affordability and how this compares to
single detached.</p>

<h3>Data</h3>

<p>The data used for this consist of merging four datasets. The Vancouver property boundary data set, the Vancouver
tax assessment dataset (for years 2006 and 2016), the Vancouver building footprint dataset (based on 2009 LIDAR data)
and the Metro Vancouver land use dataset to help identify single family homes, especially the 15% outside of RS zoned areas.
The tax assessment dataset gives an estimate of the values on July 1st of the year indicated in the visualization.</p>

<p>The datasets all have their issues and are a little out of sync. The property dataset is for a fixed point in time and
may not properly reflect the legal state of properties for all years (when for example properties were joined or split).
The tax dataset is not in all cases properly linked to the properties, in particular if the tax code changed which happens
in some but not all cases when the building gets redeveloped. There are 854 such single family home properties in Vancouver
that we can&rsquo;t trace for the whole time period. On
the map they will be visible as greyed out properties for earlier years that in later years spring to life as we can trace them.</p>

<p>The building dataset represents a snapshot in time around
2009. The Land Use dataset is lagging behind a bit and has for example not caught up with recent re-developments along
the Cambie corridor.</p>

<p>The land and building value data is estimated by BC Assessment and is based on sales of comparable properties in the time before and after July 1 and
takes into account information from building permits and other evaluations. On an individual building level, especially for older buildings that
have not been sold for a long time, the data can be somewhat inaccurate at times. But in aggregate it provides a fairly
accurate snapshot of the property and building values.</p>

<p>Neither the zoning information, not the land use have been updated to catch some of these freshly developed land assemblies.
I have manually removed a good dozen obvious ones from the dataset, there are probably some more hidden in there.</p>

<p>Most of the problems stem from the fact that BCAssessment data, which has much more comprehensive and cleaner data,
is not directly accessible.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Mixed Use]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/03/28/on-mixed-use%20(1)/"/>
    <updated>2016-03-28T23:37:16-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/03/28/on-mixed-use (1)</id>
    <content type="html"><![CDATA[<p>Redeveloping single storey commercial properties into mixed use is taking off in Vancouver right now. It&rsquo;s a little frustrating
to see how pretty much every story I have seen on this get the effect this has on property taxes wrong, including one on the generally quite good
<a href="https://pricetags.wordpress.com/2016/01/25/guest-on-retail-and-an-evolving-environment-with-a-short-historic-note/">Price Tags blog</a>.
Property taxes are an important piece of the puzzle in Vancouver, so I decided to go into a little detail on this.</p>

<p>Long story short, re-developing single story commercial into mixed use lowers the commercial property taxes. Dramatically.</p>

<!-- more -->


<h3>Commercial Property Taxes</h3>

<p>In Vancouver, the commercial property tax rate is five times the residential rate. Toronto also has higher property tax
rates for commercial, 3x in their case, I don&rsquo;t know how this plays out more broadly. Nor do I know when and why these
decisions were made. Generally cities have a hard time to keep space for residential areas in or close to the central
business district, and higher property taxes may help to uphold the zoning restrictions in place and keep residential
properties in close proximity to commercial.</p>

<p>The effect of the unequal tax rates is quite stark
<a href="http://www.mountainmath.ca/assessment_gl/map">when visualizing taxes collected per area</a>. And this keeps residential
property taxes low.</p>

<p>If Vancouver were isolated from everyone else it would not make much of a difference. Stores pass the
higher property taxes on to residents by charging higher prices, and offices pass them on to employees through lower wages.
Things get more complex when Vancouver is seen as part of metro, or Canada or in the world. But that discussion goes into
a different direction.</p>

<p><a href="http://www.mountainmath.ca/assessment_gl/map" target="_blank"><img  src="http://doodles.mountainmath.ca/images/tax_density_mixed_use.png" style="width:50%;float:right;margin-left:10px;"></a>
On the <a href="http://www.mountainmath.ca/assessment_gl/map">interactive 3D tax density map</a> we can clearly see that the some mixed use properties (in darker red) generate lower total property
taxes than comparable pure commercial properties nearby. We will explore this in more detail in the next section.</p>

<p>For Vancouver it is also important to note that commercial leases are generally trip net, in particular the tenant pays
the property taxes. And they are quite substantial. The important number is the gross rent, which is the sum of rent, taxes,
utilities and other costs borne by tenants. That number is determined by the willingness of a tenant to lease the commercial
space, which in turn determines the rent a commercial landlord can charge for the space. In theory at least, in practice
some commercial landlords develop a relationship with their tenants and don&rsquo;t increase rents for old tenants as much as
the market would bear.</p>

<h3>Taxes for Commercial vs Mixed Use</h3>

<p><img  src="http://doodles.mountainmath.ca/images/w_broadway.png" style="width:50%;float:left;margin-right:10px;">
Let&rsquo;s look at and example, the adjacent properties at
3071 and 3063 W Broadway. The first is single story commercial, the latter two story mixed use. They have almost identical
lot sizes and assessed land and building values, with the mixed use property having slightly higher overall assessed value.</p>

<p>How about the property taxes? In 2015, the 2016 taxes aren&rsquo;t out yet, the single story generated $47,797 in taxes, the mixed
use only $32,031. The reason is simple. The single story property
simply pays 5x the residential rate. For mixed use, the value gets divided up between two stories, so half the value pays
5x residential rate, the other half gets taxes at 1x residential rate. So the effective tax rate for the mixed use is 3x
residential rate. Well, the actual effective rate is 3.35x residential, probably because the first floor carries a little
higher value than the second floor.</p>

<p>The net effect is that the mixed use commercial tenant pays just over half of the property taxes as the pure commercial
tenant next door. So the mixed use commercial landlord will charge exactly that much more rent, so that the gross rent
for both commercial tenants is about the same.</p>

<p>Feel free to play with the <a href="http://mountainmath.ca/map/assessment?zoom=14&amp;layer=10">tax density</a> and
<a href="http://mountainmath.ca/map/assessment?zoom=14&amp;layer=14">land use</a> map and click into properties to see a more detailed break-down of
taxes. Be warned that the land use dataset is not perfect in identifying every commercial vs mixed use place.</p>

<h3>A Re-Developmet Example</h3>

<p><img  src="http://doodles.mountainmath.ca/images/main1.png" style="width:50%;float:right;margin-left:10px;">
Let&rsquo;s see how this works when a property gets re-developed. To keep things simple, let&rsquo;s look at something that went
from single story commercial to two story mixed use. 6621 Main Street got re-developed around 2009/2010 and now spans
addresses 6615, 6621 and 6623.</p>

<p>Behind the original 6621 are now three stratified residential units, the other two addresses are the front doors for the
two commercial units that replace the two commercial units behind the original 6621 address.</p>

<p><img  src="http://doodles.mountainmath.ca/images/main2.png" style="width:50%;float:left;margin-right:10px;">
So what happened to property taxes from 2009 to 2010? In 2009 the combined tax levy was $22,102,
in 2010 it was $16,859, of which $2,994 was payed by residential stratas and $13,865 by the two commercial.</p>

<p>For comparison,
the commercial property next door saw their property tax climb lightly from $20,754 in 2009 to $20,794 in 2010. So the
decrease in property taxes we see is really due to the re-development to mixed use.</p>

<h3>Bottom Line</h3>

<p>So what&rsquo;s the bottom line for mixed use re-development? Depends for who, but overall it looks pretty good.</p>

<h5>City bottom line</h5>

<p>For conversion to two story mixed use, the city collects less
property taxes than before and the overall mill rate goes up a tiny amount. Conversion to three stories or higher and
total tax revenue for the property goes up and the overall mill rate goes down a tiny amount.</p>

<h5>Commercial tenant bottom line</h5>

<p>The property tax rate goes down substantially, but rent will go up as the gross rent is determined by the market. So not
much changed assuming the form factor of the stores stay the same. In theory at least. If the commercial land lord was
giving a long time tenant a break on their rent before re-development, the tenant will have a hard time to find a different
commercial location with comparable gross rent.</p>

<h5>Residential tenant bottom line</h5>

<p>Added supply can only help. In some cases units will get stratified and sold off, in other cases the property owner keeps
the residential units as rental.</p>

<h5>Property owner bottom line</h5>

<p>Especially with the recent change in lending practices that focus on cache flow, mixed use re-development becomes very attractive.
After re-development, landlords can charge higher rents because commercial property taxes go down. And they can rent out
the residential units for added cash flow. That way, when they take that property to the bank to get a loan for their
next project they can show a large cache flow and get a bigger loan. And the re-development cycle keeps going. And with
almost everyone re-financing their properties a couple of years ago at roughly the same time, when interest rates hit rock-bottom, everyone is
now gearing up at the same time to re-develop.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Mixed Use]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/03/28/on-mixed-use/"/>
    <updated>2016-03-28T23:37:16-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/03/28/on-mixed-use</id>
    <content type="html"><![CDATA[<p>Redeveloping single storey commercial properties into mixed use is taking off in Vancouver right now. It&rsquo;s a little frustrating
to see how pretty much every story I have seen on this get the effect this has on property taxes wrong, including one on the generally quite good
<a href="https://pricetags.wordpress.com/2016/01/25/guest-on-retail-and-an-evolving-environment-with-a-short-historic-note/">Price Tags blog</a>. People
claim that converting single story commercial to mixed use pushes up the property taxes for the commercial tenants.
Property taxes are an important piece of the puzzle in Vancouver, so I decided to go into a little detail on this.</p>

<p>Long story short, re-developing single story commercial into mixed use lowers the commercial property taxes. Dramatically.</p>

<!-- more -->


<h3>Commercial Property Taxes</h3>

<p>In Vancouver, the commercial property tax rate is five times the residential rate. Toronto also has higher property tax
rates for commercial, 3x in their case, I don&rsquo;t know how this plays out more broadly. Nor do I know when and why these
decisions were made. Generally cities have a hard time to keep space for residential areas in or close to the central
business district, and higher property taxes may help to uphold the zoning restrictions in place and keep residential
properties in close proximity to commercial.</p>

<p>The effect of the unequal tax rates is quite stark
<a href="http://www.mountainmath.ca/assessment_gl/map">when visualizing taxes collected per area</a>. And this keeps residential
property taxes low.</p>

<p>If Vancouver were isolated from everyone else it would not make much of a difference. Stores pass the
higher property taxes on to residents by charging higher prices, and offices pass them on to employees through lower wages.
Things get more complex when Vancouver is seen as part of metro, or Canada or in the world. But that discussion goes into
a different direction.</p>

<p><a href="http://www.mountainmath.ca/assessment_gl/map" target="_blank"><img  src="http://doodles.mountainmath.ca/images/tax_density_mixed_use.png" style="width:50%;float:right;margin-left:10px;"></a>
On the <a href="http://www.mountainmath.ca/assessment_gl/map">interactive 3D tax density map</a> we can clearly see that the some mixed use properties (in darker red) generate lower total property
taxes than comparable pure commercial properties nearby. We will explore this in more detail in the next section.</p>

<p>For Vancouver it is also important to note that commercial leases are generally trip net, in particular the tenant pays
the property taxes. And they are quite substantial. The important number is the gross rent, which is the sum of rent, taxes,
utilities and other costs borne by tenants. That number is determined by the willingness of a tenant to lease the commercial
space, which in turn determines the rent a commercial landlord can charge for the space. In theory at least, in practice
some commercial landlords develop a relationship with their tenants and don&rsquo;t increase rents for old tenants as much as
the market would bear.</p>

<h3>Taxes for Commercial vs Mixed Use</h3>

<p><img  src="http://doodles.mountainmath.ca/images/w_broadway.png" style="width:50%;float:left;margin-right:10px;">
Let&rsquo;s look at and example, the adjacent properties at
3071 and 3063 W Broadway. The first is single story commercial, the latter two story mixed use. They have almost identical
lot sizes and assessed land and building values, with the mixed use property having slightly higher overall assessed value.</p>

<p>How about the property taxes? In 2015, the 2016 taxes aren&rsquo;t out yet, the single story generated $47,797 in taxes, the mixed
use only $32,031. The reason is simple. The single story property
simply pays 5x the residential rate. For mixed use, the value gets divided up between two stories, so half the value pays
5x residential rate, the other half gets taxes at 1x residential rate. So the effective tax rate for the mixed use is 3x
residential rate. Well, the actual effective rate is 3.35x residential, probably because the first floor carries a little
higher value than the second floor.</p>

<p>The net effect is that the mixed use commercial tenant pays just over half of the property taxes as the pure commercial
tenant next door. So the mixed use commercial landlord will charge exactly that much more rent, so that the gross rent
for both commercial tenants is about the same.</p>

<p>Feel free to play with the <a href="http://mountainmath.ca/map/assessment?zoom=14&amp;layer=10">tax density</a> and
<a href="http://mountainmath.ca/map/assessment?zoom=14&amp;layer=14">land use</a> map and click into properties to see a more detailed break-down of
taxes. Be warned that the land use dataset is not perfect in identifying every commercial vs mixed use place.</p>

<h3>A Re-Developmet Example</h3>

<p><img  src="http://doodles.mountainmath.ca/images/main1.png" style="width:50%;float:right;margin-left:10px;">
Let&rsquo;s see how this works when a property gets re-developed. To keep things simple, let&rsquo;s look at something that went
from single story commercial to two story mixed use. 6621 Main Street got re-developed around 2009/2010 and now spans
addresses 6615, 6621 and 6623.</p>

<p>Behind the original 6621 are now three stratified residential units, the other two addresses are the front doors for the
two commercial units that replace the two commercial units behind the original 6621 address.</p>

<p><img  src="http://doodles.mountainmath.ca/images/main2.png" style="width:50%;float:left;margin-right:10px;">
So what happened to property taxes from 2009 to 2010? In 2009 the combined tax levy was $22,102,
in 2010 it was $16,859, of which $2,994 was payed by residential stratas and $13,865 by the two commercial.</p>

<p>For comparison,
the commercial property next door saw their property tax climb lightly from $20,754 in 2009 to $20,794 in 2010. So the
decrease in property taxes we see is really due to the re-development to mixed use.</p>

<h3>Bottom Line</h3>

<p>So what&rsquo;s the bottom line for mixed use re-development? Overall it looks pretty good, let&rsquo;s go down the list of all
parties involved.</p>

<h5>City bottom line</h5>

<p>For conversion to two story mixed use, the city collects less
property taxes than before and the overall mill rate goes up a tiny amount. Conversion to three stories or higher and
total tax revenue for the property goes up and the overall mill rate goes down a tiny amount.</p>

<h5>Commercial tenant bottom line</h5>

<p>The property tax rate goes down substantially, but rent will go up as the gross rent is determined by the market. So not
much changed assuming the form factor of the stores stay the same. In theory at least. If the commercial land lord was
giving a long time tenant a break on their rent before re-development, the tenant will have a hard time to find a different
commercial location with comparable gross rent.</p>

<h5>Residential tenant bottom line</h5>

<p>Added supply can only help. In some cases units will get stratified and sold off, in other cases the property owner keeps
the residential units as rental.</p>

<h5>Property owner bottom line</h5>

<p>Especially with the recent change in lending practices that focus on cache flow, mixed use re-development becomes very attractive.
After re-development, landlords can charge higher rents because commercial property taxes go down. And they can rent out
the residential units for added cash flow. That way, when they take that property to the bank to get a loan for their
next project they can show a large cache flow and get a bigger loan. And the re-development cycle keeps going. And with
almost everyone re-financing their properties a couple of years ago at roughly the same time, when interest rates hit rock-bottom, everyone is
now gearing up at the same time to re-develop.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Surrey Traffic Loop Counts]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/03/27/surrey-traffic-loop-counts%20(1)/"/>
    <updated>2016-03-27T01:04:45-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/03/27/surrey-traffic-loop-counts (1)</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/land_use/map"><img  src="http://doodles.mountainmath.ca/images/surrey_traffic.png" style="width:50%;float:right;margin-left:10px;"></a>
Surrey published a <a href="http://data.surrey.ca/dataset/ff9c223a-57e2-49b8-900f-e4b9d9423b4b">beta version of their traffic loop counts</a>,
which is pretty awesome. Real life traffic data is very exciting, and there are lots of fun things one could do with that.
So last night I decided to take a look and make a quick map. Nothing exciting yet, just to feel may way around
what&rsquo;s there.</p>

<!-- more -->


<p>To keep things simple I again took advantage of the <a href="https://mapzen.com/projects/tangram/">awesome Tangram mapping enginge</a>
and turned it onto the traffic loop data. All I did was plot circles for each traffic loop with the area proportional to
the count. Just to get an idea what the traffic data looks like.</p>

<iframe src="http://doodles.mountainmath.ca/surrey_traffic_map.html" width="100%" height="550"></iframe>


<p><a href="http://doodles.mountainmath.ca/surrey_traffic_map.html">Full screen view</a></p>

<h3>What do we see?</h3>

<p>Really not much at this point, it&rsquo;s just a snapshot of one hour of traffic on March 24 between 9am and 10am. But it&rsquo;s pretty
obvious where to go from here. One should add some dynamic way to select the time frame. And add some animations to better
represent car traffic and the direction in which it is moving. The cool <a href="https://tangrams.github.io/carousel/?tron#15/49.1055/-122.8244">tron demo</a>
simulates traffic movement, maybe the loop count data could be massaged in a way to give a more accurate representation
of actual traffic.</p>

<p>And of course one could start to run some analysis. The fifteen minute aggregates which the server sends down are a little rough for doing some
traffic flow analysis, but I am sure that together with the lane direction information attached to the traffic loop geographic
data something interesting can be done.</p>

<h3>Hickups and feedback</h3>

<p>There were a couple of hickups along the way. The Surrey Open Data API sends down GeoJSON data in NAD 83 UTM Zone 10
instead of the default WGS84. Which is just fine, but when using the Mapzen Tangram engine we need to transform
it into WGS84. The fancy way would be to do that dynamically after directly consuming the Surrey API, but we were lazy
and just downloaded and transformed the file. So now we have the loop counter locations and can map them.</p>

<p>Next up, we want to add the traffic count data to the loop counter locations. Ideally we want to consume the Surrey open
data API and link it with the geographic loop data, but the API does not set the <code>Access-Control-Allow-Origin: *</code>
http header to allow cross-origin requests to consume the API directly from their web app. Again, no big deal when building a small
testing app, we simply downloaded the traffic loop counts for noon on March 23 and threw them up on our server.</p>

<p>Pretty smooth overall for a first run. Timestamps are a little funny, they are in local time. While easy to interpret, this
will cause problems when analysing data around daylight savings time changes.</p>

<p>Lastly we needed to add a quick hack to the Tangram mapping engine. For some reason the <a href="https://mapzen.com/documentation/tangram/sources/#transform">transform</a>
function does not get called when adding static (non-tiled) data sources. But that was easy to fix, although it took some
sloothing to track down.</p>

<p>At this point it seems that not all traffic loops are hooked up to the API yet. Going through the data, on the March 23
there are only a handful of traffic loops active, on the 24th the number of active loops jumps up significantly.
I also plotted the traffic loop locations without data (in yellow), just to give
and idea what will become available. Curious to see where this will go once out of beta mode.</p>

<h3>Want to make your own map?</h3>

<p>It&rsquo;s pretty easy. Just grab the html file from the &ldquo;full screen&rdquo; link above and download the <a href="http://doodles.mountainmath.ca/surrey_traffic_scene.yaml">Tangram scene file</a>
and add your own twist to the map. Also grab a copy of the tangram engine <a href="http://doodles.mountainmath.ca/javascripts/tangram.debug.js">with the quick-fix for the transform function</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Surrey Traffic Loop Counts]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/03/27/surrey-traffic-loop-counts/"/>
    <updated>2016-03-27T01:04:45-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/03/27/surrey-traffic-loop-counts</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/land_use/map"><img  src="http://doodles.mountainmath.ca/images/surrey_traffic.png" style="width:50%;float:right;margin-left:10px;"></a>
Surrey published a <a href="http://data.surrey.ca/dataset/ff9c223a-57e2-49b8-900f-e4b9d9423b4b">beta version of their traffic loop counts</a>,
which is pretty awesome. Real life traffic data is very exciting, and there are lots of fun things one could do with that.
So last night I decided to take a look and make a quick map. Nothing exciting yet, just to feel may way around
what&rsquo;s there.</p>

<!-- more -->


<p>To keep things simple I again took advantage of the <a href="https://mapzen.com/projects/tangram/">awesome Tangram mapping enginge</a>
and turned it onto the traffic loop data. All I did was plot circles for each traffic loop with the area proportional to
the count. Just to get an idea what the traffic data looks like.</p>

<iframe src="http://doodles.mountainmath.ca/surrey_traffic_map.html" width="100%" height="550"></iframe>


<p><a href="http://doodles.mountainmath.ca/surrey_traffic_map.html">Full screen view</a></p>

<h3>What do we see?</h3>

<p>Really not much at this point, it&rsquo;s just a snapshot of one hour of traffic on March 24 between 9am and 10am. But it&rsquo;s pretty
obvious where to go from here. One should add some dynamic way to select the time frame. And add some animations to better
represent car traffic and the direction in which it is moving. The cool <a href="https://tangrams.github.io/carousel/?tron#15/49.1055/-122.8244">tron demo</a>
simulates traffic movement, maybe the loop count data could be massaged in a way to give a more accurate representation
of actual traffic.</p>

<p>And of course one could start to run some analysis. The fifteen minute aggregates which the server sends down are a little rough for doing some
traffic flow analysis, but I am sure that together with the lane direction information attached to the traffic loop geographic
data something interesting can be done.</p>

<h3>Hickups and feedback</h3>

<p>There were a couple of hickups along the way. The Surrey Open Data API sends down GeoJSON data in NAD 83 UTM Zone 10
instead of the default WGS84. Which is just fine, but when using the Mapzen Tangram engine we need to transform
it into WGS84. The fancy way would be to do that dynamically after directly consuming the Surrey API, but we were lazy
and just downloaded and transformed the file. So now we have the loop counter locations and can map them.</p>

<p>Next up, we want to add the traffic count data to the loop counter locations. Ideally we want to consume the Surrey open
data API and link it with the geographic loop data, but the API does not set the <code>Access-Control-Allow-Origin: *</code>
http header to allow cross-origin requests to consume the API directly from their web app. Again, no big deal when building a small
testing app, we simply downloaded the traffic loop counts for noon on March 23 and threw them up on our server.</p>

<p>Pretty smooth overall for a first run. Timestamps are a little funny, they are in local time. While easy to interpret, this
will cause problems when analysing data around daylight savings time changes.</p>

<p>Lastly we needed to add a quick hack to the Tangram mapping engine. For some reason the <a href="https://mapzen.com/documentation/tangram/sources/#transform">transform</a>
function does not get called when adding static (non-tiled) data sources. But that was easy to fix, although it took some
sloothing to track down.</p>

<p>At this point it seems that not all traffic loops are hooked up to the API yet. Going through the data, on the March 23
there are only a handful of traffic loops active, on the 24th the number of active loops jumps up significantly.
I also plotted the traffic loop locations without data (in yellow), just to give
and idea what will become available. Curious to see where this will go once out of beta mode.</p>

<h3>Want to make your own map?</h3>

<p>It&rsquo;s pretty easy. Just grab the html file from the &ldquo;full screen&rdquo; link above and download the <a href="http://doodles.mountainmath.ca/surrey_traffic_scene.yaml">Tangram scene file</a>
and add your own twist to the map. Also grab a copy of the tangram engine <a href="http://doodles.mountainmath.ca/javascripts/tangram.debug.js">with the quick-fix for the transform function</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unoccupied Dwellings]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/03/08/unoccupied-dwellings/"/>
    <updated>2016-03-08T14:07:47-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/03/08/unoccupied-dwellings</id>
    <content type="html"><![CDATA[<p>Today the City of Vancouver <a href="http://council.vancouver.ca/20160308/documents/rr1presentation.pdf">released their report</a>
on unoccupied dwelling units in the city. I watched part of the presentation and read through the report, and from all
that I can see the methodology used is very solid.</p>

<p>I have seen some confusion and even some incorrect reporting on this, so I thought it would be worthwhile to look into
the report in detail.</p>

<!-- more -->


<h3>The Data</h3>

<p>Smart meter data can be used to determine very accurately which units
are occupied and which ones are not. In theory at least, in practice this is quite difficult because of the sheer magnitude
of the data. That&rsquo;s why this is best left to companies who specialize in this kind of analysis, which is exactly what the
city has done.</p>

<p>The only problem is that not all units have smart meters. I don&rsquo;t know how far BCHydro has come with their rollout, but
most condo units should have smart meters by now, as do newer single family units. So what to do about the housing that
does not have smart meters? That&rsquo;s where things get tricky. Some studies I have seen use cutoffs for monthly electricity
consumption to determine which are unoccupied. The problem is to find the right cutoff. Typically these studies report
rates computed using several cutoffs to address that uncertainty. The problem is that different cutoffs give very different
results. For example, <a href="http://www.btaworks.com/wp-content/uploads/2009/12/btaworks_condo_study_report_final2.pdf">this study</a>
looked at 75kWh and 100kWh as monthly consumption cutoffs and reported 5.5% and 8.5% unoccupancy rate, respectively. I checked my condo unit
when it was unoccupied for two months one summer, my two fridges pushed me well above the 100kWh threshold. So my unoccupied
unit would have been counted as occupied in this study.</p>

<p>So how did the new study deal with this problem. We did not hear technical details, but during the presentation it was
explained that smart meter data was used to &ldquo;train&rdquo; a model to use dumb meter data. That&rsquo;s exactly the right approach. Going
by the fact that everything was done right I am reasonably comfortable to assume that this part was also done in a way to
give accurate aggregate numbers.</p>

<p>I am hoping that the technical report will still come out so we can be confident that the details were taken care of
properly. In particular I would like to know how the model dealt with different housing types, and how the housing type
was tied to the electricity bill. What ratio of dwellings of each housing type had smart meter data available? And most
importantly, what are the uncertainty estimates on the numbers presented?</p>

<p>Bottom line on data: Solid.</p>

<h3>Results</h3>

<p>The big takeaway is this one graph:</p>

<p><img  src="http://doodles.mountainmath.ca/images/unoccupied-trends.png" style="width:50%;float:left;margin-right:10px;">
It shows that no matter what timeframe is used to define &ldquo;unoccupied&rdquo;, the rate stayed flat over time. In context of
the current affordability discussion this means that this is unlikely a driving factor of Vancouver&rsquo;s skyrocketing
real estate prices. But the absolute numbers are still quite large and there is a lot of untapped potential. While some people
have argued that supply has not been the main part of the cause of the price hike in Vancouver, it is clear that it must
be a big part of any solution. Finding effective ways to get this untapped supply online should be a priority, it is easier
and more efficient than building new housing in NIMBY-zoned Vancouver.</p>

<p>The other takeaway is that unoccupied units are dominated by condos.</p>

<p><img  src="http://doodles.mountainmath.ca/images/unoccupied-apartment.png" style="width:50%;float:right;margin-left:10px;">
The graph shows the rate for apartments, which also includes purpose built rentals. As the report points out, if we
take out the purpose-built rentals, which have vacancy rates well below 1% at any given time and are extremely unlikely
to stay vacant for two or more months, we arrive at a rate of about 12.5% of condos that are unoccupied in the City of
Vancouver. As should be expected, that&rsquo;s significantly higher than what the non-smart meter studies picked up.</p>

<h3>Are rates in Vancouver higher than elsewhere?</h3>

<p>That depends exactly on what the question asks. And this is where, in my opinion, the city report fails to give a good
answer.</p>

<p><img  src="http://doodles.mountainmath.ca/images/unoccupied-apartment-comparison.png" style="width:50%;float:left;margin-right:10px;">
The City implied, <a href="http://www.theglobeandmail.com/news/british-columbia/vancouvers-vacant-homes-in-line-with-other-cities-but-condos-most-likely-empty/article29072573/">and newspapers now report</a>
that Vancouver numbers are &ldquo;in line&rdquo; with other comparable cities. That claim is based on this graphic,
which is included in the report and stems from a <a href="http://www.urbanfutures.com/foreign-unoccupied/">study by Urban Futures</a>.
The study shows that the Metro Vancouver&rsquo;s rate of unoccupied <em>apartments</em> is not much different from other Metro regions.</p>

<p>The problem is that this study is read as giving the <em>overall</em> occupancy rates are similar across metro regions, which is not
the case. This is a fine point and a little technical, let&rsquo;s take a look at the actual rates of unoccupied dwellings in
the larges metro areas.</p>

<div id="unoccupied_cma" class="land_use" style="width:300px;float:right"></div>


<p>We took these numbers from a <a href="http://www.cmhc-schl.gc.ca/odpub/pdf/68323.pdf?lang=en">CMHC report</a> that looked into this
question and used the same data and methodology as Urban Futures, except they based it on all dwelling units instead of
just apartments. This difference is quite stunning, and it can be explained by the fact that e.g. Metro Vancouver&rsquo;s
housing units are to 41% apartments, vs 20% for Metro Calgary. And that various reports showed that the
rate of unoccupied units is much higher in condos, so we can&rsquo;t substitute comparisons for unoccupied apartments for comparisons
of unoccupied dwellings overall.</p>

<p>So what do the numbers look like when comparing the City of Vancouver to other cities in the region and across Canada.
In short, I do not know. Without going into too much details, the number of unoccupied dwelling units is derived by taking the number of
units that are &ldquo;not occupied by usual residents&rdquo;, which is part of the standard release of the census data, and subtracting
the &ldquo;unusual residents&rdquo;, that is foreign or temporary residents. So residents that have a permanent address abroad or
elsewhere in Canada and on Census day occupied in a dwelling unit that is not their primary residents. Think students
in a dorm room. Or someone in his vacation home. Or someone that works far from home and has a second unit close to work.</p>

<p>And to get the number of foreign or temporary residents requires a custom tabulation, which costs some time and money. But
overall not that much compared to the scope of the study the city just undertook, so it is not clear why they did not do this
and settle this question properly.</p>

<h3>Why do I see so many different unoccupancy rates?</h3>

<p>Again, that depends exactly on the question asked. The report offers a variety of different rates, using the definition
of unoccupied for 2 months, 6 months or 12 months. Each one gives a different rate. Then there is also the definitions
used by Stats Canada, the building being unoccupied on census day (and the questionnaire was not returned).
Again resulting in a differnt number. Then we can throw in the &ldquo;unusual residents&rdquo;, getting yet another number. Confused?
You should be! Difficult questions often don&rsquo;t have an easy answer.</p>

<p>The most important thing is to be consistent when comparing these numbers. Has the problem gotten worse? Use numbers that
were derived using the same methodology at different times. Is is worse here than elsewhere? Use numbers derived using
the same methodology across geographies.</p>

<h3>Geographic distribution</h3>

<p><a href="https://censusmapper.ca/maps/104" target="_blank"><img  src="http://doodles.mountainmath.ca/images/unoccupied.png" style="width:50%;float:left;margin-right:10px;"></a>
In absence of the data on the foreign and temporary residents, we can take a closer look at the publicly released census
data on &ldquo;dwellings not occupied by usual residents&rdquo;. The percentage of dwellings occupied by foreign or temporary residents
are quite low, in the 0.4% to 0.2% range for these metro areas we looked at above. They tend to cluster around universities,
but may also accumulate in some other areas. Keeping that in mind we can <a href="http://censusmapper.ca/maps/104">head over to CensusMapper</a>
and zoom, pan around and use the search bar to explore the geographic distribution. The city data has shown that there
was not much change in the number of unoccupied dwellings over time, no matter how the data was sliced. That gives us
some confidence that the geographic distribution has also been somewhat stable over that time frame.</p>

<p>At CensusMapper we are working on adding previous censuses, so this will be a good test case to see how the 2006 and 2001
geographic distribution of unoccupied dwellings stacks up. For City of Vancouver the corresponding numbers are 7.5% and
5.1%, respectively. The 2006 number is nicely in line with Vancouver&rsquo;s 2011 rate of 7.7%. <strike>It is not immediately clear how the significantly
lower 2001 rate squares with the city report showing a flat trendline, but could well be due to shifting definitions from
one census to another.</strike> The 2001 Census used slightly different definitions, it requires more
work to compare these numbers. The devil is in the details.</p>

<h3>Next Steps</h3>

<p>The city study settles an important questions. The rates of unoccupied homes have not gone up. Perceptions often don&rsquo;t
match facts. And there are lots of little things to poke at of course. What about people having a cleaning person stop
by every other week? How about homes awaiting renovation or demolition permits? How about AirBnB?</p>

<p>Now all these are valid questions. But before using this to discount the study do some estimates to see how large of an effect
your porposed hole actually has. How does it compare to the number of units that were found unoccupied and how would it
change that rate? In all these cases it is quite small. Then ask yourself how likely it is that the problem would be growing.</p>

<p>Then you find that AirBnB probably needs more scrutiny. Best current estimates that I have seen
<a href="https://twitter.com/karensawa/status/707288630444699650">peg entire units rented fulltime at AirBnB at 1200 in Vancouver</a>.
Counting those would bring the rates of units taken out of the rental market by being unoccupied for a year or rented out
full time via AirBnB from 4.8% to 5.3%. Noticeable, but doesn&rsquo;t make that much of a difference. Yet. That number is
very likely to grow if left unchecked and should probably be taken seriously and regulated properly right now.</p>

<h3>Update</h3>

<p>A <a href="http://vancouver.ca/files/cov/stability-in-vancouver-housing-unit-occupancy-empty-homes-report.pdf">more detailed report</a>
is now available to give more insight into the methodology. Nothing unexpected there, although the claim that the 12.5%
rate of unoccupied condos is consistent with earlier claims of 5.5% to 8.5% is hard to follow.</p>

<p>Still waiting for the technical report to get an idea of the error estimates (by housing type). Seeing the graphs for
the 2-month and 4-month cutoffs by housing type would be nice too, as well as the respective ones using the 15 day
criterion.</p>

<div><script src="http://doodles.mountainmath.ca/javascripts/unoccupied-graph.js"></script></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Physical SFH Form Over Time]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/03/05/physical-sfh-form-over-time/"/>
    <updated>2016-03-05T22:58:06-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/03/05/physical-sfh-form-over-time</id>
    <content type="html"><![CDATA[<p>I was curious how the physical parameters of Single Family Houses changed over time.</p>

<p>Using the <a href="https://mountainmath.ca/map/assessment">assessment dataset</a> merged with the <a href="https://mountainmath.ca/land_use/map">land use dataset</a>
allows to fairly accurately pick out single family houses, and also holds the age of most properies. Together with
the City of Vancouver LIDAR-generated building dataset <a href="https://mountainmath.ca/vancouver_lidar/map">that I have played with before</a>
we can look at physical building parameters.</p>

<!-- more -->


<p>The city dataset is a little coarse, it only contains the main building and does not map things like garages. It&rsquo;s a little
rough and should probably be interpreted cautiously on an individual building level. One could spend the time and derive the
building data directly from the raw LIDAR dataset optimized for this purpose, but for some overview statistics the building
dataset that was derived by the city is just fine. The LIDAR was taken in 2009, so we only consider houses built before that time.</p>

<h3>Site coverage</h3>

<p>One question is how the site coverage of the buildings have changed over time. We simply line up the buildings by the year
they were built, compute the site coverage of the main building relative to the parcel size and graph the quintiles for
each year. For good measure, we throw in the 10 and 90 percentile whiskers.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;" class="whiskers">
<div id="sfh_coverage" style="height:200px;max-width:640px;" data-url="/data/sfh_coverage.json"></div>
<div class="legend no-margin" style="padding-bottom:2em;">
</div>
</div>


<p>What jumps out immediately is that around 1988 there apparently was a change in zoning law that reduced site coverage
of the main building. <strike>I don&rsquo;t know anything about the history of building codes, maybe someone that does could shed some
light onto this.</strike> <a href="https://twitter.com/BrendanDawe/status/706737206484795392">Twitter was fast to provide the answer</a>
it appears that changes in building code were implemented to counteract the growing footprint of houses at the time, details
can be found in <a href="https://open.library.ubc.ca/cIRcle/collections/ubctheses/831/items/1.0086386">Barbara Pettit&rsquo;s Ph.D. thesis</a>.</p>

<h3>Building Height</h3>

<p>The next obvious point of analysis is how building height, in meters, changed over time.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;" class="whiskers">
<div id="sfh_height" style="height:200px;max-width:640px;" data-url="/data/sfh_height.json"></div>
<div class="legend no-margin" style="padding-bottom:2em;">
</div>
</div>


<p>We observe a similar jump in the data around 1988, with building height jumping up as site coverage decreases.</p>

<h3>Building Volumes</h3>

<p>Now let&rsquo;s look at how &ldquo;massive&rdquo; the buildings are, measured by volume in cubic metres.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;" class="whiskers">
<div id="sfh_volume" style="height:200px;max-width:640px;" data-url="/data/sfh_volume.json"></div>
<div class="legend no-margin" style="padding-bottom:2em;">
</div>
</div>


<p>The graph does show that in general houses have gotten more massive starting in the late 80s, but after that have more or less
maintained the same volume. It shows that the combined effect of the regulation change in the late 80s that resulted in
smaller building footprints and taller buildings was that buildings overall got bulkier.</p>

<h3>Roof Type</h3>

<p>Lastly we see how the roof type changes over time, plotting the number of buildings with given roof type for each year.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;" class="whiskers">
<div id="sfh_roof" style="height:200px;max-width:640px;" data-url="/data/sfh_roof.json"></div>
<div class="legend no-margin" style="padding-bottom:2em;">
</div>
</div>


<p>Flat roofs seem to have been quite popular between the mid 60s to mid 80s.</p>

<h3>Length to Width Ratio (Update)</h3>

<p>A question about a regulation change in 1938 <a href="https://twitter.com/GRIDSVancouver/status/706879555550613504">came up on Twitter</a>,
so I thought I should check into the length-to-width ratio to see if anything can be seen there. A ratio of 1
would mean a square footprint, a ratio of 2 means the house is twice as long as wide.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;" class="whiskers">
<div id="sfh_square" style="height:200px;max-width:640px;" data-url="/data/sfh_square.json"></div>
<div class="legend no-margin" style="padding-bottom:2em;">
</div>
</div>


<p>No obvious changes around 1938, but our houses became a lot more square after the late 80s zoning changes.</p>

<h3>Next Steps</h3>

<p>After starting to see what kind of data can be derived from LIDAR data we can start to explore different questions. The
graphs open a small window into the physical parameters of single family homes over time and are only of limited general
interest.</p>

<p>For
more serious analysis we would most likely have to start from the raw LIDAR data, which takes a little bit of effort.</p>

<div><script src="http://doodles.mountainmath.ca/javascripts/box.js"></script></div>




<script>

function stacked_bar_graph(div,shiftAxis,domainFormatter,rangeFormatter,domainLabelFormatter){
    if (!domainFormatter) domainFormatter=d3.format("d")
    if (!rangeFormatter)
     rangeFormatter = function (y) {
        return y;
     };
     if (!domainLabelFormatter) domainLabelFormatter=domainFormatter;

var margin = {top: 20, right: 20, bottom: 40, left: 70},
    width = parseInt(div.style("width")) - margin.left - margin.right,
    height = parseInt(div.style("height")) - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);


var xAxis = d3.svg.axis()
    .scale(x)
    .tickFormat(domainFormatter)
    .orient("bottom");


var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .tickFormat(rangeFormatter)
    .ticks(5, rangeFormatter);

var svg = div.append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var data_url=div[0][0].dataset.url;
var legend=d3.select(div.node().parentNode).select('.legend');


d3.json(data_url, function(error, json) {
  if (error) throw error;
  var graphData=json[0];
  var data=graphData.data;
  var color = d3.scale.ordinal().domain(graphData.colors.map(function(d,i){return i}))
  .range(graphData.colors);
  var domain=data.map(function(d){return d.date;});
  x.domain(domain);

  function graphValueId(i){
      return graphData.class + '_' + i + '_value'
  }

  graphData.labels.forEach(function(text,i){
    var color=graphData.colors[i];
    var html='<i style="background:' + color + '"></i> ' + text + ' <span style="float:right;margin-right:10px;" id="' + graphValueId(i) + '"></span>'
    legend.append('p').html(html);
  });
  
  data.forEach(function(d) {
      var y0 = 0;
      d.values = color.domain().map(function(i) { return {date: d.date, y0: y0, y1: y0 += +d.count[i]}; });
      d.total = d.values[d.values.length - 1].y1;
  });
  y.domain([0, d3.max(data, function(d) { return d.total; })]);

  var domainTickValues=[];
  var skip=Math.round(40/x.rangeBand());
  if (skip<=0) skip=1;
  for (var i=0;i<x.domain().length;i++) {
    if (i % skip==0) domainTickValues.push(x.domain()[i]);
  }
  if (x.domain().length % 5 !=0) domainTickValues.push(x.domain()[x.domain().length-1]);
  xAxis.tickValues(domainTickValues);

  var xShift=shiftAxis ?  x.rangeBand()/2.0 * 1.1 : 0;
  
  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(" + xShift + "," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis);
//    .append("text")
//      .attr("transform", "rotate(-90)")
//      .attr("y", 6)
//      .attr("dy", ".71em")
//      .style("text-anchor", "end")
//      .text("Probability");

    function updateTooltip(d,i){
       color.domain().forEach(function(j){
             var value=d && i==j ? (domainLabelFormatter(d.date) + ': ' +rangeFormatter(d.y1-d.y0)) : '';
             d3.select('#'+graphValueId(j)).text( value);
       });
    }

  var year=svg.selectAll(".year")
    .data(data)
        .enter().append("g")
          .attr("class", "g");
  year.selectAll(".color-bar")
      .data(function(d) { return d.values; })
    .enter().append("rect")
      .attr("class", graphData.class + " color-bar")
      .attr("fill", graphData.color)
      .attr("x", function(d) { return x(d.date); })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.y1); })
      .attr("height", function(d) { return Math.max(0, y(d.y0) - y(d.y1)); })
      .attr("fill",function(d,i) {return color(i);})
      .on('mouseover',updateTooltip)
      .on('click',updateTooltip)
      .on('touch',updateTooltip) 
      .on('mouseout',function(){updateTooltip(null,i)});

      
});

}

var yearFormatter=d3.format();
stacked_bar_graph(d3.select("#sfh_roof"),true,yearFormatter,null,yearFormatter);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Property Taxes and Land Use]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/03/02/property-taxes-and-land-use/"/>
    <updated>2016-03-02T20:42:02-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/03/02/property-taxes-and-land-use</id>
    <content type="html"><![CDATA[<p>Since I started thinking about <a href="http://doodles.mountainmath.ca/blog/2015/05/31/density-in-vancouver/">tax density</a>,
the amount of property taxes collected per area, I always felt that the data presentation in the map fell short.</p>

<p>Property taxes are somewhat insulated from the ups and downs of the real estate market as they are need-based and the mill
rate changes to flatten out the crazyness of the market. But what they lack in interesting patterns over time they more than
makes up for in interesting patterns in space.</p>

<p>To recognize these spacial patterns, one needs to switch layers in my
<a href="https://mountainmath.ca/map/assessment?zoom=14&amp;lat=49.2741&amp;lng=-123.1321&amp;layer=10">assessment maps</a> and that&rsquo;s
a little awkward and most people won&rsquo;t do this.</p>

<p>I can&rsquo;t remember where I first saw some 3D data visualization around something like this, maybe in
<a href="http://www.strongtowns.org/journal/2015/11/18/mapping-the-effects-of-parking-minimums">this excellent Strong Towns post</a> that
explores tax density and parking.
When I was chatting with <a href="https://twitter.com/dnproulx">Darren Proulx</a> about this I realized this is the perfect case
to rev up <a href="https://mapzen.com">Mapzen&rsquo;s awesone 3D map engine</a> for data visualization. Previously I had tried out the 3D
mapping capabilities to map
<a href="https://mountainmath.ca/vancouver_lidar/map">the physical form of Vancouver&rsquo;s buildings obtained from LIDAR data</a>,
now it was time to map abstract data in 3D.</p>

<!-- more -->


<h3>3D Tax Density Maps</h3>

<p><a href="https://mountainmath.ca/assessment_gl/map?zoom=14&lat=49.2814&lng=-123.1312" target="_blank"><img  src="http://doodles.mountainmath.ca/images/tax_density.png" style="width:50%;float:left;margin-right:10px;"></a>
The spacial patterns of tax density are closely related to zoning, so the obvious thing to do is to map both at the same
time. Tax density as the height and the zoning as the colour. But the relation between tax density and zoning is indirect,
it is given through land use. So when I
<a href="http://doodles.mountainmath.ca/blog/2016/01/31/land-use/">folded the Metro Vancouver land use dataset into the asessment data</a>
I added another option to colour the map by land use. And the visualization works pretty well to show the relationship
between the variables.
Check out the <a class='btn' href="https://mountainmath.ca/assessment_gl/map?zoom=14&lat=49.2814&lng=-123.1312" target="_blank">interactive map</a>.</p>

<h3>Some Quick Observations</h3>

<p>When comparing this to the <a href="http://www.strongtowns.org/journal/2015/11/18/mapping-the-effects-of-parking-minimums">Strong Towns post</a>
mentioned above it is interesting to note that the &ldquo;tax contribution of bare pavement&rdquo; is actually quite high relative to
many other commercial properties. For example, the giant parking lot around Safeway on 10th near UBC can&rsquo;t be discerned
from the map. Similarly, Oakridge Mall does not stand out as a low tax area. That&rsquo;s basically because in today&rsquo;s property
market the value of the property is dictated by the land value and few properties, even few commercial properties outside
of downtown, have building values in the same order of magnitude as the land value.</p>

<p>And that keeps the pressure on to redevelop the low building value commercial stock, typically replacing it with mixed use.
One example is the Safeway on Granville and 70th, that went from parking lot to mixed use. The high land values make this
kind of redevelopment attractive. Typically this redevelopment process will raise commercial rents, lower commercial
property taxes, add housing supply and increase tax density.</p>

<p><a href="https://mountainmath.ca/assessment_gl/map?zoom=16&lat=49.2433&lng=-123.151" target="_blank"><img  src="http://doodles.mountainmath.ca/images/triangle.png" style="width:50%;float:right;margin-left:10px;"></a>
Another interesting area is the
<a href="https://mountainmath.ca/assessment_gl/map?zoom=16&lat=49.2433&lng=-123.151" target="_blank">condo/apartment triangle at Arbutus and 33rd</a>.
These condos/apartments abut Quilchena Park, have a grocery store in proximity but otherwise sit in a see of single family housing.
I would like to highlight two alternative development models. One is the single family housing that surrounds this area,
the other is one where one takes the condo/apartment complex and throws in the adjacent Quilchena Park. The park does not
house any people and does not generate tax revenue but provides great recreational value. Overall, the tax density, that is the
amount of taxes collected per area, of those two models are the same. So the condo complex together with the entire park
collects the same amount of taxes per area as single family housing. On the other hand, the condo complex plus park houses
3 to 4 times as many people per (combined) area as the surrounding single family housing.</p>

<p>Presenting these as alternative
development models one can ask how many people would prefer condo (and one dedicated rental building) plus park over
single family housing without park.
Condo values range between $448k to $2.6m with median of $745k. 56% of all units have 2 bedrooms, 9% with 3 bedrooms.
The surrounding single family housing start at
around $2.5m for a teardown. My guess is that there are more people interested in the condo plus park model than the
current Vancouver market offers.</p>

<h3>More Detailed Analysis</h3>

<p>Now let&rsquo;s take the tax density information and measure it against city services rendered. In other words, let&rsquo;s see who
pays their &ldquo;fair share&rdquo; of property taxes. Well, that&rsquo;s where I ran out of juice. There is lots of
work involved to do it right, too much to fit into my evening side project time budget.</p>

<p>Some have done some
<a href="http://mapstoryblog.thenittygritty.org/costofstreets/">really amazing work</a> trying to understand the give and take of
property taxes and services rendered, to the bar is pretty high.</p>

<p>Vancouver&rsquo;s situation, with the commercial property tax 5x the residential rate, has it&rsquo;s own quirks to add to the
complexity.</p>

<p>If someone is interested in spending the time to sieve through the city budget and process the data I would be happy to help out
with graphing and mapping. As long as the data is clean enough that it stays under my &ldquo;one evening side project time budget&rdquo;. :-)</p>

<h3>Looking Forward</h3>

<p>General public awareness about taxes, zoning and fairness seem to be rising. Recently a
<a href="http://www.capitalnewyork.com/article/city-hall/2014/02/8540903/lawsuit-claims-discrimination-real-estate-taxes">class action alleging discrimination in New York&rsquo;s property tax code</a>
was launched. An interesting precedent, and it will be interesting to watch the outcome.
And to see how this could apply to the tax and services balance
of low density vs high density. Or exclusionary zoning policies as the motor of unequal tax density.
Or minimum parking requirements which <a href="https://www.washingtonpost.com/news/in-theory/wp/2016/03/03/how-parking-requirements-hurt-the-poor/">disproportionally hurt poor people</a>.</p>

<p>As this kind of data becomes more publicly accessible and better visualizations make this complex issue more
accessible I expect these kind of questions to gain more traction.</p>

<hr>


<h5>I can&rsquo;t view the 3D map!</h5>

<p>Sorry. Welcome to the world of modern web technology. (WebGL in this case.) If you try to look at the interactive map and don&rsquo;t see anything like the image above, then that&rsquo;s probably because
you either have and old (windows?) computer and/or an old (Internet Explorer) browser. If switching to Chrome or Firefox does
not fix this for you your best option is to cramp that map into your phone screen. Your phone is probably newer and supports
modern web technology much better than your computer. Or you can always view the 2D maps linked above.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Land Use, Roads (and Parking)]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/02/29/land-use/"/>
    <updated>2016-02-29T21:10:05-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/02/29/land-use</id>
    <content type="html"><![CDATA[<p>The other day at the <a href="https://www.sfu.ca/publicsquare/upcoming-events/city-conversations/2016/Feb-18-2016.html">SFU&rsquo;s City Conversations</a>
someone asked a question about space dedicated to roads, and how that could be unlocked
to aid housing. He mentioned what percentage of space is currently dedicated to roads. I forgot the number,
but I thought to myself that I should look that up for all Metro Vancouver communities. So here we go.</p>

<!-- more -->


<p>Actually, it would be interesting to compare how land is allocated to all kinds of land uses across Metro Vancouver, not just roads.
And with their excellent
<a href="http://doodles.mountainmath.ca/blog/2016/01/31/land-use/">land use dataset</a> it&rsquo;s easy enough to do, justifying spending
two hours on this. <a href="https://mountainmath.ca/land_use/map">Looking at the land use map</a>
one can see that there are some issues with using the dataset for
that purpose, for example the roads within
Stanly Park are missing, as parking lots in Vancouver parks. So this slightly overestimates the green space. But when
added up this won&rsquo;t do much to change the overall area of each land use.</p>

<h3>Land Use Breakdown</h3>

<p>Let&rsquo;s start with a simple chart summing up the land use in each of Metro Vancouver&rsquo;s Municipalities. Just select the one
you are interested in from the dropdown.</p>

<div id="land_use_breakdown" class="land_use"></div>


<p>City of Vancouver stands out as the municipality with the largest proportion of area dedicated to roads right of way.
The right of way includes nature strips, sidewalks, and of course on-street parking.</p>

<h3>Built Area Land Use</h3>

<p>For some of the municipalities large &ldquo;Natural Areas&rdquo; and &ldquo;Undeveloped Area&rdquo; make it difficult to discern the makeup of
the built up areas. So here comes the same graph with some of the large space-cosuming categories taken out.</p>

<div id="land_use_breakdown2" class="land_use"></div>


<p>What&rsquo;s remarkable is that City of Vancouver still is the clear winner in terms of space taken up by roads right of way.</p>

<h3>Roads right of way</h3>

<p>It might be interesting to compute more precisely how the roads right of way is used. But that requires a lot more work.
The City of Vancouver has a dataset with road widths, which would help separate area taken by roads from area taken by
sidewalks and nature strips. One could use google maps to estimate to estimate the amount of space taking up by on-street
parking by sampling a couple of roads and scaling this according to road area. Too much work for me right now, maybe one day I will
have a good enough reason to dedicate some time to this.</p>

<h3>Parking</h3>

<p>Parking in particular is a land use that is artificially inflated. We pay lots of money to buy private property to live
on, and then continue to pay property taxes for that privilege. But we pay nothing to store our vehicles on public roads.</p>

<p>At current land values in Vancouver it just does not make any sense to socialize the cost of parking. A 12m&sup2; on-street
parking space at a low-balled $3000/m&sup2; value is worth $36,000. If we were to price parking at value, rather than
socializing the cost we could have a discussion about using that space more effectively. Killing the parking subsidy
would have a substantial impact on demand (and car ownership), freeing up space to be re-allocated for other uses. For
example for housing by increasing lot sizes and making multi-unit structures more feasible, especally on corner lots.</p>

<hr>


<h3>Update</h3>

<p>Saw a <a href="https://twitter.com/d3visualization/status/705421356222029824">great little data visualization fly by today</a> that
is just the missing link in the above visualizations. A simple way to visualize where all the municipalities stand in
relation to one another. Thought I would throw that in real quick.</p>

<div>
 <div id="radviz" class="radviz"></div>
<div class="radviz-list-container">
        <div class="muni"></div>
        <div class="list"></div>
</div>
<div style="clear:both;"></div>
</div>




<div><script src="http://doodles.mountainmath.ca/javascripts/land_use_breakdown.js"></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Condos]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/02/10/on-condos/"/>
    <updated>2016-02-10T11:01:47-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/02/10/on-condos</id>
    <content type="html"><![CDATA[<p>Lots has been said about the upper end of owned dwellings. The movement of the &ldquo;million dollar line&rdquo;, the emergence of
the &ldquo;two million dollar line&rdquo; and &ldquo;multi-million dollar lines&rdquo;. Most of that discussion is focused on
<a href="http://www.mountainmath.ca/map/assessment?filter=sfh" target="_blank">single detached homes</a> or on proxies for &ldquo;single detached&rdquo; like
<a href="http://www.mountainmath.ca/map/assessment?filter=rs" target="_blank">RS zoned properties</a>.</p>

<p>But all of these maps have a clear bias toward the more expensive homes. Everyone knows by now where the most expensive properties
are. But where are the more affordable ones?</p>

<!-- more -->


<p>Before continuing and getting disappointed at the end, we want to highlight the huge limitations when looking for affordable homes.
There is no freely available data on the
floor area or number of bedrooms for each dwelling unit. Next to the price and location (which we have), these are the most
important features of a dwelling. And this limits the usefulness of all that follows.</p>

<p>Why continue? It gives a glimpse of what kind of analysis could be done if the information would be <em>freely</em> available.</p>

<h3>The Elephant in the Room</h3>

<p>In principle all this information is available, <a href="https://twitter.com/bcassessment">BCAssessment</a> has all this data. And they even make it
<a href="http://evaluebc.bcassessment.ca/Default.aspx#">available on their (quite nice) eValue website</a>. But their terms of use
prevent us and others from using this information. Most of the information used in the analysis here originates from BCAssessment,
but it comes via the City of Vancouver that has made the data available through their open data portal.</p>

<p>As I understand it, the main reason why this data is not freely available is that BCAssessment is charged to recover their
own operating cost. So they hold onto their data and try to sell it for cost recovery. In the process of which they harm
the ability of municipalities to plan properly and the public to get a clear idea where things are at and have a fact-based
discussion on how to move forward. Which creates large amounts of friction and may lead to social and economic damages far
exceeding any revenue collected by BCAssessment.</p>

<p>Personally, I don&rsquo;t believe that holding back data is a smart way to run a government. You might want to
<a href="http://twitter.com/home/?status=@bcassessment Please add %23openData license to information you publish on eValue! %23giveUsData" target="_blank">ask BCAssessment to #giveUsData</a>.</p>

<p>For now, let&rsquo;s put the data that we do have to the best use and see what we can tease out.</p>

<h3>Locating the Most Affordable Dwellings</h3>

<p><a href="https://mountainmath.ca/map/special/101" target="_blank"><img  src="http://doodles.mountainmath.ca/images/strata.png" style="width:50%;float:left;margin-right:10px;"></a>
Stating the obvious, the most affordable homes aren&rsquo;t &ldquo;single detached&rdquo;, they are condos. And condos have been largely
absent from the affordability debate, although they make up the majority of owned dwelling units in the City of Vancouver.</p>

<p>Let&rsquo;s start off with a <a href="http://mountainmath.ca/map/special/101" target="_blank">map of the roughly 4,500 stratified residential or mixed use properties in Vancouver housing a total of about 10,2000 strata units</a>.
The exact numbers are hard to pin down. (<a href="http://twitter.com/home/?status=@bcassessment Please add %23openData license to information you publish on eValue! %23giveUsData" target="_blank">#giveUsData</a>)</p>

<p>The distribution of stratified units by the total number of strata units
per building gives an idea of the types of strata units that are out there.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_strata_by_number" style="height:200px;max-width:640px;" data-url="/data/strata_by_number.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of strata units by size of strata <span style="float:right;margin-right:10px;" id="unit_counts_value"></span></p>
</div>
</div>


<p>The horizontal axis is not linear, and the bin sizes are not equal, which makes the graph a little difficult to read. We chose bins at
2, 4, 8, 10, 16, 20 at the low end, then of width 10 up to 200, from there width 20 up to 300 and then 400, 500 and 700 at
the top end. Lazy me apologizes for the poor graphic.</p>

<p>We immediately notice the 4,152 strata units in stratas with exactly 2 units usually referred to as &ldquo;duplexes&rdquo;.
There are a lot of strata units are in stratas of size between 20 and 50, but
otherwise the units are fairly well distributed over different building sizes. At the high end there are fewer buildings,
but each with lots of units. So there our graph becomes a little jerky and heavily depends on the cutoffs we choose.</p>

<p>To get a basic idea on how much these units cost we plot the number of strata units by price bracket.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_strata_by_price" style="height:200px;max-width:640px;" data-url="/data/strata_by_price.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of strata units by price <span style="float:right;margin-right:10px;" id="affordable_value"></span></p>
</div>
</div>


<p>The distribution looks largely as one would expect. It peaks between the $400k and $500k mark, with a median value of
$482,000. But there are a couple of things that jump out. Firstly, there are suspiciously many condos
below $100k, more than half of which (445 to be precise), are less than $50k. There are places where one could by
a condo for that price, but not in Vancouver. These are stratified parking spaces or other amenity spaces.</p>

<p>Next let&rsquo;s focus in on the 8,313 duplex and multi-plex units with 8 or fewer units (in 2,995 buildings) and plot these
separately.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_multiplex_by_price" style="height:200px;max-width:640px;" data-url="/data/multiplex.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of multiplex units by price <span style="float:right;margin-right:10px;" id="multiplex_value"></span></p>
</div>
</div>


<p>We see that the price distribution for multiplex units peaks at a higher price between $800k and $900k, but the overall
numbers are quite small when compared to all strata units.</p>

<h3>Affordable Strata Units</h3>

<p>Let&rsquo;s try to understand where the &ldquo;affordable&rdquo; housing stock is, which we take to be units below $500k, or roughly the
bottom half of the distribution. We would like to map the properties containing housing
units for each of our price brackets, but this gets tricky since the dataset does not hold
information which units are parking spaces and which are commercial.</p>

<p>We need to take care of the problem with needing to distinguish parking spaces from housing units. And from commercial units.
(<a href="http://twitter.com/home/?status=@bcassessment Please add %23openData license to information you publish on eValue! %23giveUsData" target="_blank">#giveUsData</a>) So we are left with
using proxies, so let&rsquo;s set a cutoff price so that most units below are parking stalls and most units above are (residential)
units.</p>

<p>Generally, parking spaces should not cost more than $50,000 which is roughly the cost to build underground
parking. Around $40,000 if it&rsquo;s only one level, getting up to $75,000 (or sometimes even more) when having to go down deeper or water becomes an issue. The
fact that some spaces go for significantly less is the result of mandatory parking minimums. Some parking spaces may be worth
more than $100,000 as some people are willing to pay a premium for a convenient spot. That may mean a spot next to the
elevator by the exit ramp, or simply a parking spot in a specific building that is under supplied and trekking across the
street to a spot somewhere else is inconvenient enough. Or some stratified parking spaces might consist of several
parking spots.</p>

<h3>Location of Affordable Condos</h3>

<p><a href="https://mountainmath.ca/map/special/110" target="_blank"><img  src="http://doodles.mountainmath.ca/images/strata_500k.png" style="width:50%;float:left;margin-right:10px;"></a>
Since it is impossible to pick out the housing units out of all the strata units, all we can do is map all strata units,
understanding that
those below $50k are most likely parking spaces, those between $50k and $100k could be parking or housing (or commercial) and the
majority of units above $100k are housing.</p>

<p>And we can&rsquo;t actually map the individual units, only the buildings that house the units. Here are maps of the buildings
housing the units in each of the lower brackets.</p>

<ul>
<li><a href="https://mountainmath.ca/map/special/41" target="_blank">&lt; $100k (715 units in 17 buildings)</a></li>
<li><a href="https://mountainmath.ca/map/special/42" target="_blank">$100k &ndash; $200k (2,415 units in 198 buildings)</a></li>
<li><a href="https://mountainmath.ca/map/special/43" target="_blank">$200k &ndash; $300k (10,270 units in 644 buildings)</a></li>
<li><a href="https://mountainmath.ca/map/special/44" target="_blank">$300k &ndash; $400k (19,128 units in 1,112 buildings)</a></li>
<li><a href="https://mountainmath.ca/map/special/45" target="_blank">$400k &ndash; $500k (20,538 units in 1,219 buildings)</a></li>
</ul>


<p>Clickin into a particular building and hitting the &ldquo;more&rdquo; button will pull up (a slightly cleaned) tax roster where you
can get more information on the units. And if you are really interested in finding out more about a particular one
you can always <a href="http://evaluebc.bcassessment.ca/Default.aspx#">head on over to BCAssessment&rsquo;s eValue website</a> to look up
more of those details that BCAssessment keeps in public view but locked off from systematic public scrutiny.
(<a href="http://twitter.com/home/?status=@bcassessment Please add %23openData license to information you publish on eValue! %23giveUsData" target="_blank">#giveUsData</a>)</p>

<h3>Makeup of Condos</h3>

<p>The built quality of strata units is generally higher, dollar for dollar, than that of single family homes. This should not
come as a surprise as land is very expensive and stata units tend to use land more efficiently. There are only 27 residential
or mixed use strata buildings that classify as <em>teardowns</em>,
<a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">with a teardown coefficient below 5%</a>.
And 25 of these are duplexes, one is a 3-plex and one an 8-plex. There are no condos buildings with more than 8 units that
fit our most restrictive definition of <em>teardown</em>.</p>

<p>Next we explore what kind of buildings the affordable units are in by graphing the number of units in several price ranges
per size of the strata building it is in.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_affordable_strata_by_number" style="height:200px;max-width:640px;" data-url="/data/affordable_strata_by_number.json"></div>
<div class="legend no-margin">
</div>
</div>


<p>It looks like buildings with 20 to 60 units are quite good at producing affordable units. Partially that&rsquo;s due to their
abundance, but graphing the percentage of units in each price bracket confirms that these buildings tend to produce a
nice mixture of low and high value condos. But larger condo buildings can also achieve this, although the actual performance
of what has been built is mixed. Other factors, for example building age, are likely also at play here.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_percentage_affordable_strata_by_number" style="height:200px;max-width:640px;" data-url="/data/percentage_affordable_strata_by_number.json"></div>
<div class="legend no-margin">
</div>
</div>


<p>To see what the distribution of strata units by age is, we use the same price cutoffs and sort the strata units into age
brackets. Since we are mostly interested in the more affordable units we sort out units in buildings with 4 or fewer units.
These units might skew some of the age brackets.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_affordable_strata_by_age" style="height:200px;max-width:640px;" data-url="/data/affordable_strata_by_age.json"></div>
<div class="legend no-margin">
</div>
</div>


<p>To highlight the proportional makeup we again and graph this again as percentages.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_percentage_affordable_strata_by_age" style="height:200px;max-width:640px;" data-url="/data/percentage_affordable_strata_by_age.json"></div>
<div class="legend no-margin">
</div>
</div>


<p>We see how generally the percentage of more affordable units increases with the age of the building. Newer units generally
tend to be more expensive at first and becoming more affordable over time as the building ages.</p>

<h3>Taking Stock</h3>

<p>If you read this far you probably agree that this exercise was mostly a waste of time. Hopefully you are earger to see this
analysis split up by number of bedrooms and floor area. If you are all riled up at the lack of data and BCAssessment
not giving out this information with a clean open data license you might want to drop them a line and
<a href="http://twitter.com/home/?status=@bcassessment Please add %23openData license to information you publish on eValue! %23giveUsData" target="_blank">ask for at least the information they publish eValue to be made available with an #openData license!</a></p>

<p>I am still hopeful that this might happen some day, and we can get a much better picture of the buildings in BC cities
and check how they perform in fulfilling the needs of the community. And learn from that to make informed policy choices.</p>

<script>

function stacked_bar_graph(div,shiftAxis,domainFormatter,rangeFormatter,domainLabelFormatter){
    if (!domainFormatter) domainFormatter=d3.format("d")
    if (!rangeFormatter)
     rangeFormatter = function (y) {
        return y;
     };
     if (!domainLabelFormatter) domainLabelFormatter=domainFormatter;

var margin = {top: 20, right: 20, bottom: 40, left: 70},
    width = parseInt(div.style("width")) - margin.left - margin.right,
    height = parseInt(div.style("height")) - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);


var xAxis = d3.svg.axis()
    .scale(x)
    .tickFormat(domainFormatter)
    .orient("bottom");


var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .tickFormat(rangeFormatter)
    .ticks(5, rangeFormatter);

var svg = div.append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var data_url=div[0][0].dataset.url;
var legend=d3.select(div.node().parentNode).select('.legend');


d3.json(data_url, function(error, json) {
  if (error) throw error;
  var graphData=json[0];
  var data=graphData.data;
  var color = d3.scale.ordinal().domain(graphData.colors.map(function(d,i){return i}))
  .range(graphData.colors);
  var domain=data.map(function(d){return d.date;});
  x.domain(domain);

  function graphValueId(i){
      return graphData.class + '_' + i + '_value'
  }

  graphData.labels.forEach(function(text,i){
    var color=graphData.colors[i];
    var html='<i style="background:' + color + '"></i> ' + text + ' <span style="float:right;margin-right:10px;" id="' + graphValueId(i) + '"></span>'
    legend.append('p').html(html);
  });
  
  data.forEach(function(d) {
      var y0 = 0;
      d.values = color.domain().map(function(i) { return {date: d.date, y0: y0, y1: y0 += +d.count[i]}; });
      d.total = d.values[d.values.length - 1].y1;
  });
  y.domain([0, d3.max(data, function(d) { return d.total; })]);

  var domainTickValues=[];
  var skip=Math.round(40/x.rangeBand());
  if (skip<=0) skip=1;
  for (var i=0;i<x.domain().length;i++) {
    if (i % skip==0) domainTickValues.push(x.domain()[i]);
  }
  if (x.domain().length % 5 !=0) domainTickValues.push(x.domain()[x.domain().length-1]);
  xAxis.tickValues(domainTickValues);

  var xShift=shiftAxis ?  x.rangeBand()/2.0 * 1.1 : 0;
  
  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(" + xShift + "," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis);
//    .append("text")
//      .attr("transform", "rotate(-90)")
//      .attr("y", 6)
//      .attr("dy", ".71em")
//      .style("text-anchor", "end")
//      .text("Probability");

    function updateTooltip(d,i){
       color.domain().forEach(function(j){
             var value=d && i==j ? (domainLabelFormatter(d.date) + ': ' +rangeFormatter(d.y1-d.y0)) : '';
             d3.select('#'+graphValueId(j)).text( value);
       });
    }

  var year=svg.selectAll(".year")
    .data(data)
        .enter().append("g")
          .attr("class", "g");
  year.selectAll(".color-bar")
      .data(function(d) { return d.values; })
    .enter().append("rect")
      .attr("class", graphData.class + " color-bar")
      .attr("fill", graphData.color)
      .attr("x", function(d) { return x(d.date); })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.y1); })
      .attr("height", function(d) { return Math.max(0, y(d.y0) - y(d.y1)); })
      .attr("fill",function(d,i) {return color(i);})
      .on('mouseover',updateTooltip)
      .on('click',updateTooltip)
      .on('touch',updateTooltip) 
      .on('mouseout',function(){updateTooltip(null,i)});

      
});

}

function bar_graph(div,shiftAxis,domainFormatter,rangeFormatter,domainLabelFormatter){
    if (!domainFormatter) domainFormatter=d3.format("d")
    if (!rangeFormatter)
     rangeFormatter = function (y) {
        return y;
     };
     if (!domainLabelFormatter) domainLabelFormatter=domainFormatter;

var margin = {top: 20, right: 20, bottom: 40, left: 70},
    width = parseInt(div.style("width")) - margin.left - margin.right,
    height = parseInt(div.style("height")) - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);


var xAxis = d3.svg.axis()
    .scale(x)
    .tickFormat(domainFormatter)
    .orient("bottom");


var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .tickFormat(rangeFormatter)
    .ticks(5, rangeFormatter);

var svg = div.append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var data_url=div[0][0].dataset.url;

d3.json(data_url, function(error, json) {
  if (error) throw error;
  var graphData=json[0];
  var data=graphData.data;
  x.domain(data.map(function(d) { return d.date }));
  y.domain([0, d3.max(data, function(d) { return d.count; })]);
  
  var domainTickValues=[];
  var skip=Math.round(40/x.rangeBand());
  if (skip<=0) skip=1;
  for (var i=0;i<x.domain().length;i++) {
    if (i % skip==0) domainTickValues.push(x.domain()[i]);
  }
  if (x.domain().length % 5 !=0) domainTickValues.push(x.domain()[x.domain().length-1]);
  xAxis.tickValues(domainTickValues);

  var xShift=shiftAxis ?  x.rangeBand()/2.0 * 1.1 : 0;
  
  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(" + xShift + "," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis);
//    .append("text")
//      .attr("transform", "rotate(-90)")
//      .attr("y", 6)
//      .attr("dy", ".71em")
//      .style("text-anchor", "end")
//      .text("Probability");

  svg.selectAll(".bar")
      .data(data)
    .enter().append("rect")
      .attr("class", graphData.class + " bar")
      .attr("fill", graphData.color)
      .attr("x", function(d) { return x(d.date); })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.count); })
      .attr("height", function(d) { return height - y(d.count); })
      .on('mouseover',function(d){
         d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('click',function(d){
       d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('touch',function(d){
         d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});

      
});

}


var priceFormatter2=d3.format("$,");
    var priceFormatter = function (y) {
        return y>=1000000 ? (priceFormatter2(y/1000000) + 'm') : (priceFormatter2(y/1000) + 'k');
    };
    var brackets=[100000,200000,300000,400000,500000,600000,700000,800000,900000,1000000,2000000,10000000,20000000,40000000]

var binFormatter=function(top){
    var bottom=0;
    if (top<=1000000) bottom=top-100000;
    else if (top==2000000) bottom= 1000000;
    else if (top==10000000) bottom= 2000000;
    else if (top=20000000) bottom= 10000000;
    else bottom=20000000;
    return priceFormatter(bottom) + ' - ' + priceFormatter(top);
}
var percentageFormatter=d3.format("%");
var numberFormatter=d3.format(",");
var numberBinFormatter=function(top){
    var     bins=[0,1,2,4,8,10,16,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,300,400,500,700];
    var index=0;
    while (bins[index]<top && index<bins.length) index ++;
    bottom=bins[index-1]+1;
    return (bottom == top) ? numberFormatter(bottom) : numberFormatter(bottom) + ' - ' + numberFormatter(top);
}
var yearFormatter=d3.format();//function(d){return d;};
var yearBinFormatter=function(top){
     var     bins=[1970,1980,1990,1995,2000,2005,2010,2020];
     var index=0;
     while (bins[index]<top && index<bins.length) index ++;
     bottom=bins[index-1]+1;
     return (bottom == top) ? yearFormatter(bottom) : yearFormatter(bottom) + ' - ' + yearFormatter(top);
}
bar_graph(d3.select("#graph_strata_by_price"),true,priceFormatter,null,binFormatter);
bar_graph(d3.select("#graph_multiplex_by_price"),true,priceFormatter,null,binFormatter);
bar_graph(d3.select("#graph_strata_by_number"),true,numberFormatter,null,numberBinFormatter);
stacked_bar_graph(d3.select("#graph_affordable_strata_by_number"),true,numberFormatter,null,numberBinFormatter);
stacked_bar_graph(d3.select("#graph_percentage_affordable_strata_by_number"),true,numberFormatter,percentageFormatter,numberBinFormatter);
stacked_bar_graph(d3.select("#graph_affordable_strata_by_age"),true,yearFormatter,null,yearBinFormatter);
stacked_bar_graph(d3.select("#graph_percentage_affordable_strata_by_age"),true,yearFormatter,percentageFormatter,yearBinFormatter);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Loss of Character]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/02/01/loss-of-character/"/>
    <updated>2016-02-01T11:16:28-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/02/01/loss-of-character</id>
    <content type="html"><![CDATA[<p>At the heart of the Vancouver affordabiliy crisis is a phrase that comes up again and again. <em>Loss of Character</em>. To effectively
move forward I think it is important to look closely what it means and how it is used.</p>

<p>And what the data can tell us about <em>loss of character</em>.</p>

<!-- more -->


<h3>Loss of Character</h3>

<p>Very broadly interpreted, <em>change of character</em> is how people describe the process when
the environment around them is changing in profound ways. And the neighbourhoods in Vancouver are indeed changing fast.</p>

<p><em>Loss of character</em>, as opposed to <em>change of character</em>, adds a value judgement. It implies that the former &ldquo;character&rdquo;
was preferable to the new one.</p>

<p>I want to highlight two ways in which this change manifests itself in Vancouver: Change in buildings and change in demographics.</p>

<h3>Change in buildings</h3>

<p>There has been a lot of focus on buildings getting torn down and rebuilt. When a city runs out of land to build new housing,
part of the existing stock will get increasingly cannibalized to make space for new buildings. It pays off to try and
understand how this renewal process works.</p>

<p>Let&rsquo;s take a look at the age distribution of Vancouver&rsquo;s single family homes.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_sfh" style="height:200px;max-width:640px;" data-url="/data/sfh_age.json"></div>
<div class="legend no-margin"></div>
</div>


<p>We divided the building stock for each age into two groups, one with building value less than 5% of the total property
value, and one with building value greater than 5%. The significance of that number is that historically over the last
10 years, buildings with this precentage below 5% <a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">had a 1 in 6 change of being torn down and rebuilt</a>
over a 10 year time frame, with sharply dropping frequency of tear downs of buildings above that threshold. Check the link
for a more detailed analysis.</p>

<p>We interpret this graph as giving us a decent prediction of what part of our building stock will get torn down and rebuilt
in the next 10 years, 1 in 6 of the red is predicted to go.</p>

<p>What&rsquo;s remarkable is that essentially the entire SFH stock built between 1935 and 1965 is at high risk of being torn down,
while the period before that has a much better chance of surviving. This comes out of purely looking at the data, it would
be helpful if people out there that understand
the characteristics of the homes built at different times could shed some light onto why this maybe be the case.</p>

<p>Appreciating some of the economic drivers of this process, we can try to understand how people feel about the process.
There seem to be several different reactions to and thoughts on buildings being torn down
that can be summed up with the words &lsquo;heritage&rsquo;, &lsquo;waste&rsquo; and &lsquo;McMansion&rsquo;.</p>

<h4>Heritage</h4>

<p><a href="https://mountainmath.ca/map/special/11?zoom=13&amp;lat=49.264&amp;lng=-123.1276&amp;layer=4&amp;filter=sfh"><img  src="http://doodles.mountainmath.ca/images/heritage.png" style="width:50%;float:left;margin-right:10px;"></a>
Some feel strongly about &lsquo;heritage homes&rsquo; which typically goes beyond the
<a href="https://mountainmath.ca/map/special/11?zoom=13&amp;lat=49.264&amp;lng=-123.1276&amp;layer=4">City of Vancouver list of heritage sites</a>
or more specifically the <a href="https://mountainmath.ca/map/special/11?zoom=13&amp;lat=49.264&amp;lng=-123.1276&amp;layer=4&amp;filter=sfh">single family houses on that list</a>.
The term is used differently by different people and may be confined to offical heritage sites, or ones that have been
&lsquo;overlooked&rsquo; and should be part of that list, to houses of a certain age or style (&ldquo;cottage homes&rdquo;), to including pretty
much anything old or decent looking like this <a href="http://www.vancouversun.com/business/bulldozer+bait+million+mansion+just+another+vancouver+tear+down/11686111/story.html?__lsa=8524-b4e7">1996 building that made the news recently</a>.
Many of those heritage sites are well kept up, with a teardown coefficient well above 5%. But there are also some in quite desolate
state.</p>

<p>Most people see value in heritage, for this term to be a more usefuly driver in the discussion it would greatly help to
narrow it down.</p>

<h4>Waste</h4>

<p>Another narrative is that of &lsquo;waste&rsquo;, when a house gets sent to the landfill. And waste is an issue everyone
is concerned about, land fill space is limited and the impact of the building industry is huge,
<a href="http://www.bcbusiness.ca/real-estate/disposable-housing-vancouver-home-demolition">as described in this 2011 article</a>.
The City of Vancouver has enacted some rules on <a href="http://vancouver.ca/home-property-development/demolition-permit.aspx">how much of a building needs to be recycled</a>,
but they are very weak. In particular, there are no binding recycling requirements at all for post 1940 houses beyond recycling drywall.
Generally the sentiment seems to be more concerned with the fact that a building is getting
torn down, rather than focused on how to improve the process of salvaging and recycling the old home.</p>

<p>The good news is that most of the teardown activity in Vancouver has been
<a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">focusing on low relative building value properties</a>, so
cases like the demolition of <a href="http://www.vancouversun.com/business/bulldozer+bait+million+mansion+just+another+vancouver+tear+down/11686111/story.html?__lsa=8524-b4e7">that 1996 building assessed at $1.9m</a>
are quite rare. That building has a teardown coefficient of 25%, and historically buildings in that range had a 1 in 200
chance of being torn down in a 10 year period.</p>

<p>What&rsquo;s concerning though is that as land values increase, so do the value (dollar amount) of the buildings that move into
the high-risk category of teardown coefficient below 5%.</p>

<h4>McMansion</h4>

<p>The last important word is &lsquo;McMansion&rsquo;, which tackles the question of what the building gets replaced with. Typically
the term &lsquo;McMansion&rsquo; is used for a building whose primary concern is to maximize the allowable footprint.
Economically speaking, this is a rational reaction to the exorbitant land values. Esthetically it can be quite jarring.</p>

<p>This is where the value judgement in &ldquo;<em>loss</em> of character&rdquo; becomes most apparent. While some people feel visually offended
by them, people that buy these properties clearly prefer them over what was there before. Either way, they serve as constant
reminders of the change the neighbourhoods are experiencing.</p>

<h3>Change in Demographics</h3>

<p>Probably more important than the change in buildings is the change in demographics. Vancouver seems to be experiencing
a gentrification process where the traditional financial elite of &lsquo;high income&rsquo; households is priced out of the market by the new
financial elite of &lsquo;high net worth&rsquo; households. That change is dramatic and it has been going on for long enough now to
have left very visible signs in the 2011 Census data.</p>

<p><a href="http://censusmapper.ca/maps/37"><img  src="http://doodles.mountainmath.ca/images/local_affordability.png" style="width:50%;float:right;"></a>
Looking at the <a href="http://censusmapper.ca/maps/37">&lsquo;local affordability&rsquo; in Vancouver</a> in 2011 we see that in many (not all)
neighbourhoods the people that live there could
not afford to buy again (based on their income). What that means is simply that the people moving in will be demographically
very different from the ones living there right now. And yes, this problem is much worse in Vancouver than in other places in
Canada, which can easily be seen by searching and panning in the map.</p>

<p>This problem has likely gotten much worse since, as housing prices continued to climb much more steeply than income since 2011.</p>

<p><a href="http://www.mountainmath.ca/map/assessment?zoom=14&amp;lat=49.253&amp;lng=-123.1244&amp;layer=12"><img  src="http://doodles.mountainmath.ca/images/twiddling_thumbs.png" style="width:50%;float:left;margin-right:10px;"></a>
Another indicator of just how disconnected the property market, especially that of single family homes, is from the
local economy is that the combined rise of land value of all single family homes combined was $24.6bn, more than the entire
population of Vancouver combined income through work, investment (other than housing) or registered retirement income and
government transfer payments during that year. Even considering pre-tax income (although the housing price increase is tax free for all those
living in their house the year before they sell). <a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">Read here more details on this</a>
or <a href="http://www.mountainmath.ca/map/assessment?zoom=14&amp;lat=49.253&amp;lng=-123.1244&amp;layer=12">browse through the map</a> to see
just how much each Vancouver homeowner made last year twiddling thumbs and waiting for land values to rise.</p>

<p>So how can people still buy in this market? Except for the rare cases of people with very large salaries to be able to save
large sums for down payments and handle the mortgage payments, those purchases are founded in wealth, not income. Local wealth,
maybe inherited or handed down from parents when they cached out on their home, or money brought in from abroad.</p>

<p>What&rsquo;s clear is that this wealth-based gentrification process is real and rapidly changing the social character of the
neighbourhoods.</p>

<h3>Hard truths</h3>

<ol>
<li>We can&rsquo;t stop teardowns. The economic drivers are too strong. We can do better at designating and preserving heritage
sites and nudging the process in other ways. Naively projecting past data forward
<a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">we predict around 8,000 properties will be torn down and rebuilt</a>
in the next 10 years.</li>
<li>Affordable single family homes are a thing of the past. Demand is growing and supply is shrinking, so prices will
remain high and nothing short of Soviet-style interventions can change that.</li>
<li>Housing in many neighbourhoods is now out of reach for working professionals that can&rsquo;t rely on outside financial help
like inheritance or mortgages backed by the assets of their parents.</li>
<li>Foreign money may have played some role in getting us here, but any solution that solely revolves around dealing with
foreign money won&rsquo;t solve the problem. (Someone should run some
custom tabulations through Stats Canada to get a better handle on the signs of foreign money that can be picked up from the basic 2011
Census data.)</li>
</ol>


<h3>Preserving and Restoring Character (my 2c)</h3>

<p>Looking forward, how can the character of the neighbourhoods be preserved or restored? Most of the public discussion
focuses on the how to preserve the build form; the question how to preserve the
social structure of the neighbourhoods receives much less attention.</p>

<p>I don&rsquo;t have a good overview about what ideas are out there, so I won&rsquo;t attempt to summarize them. Usually I try to focus
on data and keep my opinions out of this, but I could not help myself on this one. So here comes my 2c on what should be done.</p>

<h4>Character of Buildings</h4>

<p>For buildings, we need to focus on what&rsquo;s important to
preserve. We need better rules on building waste. And we need to create policies that guide what teardowns will replace.</p>

<h4>Social Character</h4>

<p>To preserve and restore the social character of the neighbourhoods we need to create housing for the types of people that
used to be able to buy in the neighbourhood and are now being pushed out. That is working professionals. Given that single
family houses will remain out of reach, the only way to do that is to allow (low rise) condos in neighbourhoods. I don&rsquo;t know
if that means just upzoning arterials, or creating pockets of low-rise throughout neighbourhoods like it is common in my
<img  src="http://doodles.mountainmath.ca/images/arbutus-33.png" style="width:50%;float:right;">
native Germany. One example of this in Vancouver is the low-rise pocket at Arbutus and 33rd.
That complex, when taken together with all of Quilchena park, still houses roughly 3 times more people per area than the typical
single family housing neighbourhood with the same total area. And it collects the same amount of property taxes
per square metre as the typical single family neighbourhood (without a park) while it is cheaper to service.</p>

<p>Alternatives, like encouraging laneway houses and secondary suites can also increase supply and help ease the housing crunch.
They are great at increasing rental stock. But they don&rsquo;t increase supply fast enough, don&rsquo;t serve the
demand for home ownership and scatter up the green space into lots of tiny yards instead of freeing up space for larger
parks that can be effectively used for kids to play and recreation. Focusing only on towers instead just intensifies the
gentrification process, but there probably are areas where they make sense. There is no doubt that increasing supply
has to be part of the solution, any ideas like <a href="https://pricetags.wordpress.com/2016/01/31/qa-on-the-toughest-topic-in-town/">&lsquo;downzoning&rsquo;</a>
that fail to increase supply are counterproductive in restoring the social character of the neighbourhoods.</p>

<p><a href="http://censusmapper.ca/maps/88?zoom=14&amp;lat=49.2395&amp;lng=-123.1518"><img  src="http://doodles.mountainmath.ca/images/child_household_density.png" style="width:50%;float:left;margin-right:10px;"></a>
If we are concerned about creating space for families the question becomes where to house them. With land limited, let&rsquo;s
map the density of households with children, so the <a href="http://censusmapper.ca/maps/88?zoom=14&amp;lat=49.2395&amp;lng=-123.1518">number of households with children per km&sup2;</a>.
We immediately see that the existing pockets of mid and high-rise are very effective at housing children households. Using
Census data to model presence of children in households on physical housing parameters we see that having 3 or more bedrooms
is the single most determening factor. If accommodating families is a priority, then we need to ensure that all neighbourhoods
have 3 bedroom options, <a href="http://censusmapper.ca/maps/97">which is currently not the case (2011 data)</a>.</p>

<h4>Dealing with a Wealth-based market</h4>

<p>Moving from a housing market supported by income to one supported by wealth, either of domestic or foreign origin, requires
new rules to ensure fairness and preserve the basic social contract that our society is founded on: That everyone starts
out with more or less the same opportunities, and work and grit can lead to social and economic upward mobility.</p>

<p>Wealth, especially foreign wealth, tends to be much more mobile and can decide on short notice to leave Vancouver. This introduces
enormous risks to the local economy, and smart policies are needed to guard against these risks.</p>

<p>In Canada, people don&rsquo;t pay capital gains on their primary residence. That works fine as long as increases
in land value are relatively low, but with current situation where people <a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">earn more money sitting on their house than
actually working</a> the idea that income from
work should be taxes and income from twiddling thumbs should not is just ludicrous. Let&rsquo;s tax all capital gains on housing.
We can deduct investments into improvements and can argue about details like indexing taxable gains to inflation. And
let&rsquo;s tax it at a rate that is higher than the one on income or investments in stocks to re-focus housing as being treated
primarily to house people and not as investment.</p>

<p>This train of thought is similar in nature to the
<a href="http://www.theglobeandmail.com/news/british-columbia/bc-professors-call-for-property-surtax-on-vacant-vancouver-housing/article28235302/">Vacant Housing Tax</a>
in that it would have minimal direct impact
on the market (so it in itself won&rsquo;t bring prices down in a meaningful way), but it would generate substantial amounts of revenue
that could be used to provide relief. And the revenue is large enough that, if spent strategically (for example on building
affordable rental), could have a meaningful impact on the overall property market. And restore fairness in a market where
initial captial barriers are so high only the rich can participate.</p>

<p>Next, with the property market driven by wealth (especially for single family houses), while the way we finance
public services is based on income, we need to ask the question if our current taxation system is still adequate. A capital
gains tax on housing is just one part of this, the proposal to tax vacant properties is another. We could strengthen
legislation and enforcement on foreign income, assuming that this is a big enough issue (Give us better data!).
Other options that should get discussed are a higher property transfer tax, progressive property taxes
(possibly discounted against income tax) and other measures that pay more attention to the role of wealth vs income in
the Vancouver property market. This discussion is important, but requires better data to be meaningful and effective.</p>

<h4>Data</h4>

<p>The profound lack of data is a major problem to find effective ways to fix the affordability crisis. Lack of clear data
can provide a convenient excuse for inaction.</p>

<p>But data does not need to be perfect.<br/>
Much can be learned from the data at hand. A good list of data sources and reproducible findings would be a great way
to establish a baseline, build better arguments and keep the discussion focused on the &ldquo;big fish&rdquo;.</p>

<p>Maybe a dedicated github repro could provide a suitable format for this.</p>

<script>

function stacked_bar_graph(div,shiftAxis,domainFormatter,rangeFormatter,domainLabelFormatter){
    if (!domainFormatter) domainFormatter=d3.format("d")
    if (!rangeFormatter)
     rangeFormatter = function (y) {
        return y;
     };
     if (!domainLabelFormatter) domainLabelFormatter=domainFormatter;

var margin = {top: 20, right: 20, bottom: 40, left: 70},
    width = parseInt(div.style("width")) - margin.left - margin.right,
    height = parseInt(div.style("height")) - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);


var xAxis = d3.svg.axis()
    .scale(x)
    .tickFormat(domainFormatter)
    .orient("bottom");


var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .tickFormat(rangeFormatter)
    .ticks(5, rangeFormatter);

var svg = div.append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var data_url=div[0][0].dataset.url;
var legend=d3.select(div.node().parentNode).select('.legend');


d3.json(data_url, function(error, json) {
  if (error) throw error;
  var graphData=json[0];
  var data=graphData.data;
  var color = d3.scale.ordinal().domain(graphData.colors.map(function(d,i){return i}))
  .range(graphData.colors);
  var domain=data.map(function(d){return d.date;});
  x.domain(domain);

  function graphValueId(i){
      return graphData.class + '_' + i + '_value'
  }

  graphData.labels.forEach(function(text,i){
    var color=graphData.colors[i];
    var html='<i style="background:' + color + '"></i> ' + text + ' <span style="float:right;margin-right:10px;" id="' + graphValueId(i) + '"></span>'
    legend.append('p').html(html);
  });
  
  data.forEach(function(d) {
      var y0 = 0;
      d.values = color.domain().map(function(i) { return {date: d.date, y0: y0, y1: y0 += +d.count[i]}; });
      d.total = d.values[d.values.length - 1].y1;
  });
  y.domain([0, d3.max(data, function(d) { return d.total; })]);

  var domainTickValues=[];
  var skip=Math.round(40/x.rangeBand());
  if (skip<=0) skip=1;
  for (var i=0;i<x.domain().length;i++) {
    if (i % skip==0) domainTickValues.push(x.domain()[i]);
  }
  if (x.domain().length % 5 !=0) domainTickValues.push(x.domain()[x.domain().length-1]);
  xAxis.tickValues(domainTickValues);

  var xShift=shiftAxis ?  x.rangeBand()/2.0 * 1.1 : 0;
  
  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(" + xShift + "," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis);
//    .append("text")
//      .attr("transform", "rotate(-90)")
//      .attr("y", 6)
//      .attr("dy", ".71em")
//      .style("text-anchor", "end")
//      .text("Probability");

    function updateTooltip(d,i){
       color.domain().forEach(function(j){
             var value=d && i==j ? (domainLabelFormatter(d.date) + ': ' +rangeFormatter(d.y1-d.y0)) : '';
             d3.select('#'+graphValueId(j)).text( value);
       });
    }

  var year=svg.selectAll(".year")
    .data(data)
        .enter().append("g")
          .attr("class", "g");
  year.selectAll(".color-bar")
      .data(function(d) { return d.values; })
    .enter().append("rect")
      .attr("class", graphData.class + " color-bar")
      .attr("fill", graphData.color)
      .attr("x", function(d) { return x(d.date); })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.y1); })
      .attr("height", function(d) { return Math.max(0, y(d.y0) - y(d.y1)); })
      .attr("fill",function(d,i) {return color(i);})
      .on('mouseover',updateTooltip)
      .on('click',updateTooltip)
      .on('touch',updateTooltip) 
      .on('mouseout',function(){updateTooltip(null,i)});

      
});

}


var priceFormatter2=d3.format("$,");
    var priceFormatter = function (y) {
        return y>=1000000 ? (priceFormatter2(y/1000000) + 'm') : (priceFormatter2(y/1000) + 'k');
    };
    var brackets=[100000,200000,300000,400000,500000,600000,700000,800000,900000,1000000,2000000,10000000,20000000,40000000]

var binFormatter=function(top){
    var bottom=0;
    if (top<=1000000) bottom=top-100000;
    else if (top==2000000) bottom= 1000000;
    else if (top==10000000) bottom= 2000000;
    else if (top=20000000) bottom= 10000000;
    else bottom=20000000;
    return priceFormatter(bottom) + ' - ' + priceFormatter(top);
}
var numberFormatter=d3.format(",");
var numberBinFormatter=function(top){
    var     bins=[0,1,2,4,8,10,16,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,300,400,500,700];
    var index=0;
    while (bins[index]<top && index<bins.length) index ++;
    bottom=bins[index-1]+1;
    return (bottom == top) ? numberFormatter(bottom) : numberFormatter(bottom) + ' - ' + numberFormatter(top);
}
stacked_bar_graph(d3.select("#graph_sfh"));
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Land Use Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/01/31/land-use/"/>
    <updated>2016-01-31T10:25:42-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/01/31/land-use</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/land_use/map"><img  src="http://doodles.mountainmath.ca/images/land use.png" style="width:50%;float:left;margin-right:10px;"></a>
Metro Vancouver has a <a href="http://www.metrovancouver.org/data">fairly good land use dataset</a> that I imported quite some time
ago so that <a href="https://mountainmath.ca/land_use/map">I could take a good look at it</a>. But then I forgot about it until
<a href="https://twitter.com/HealthyCityMaps">@HealthyCityMaps</a> <a href="https://twitter.com/HealthyCityMaps/status/692832946714050564">reminded me</a> just
at the time when the discussions about <a href="https://mountainmath.ca/map/assessment">assessmened values</a> in Vancouver were flaring up again.</p>

<p>So finally I decided to mark up the City of Vancouver assessment dataset with the land use from the Metro Vancouver land
use dataset. And I used the occasion to update my color schemes from two years ago and gave it a dark background that
works better with the <a href="http://colorbrewer2.org">colorbrewer</a> palettes.</p>

<p>The result of the land use and assessment data mashup is a much better understanding what kind of properties we have right now in Vancouver.</p>

<!-- more -->


<h3>Land Use for Assessment Data</h3>

<p>There is a strong relationship between land use and zoning. But that relationship is far from perfect, zoning is a funny
beast. For example, zoning laws prevent low-rise apartments to be built in areas zoned for single family housing. But
they permit single family housing in areas zoned for low-rise apartments. Or in areas zoned as commercial. The land
use dataset allows to tag every property based on what it is actually used for.</p>

<p>Of course the land use dataset is not perfect and has some issues, but overall it gives a far superior view on how land
is used when compared to soley relying on zoning. But putting the two datasets together one can even further filter down
to tease out actual land use.</p>

<h3>The Forgotten Single Family Houses</h3>

<p>Single family houses have received a lot of attention lately, not least because of the
<a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">incredible gains in land value during the last year</a>.</p>

<p>But how to narrow down the assessed properties to only single family housing? The easiest way to do this is to use zoning
as a proxy, so <a href="https://mountainmath.ca/map/assessment?filter=rs">only use RS zoned properties</a>. But there are a number
of problems with that.</p>

<p><a href="https://mountainmath.ca/map/assessment?filter=rs_not_sfh&amp;layer=14"><img  src="http://doodles.mountainmath.ca/images/rs_not_sfh.png" style="width:50%;float:right;"></a>
Firstly, only filtering by zone will still include lots of unwanted properties like schools and parks. I used to filter out
parks using the City of Vancouver parks dataset, but it actually does not capture all parks. And then I filtered by area
to get rid of large properties that typically house schools or other institutions. But that keeps smaller institutions in
and throws larger single family houses out. Using the land use data we can better focus in on all the residential properties
in the single family housing zone. But even then, there are <a href="https://mountainmath.ca/map/assessment?filter=rs_not_sfh&amp;layer=14">1,255 residential properties in &lsquo;RS&rsquo; zoned areas that are
not single family houses</a>.</p>

<p><a href="https://mountainmath.ca/map/assessment?filter=sfh_not_rs&amp;layer=6"><img  src="http://doodles.mountainmath.ca/images/sfh_not_rs.png" style="width:50%;float:left;margin-right:10px;"></a>
Secondly, and probably more importantly, it misses
<a href="https://mountainmath.ca/map/assessment?filter=sfh_not_rs&amp;layer=6">the 15% (12,111 total) of the single family stock that is situated in other zones</a>.
&lsquo;Upzoning&rsquo; a neighbourhood
only means that other forms of housing are also permitted, but people can still build single family homes there if they
would like.
<a href="https://mountainmath.ca/map/assessment?filter=sfh_not_rs&amp;layer=6">Zoom into the map</a> and choose different ways to colour
the properties if you want to learn more about where they are.</p>

<h3>All Single Family Houses</h3>

<p><a href="https://mountainmath.ca/map/assessment?filter=sfh&amp;layer=9"><img  src="http://doodles.mountainmath.ca/images/sfh.png" style="width:50%;float:right;"></a>
When adding in the land use data we can filter by actual land use. That immediately takes care of parks and schools and other institutions,
and at the same time it allows to find all &ldquo;single family or duplex&rdquo; properties. That gets us closer to what we are looking
for, but it also includes duplexes. But those are easy to detect in the assessment dataset, they show up with two separate
tax bills. So combining the two we get the actual <a href="https://mountainmath.ca/map/assessment?filter=sfh&amp;layer=9">map of all single family homes</a>.</p>

<p>Are these really <em>all</em> single family homes? Not quite. There are some issues with the land use dataset that mis-classify
a couple of properties. In particular some RS lots without a house are classified as &lsquo;Undeveloped&rsquo;, and some are classfied as
&lsquo;Single Detached &amp; Duplex&rsquo;, probably depending on what was there around the time the data was collected. The correct classification really
depends on what one is interested in, and will likely be outdated fast.</p>

<p>Moreover, the property assessment dataset of the City of Vancouver is to correctly link a couple of hundred properties
to their tax data. This won&rsquo;t be an issue when analysing the tax data, but it does cause a problem when mapping the properties
as they won&rsquo;t have any assessment or zoning data associated with them.</p>

<p>And there are issues on the fringes. Some properties in Southlands are used as single family homes on large lots, rather
than the limited agricultural that it is zoned at and labeled as in the land use dataset.</p>

<p>But overall, adding in the land
use information gives a huge improvement on narrowing down single family homes. We should remember that
&ldquo;single family homes&rdquo; are better described as &ldquo;single owner properties that can&rsquo;t be stratified&rdquo;,
as they may include secondary suites and laneway houses.</p>

<h3>Historical Data</h3>

<p>One shortcoming of the land use dataset is that there are no historic versions available. It would be very nice to be able
to see how land use changed over time, but that dataset won&rsquo;t do the trick. The City of Vancouver assessment dataset
comes with historic data all the way up to 2006, and can be used to do some fun
<a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">analysis</a>. But the historic dataset is incomlete in that
zoning information is only available from 2014 forward and in that historic property tax data can&rsquo;t always be linked to
physical properties if the property has been re-developed. But overall putting the datasets together allows to refine
the analysis previously made. For example re-running the <a href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/">thumb twiddling rates</a>
using only Single Family Homes, we get pretty much the same medians, but the averages are higher. We could also refine
our <a href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/">teardown analysis</a> to include only single family homes
and better filter out institutional properties, although some issues with the lack of historic data remain.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Work vs Twiddling Thumbs]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs/"/>
    <updated>2016-01-24T10:28:37-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/01/24/work-vs-twiddling-thumbs</id>
    <content type="html"><![CDATA[<p><a href="http://www.mountainmath.ca/map/assessment?zoom=14&amp;lat=49.253&amp;lng=-123.1244&amp;layer=12"><img  src="http://doodles.mountainmath.ca/images/twiddling_thumbs.png" style="width:50%;float:left;margin-right:10px;"></a>In
the ongoing debade on affordability of housing in the City of Vancouver we regularly hear voices expressing concern for
&ldquo;<a href="http://news.nationalpost.com/full-comment/andrew-coyne-dont-blame-chinese-billionaires-for-vancouver-housing-prices">elderly folks of modest means who had been counting on using their home as a pension</a>&rdquo;
to argue against doing anything to ease the housing affordability problems many (younger) people are facing. In these
arguments the concern for &ldquo;elderly folks of modest means&rdquo; seems curiously focused to those who own property as opposed to,
for example, the 20% of the elderly people in Vancouver that live in poverty (15k total).</p>

<p>Which got me a little thinking about &ldquo;earning money&rdquo; in this day and age.</p>

<h3>Work</h3>

<!-- more -->


<p>Most people earn money by working. We are interested in after tax household income, for simplicity we will look at 2010 income
data reported in the 2011 census. The median after-tax household income in Vancouver
was $50k (the average was $65k), although there is <a href="http://censusmapper.ca/maps/224">large regional variability</a>.
The pre-tax households incomes were $56k for the median
and $80k for the average income.</p>

<p>Incomes have grown a bit since then, median income levels in
Vancouver grew by 7% from 2010 to 2013 (<a href="http://www5.statcan.gc.ca/cansim/a47">the latest year Stats Canada reports on</a>)
and will have grown a little more since then, but these are relatively small changes compared to other numbers I want
to talk about.</p>

<p><a href="http://censusmapper.ca/maps/189">Most of this income stems from employment income</a> which also includes self-employment,
but 22% of pre-tax income in Vancouver
came from outher sources, mostly investment income but also government transfer payments like child benefits.</p>

<p>Cumulative, the after tax income put $17.8bn into Vancouverite&rsquo;s pockets.</p>

<h3>Twiddling Thumbs</h3>

<p>Let&rsquo;s compare this to how much Vancouver home owner households earned last year by twiddling thumbs. Let&rsquo;s do a simple
calculation and focus on RS (&ldquo;One Family Dwelling&rdquo; zone) homes. We
will consider the change in land value only, changing the building value because of renovation or rebuilding certainly
does not happen by twiddling thumbs. And we will divide the land value rise of each property by the number of owners,
for RS that evenly divides up the land value increase between strata or duplex households that managed to sneak into the RS zoning.</p>

<p>The typical RS household made $267,000 last year (so that&rsquo;s the median land value rise in RS between 2015 and 2016
property tax assessment years, the average was $331,043), for a tity toal of $28,789,790,980. Not too shabby.
And considering that the typical owner household
lives in their home, these are tax-free earnings once the owners sell. That&rsquo;s substantially more than what the typical household
in Vancouver takes home. The total land value increase for just the RS zone properties amounts to $22.2bn out of
<a href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/">the $45.2bn by which land values in Vancouver increased overall</a>. For context,
recall that the overall income earned by all Vancouverites in 2010 was $17.8bn after tax (21.3bn before tax).</p>

<p>In other words, just the RS property owners earned more by twiddling their tumbs than the entire population of the
City of Vancouver did by &ndash; actually working. Yet another
indication of how <a href="http://censusmapper.ca/maps/37">out of touch the property market is with the local enconomy</a>.</p>

<h3>Mappig the Result of Thumb Twiddling</h3>

<p><a href="http://www.mountainmath.ca/map/assessment?zoom=14&amp;lat=49.253&amp;lng=-123.1244&amp;layer=12">There is a map for this</a>. (I probably would not take the time to write about this otherwise.) Last year&rsquo;s assessment
increase was quite large, not every year is like this (although it looks like the coming one will also be substantial).
To even things out a little I mapped the average yearly gross land value increase for each property. And for good measure I, divided
by the number of households for strata, duplex and other properties that are tied to several owners. For properties that I
could not trace back all the way to 2006 I averaged over as many years as I could find.</p>

<p><a href="http://www.mountainmath.ca/map/assessment?zoom=14&amp;lat=49.253&amp;lng=-123.1244&amp;layer=12"><img  src="http://doodles.mountainmath.ca/images/twiddling_thumbs.png" style="width:50%;float:right;"></a>
<a href="http://www.mountainmath.ca/map/assessment?zoom=14&amp;lat=49.253&amp;lng=-123.1244&amp;layer=12">Looking at the map</a> we see that
the average yearly increase depends to a large extend on the square footage of the property. Large properties naturally stand
out, unless they are stratified where the land value increase gets distributed across all the owners.</p>

<p>Averaged over 10 years, the yearly land value increase for the typical RS property in Vancouver was $82,700. Using Main as
a divider the numbers are $66,600 for the
typical east of Main and $145,100 for the typical west of Main property (using median increases). The average increases were $117,334 city wide,
$73,294 east and $175,773 west of Main.</p>

<p>A good portion of the east-west difference is due to larger lot size, but even the land values increase per m&sup2; for properties
on the west side was 35% higher than that of properties on the
east side. These are yearly increases, to be accumulated and collected tax free in most cases when the property is sold.</p>

<h3>Hourly Rate for Twiddling Thumbs</h3>

<p>Just for fun we can work out the hourly rate for twiddling thumbs. All we need to do is divide by the number of work
hours by year. We can look up the average work weeks for a Vancouver worker in the 2011 census and, for style points also consider the
number of household maintainers. After all, when we compare household income with land value increases we should consider how many
people in the household work. 63% of Vancouver households have only one household maintainer, but
<a href="http://censusmapper.ca/maps/223">33% have two and 4% three or more household maintainers</a> contributing to household income.
On average a Vancouver household <a href="http://censusmapper.ca/maps/222">worked a total of 62 person-weeks per year</a> to earn
that income. Assuming a 40 hour work week, we compute the average after-tax earning to be $26 per hour from regular work
and average after tax earnings of $113 per hour from twiddling thumbs. That&rsquo;s assuming all household maintainers are twiddling
the same amount of time that they dedicate to regular work. (In this calculation we use average not median income and land value
increases, medians don&rsquo;t lend themselves well to these kind of reasoning.)</p>

<p>Using the 10 year averaged numbers instead of focusing on just the last year we still get a healthy average after-tax twiddling thumbs rate of $47 per hour.</p>

<p>I am not entirely sure why anyone would want twiddling thumbs to stay so lucrative. Unless maybe the super self-centered thumb-twiddlers.
Or why earnings from thumb-twiddling should remain tax-free. While employment income is taxed (as it should be).</p>

<p>Considering to change your line work to thumb twiddling? To bad thumb twiddling is so unaffordable!</p>

<h2>Update</h2>

<p><a href="http://www.mountainmath.ca/map/special/34?layer=12"><img  src="http://doodles.mountainmath.ca/images/RS_original.png" style="width:50%;float:right;"></a>
I got some good feedback on the post so I decided to clear some things up and re-run the analysis with some small changes.</p>

<p>Linking a map of all properties when the analysis was only done on RS properties was a little confusing. So I added a map
<a href="http://www.mountainmath.ca/map/special/34?layer=12">of all RS zoned properties only</a> that the analysis was based on.</p>

<p>Some people correctly pointed out that there are some schools and other large properties that are RS zone but probably
should not be included in the analysis. So I decided to quickly re-run the analysis where I excluded properties with
area larger than 10,000m&amp;sup2. The cutoff is somewhat arbitrary, but it seemed about right so that schools and other large
properties are excluded, but most large single family lots are still included. The filter is not perfect, but probably
better than running the analysis without the filter. Here is the raw output with the area cutoff:</p>

<h5>Land Value Rise in RS (2015 - 2016)</h5>

<ul>
<li>City Wide: Average $318,877 Median $267,000, Count: 67269, Total: $21,450,566,401, Hourly: $128</li>
<li>East of Main: Average $235,871 Median $231,000, Count: 38319, Total: $9,038,373,901, Hourly: $95</li>
<li>West of Main: Average $428,745 Median $381,000, Count: 28950, Total: $12,412,192,500, Hourly: $172</li>
</ul>


<p>As expected, the area cutoff had almost no effect on the median, but did push the average down a bit.</p>

<p><a href="http://www.mountainmath.ca/map/special/37?layer=12"><img  src="http://doodles.mountainmath.ca/images/RS.png" style="width:50%;float:left;margin-right:10px;"></a>
The map shows the average increases over a 10 year timeframe, here are the numbers with the <a href="http://www.mountainmath.ca/map/special/37?layer=12">larger properties exluded</a>:</p>

<ul>
<li>City Wide: Average $112,588 Median $82,500, Count: 66556, Total: $7,493,456,449, Hourly: $45</li>
<li>East of Main: Average $70,109 Median $66,600, Count: 37960, Total: $2,661,346,243, Hourly: $28</li>
<li>West of Main: Average $168,978 Median $145,100, Count: 28596, Total: $4,832,110,206, Hourly: $68</li>
</ul>


<p>Also, I decided to do another run of the analysis including RT and RM zones (Two and Multiple Family Dwelling) next to RS (One Family Dwelling),
as well as Shaugnessy&rsquo;s FSD zone. Things get a little tricky in RT and RM. There are many single family properties in these
zones, so for those things work as expected. Then there are stratified properties, and we divide by the number of owners
to split up the land value rise. And there are larger rental buildings where the owner takes the entire land value increase.</p>

<p>Here is the raw output of the analysis:</p>

<h5>Land Value Rise in RS, RT, RM and FSD (2015 - 2016)</h5>

<ul>
<li>City Wide: Average $336,504 Median $265,000, Count: $84683, Total: $28,496,185,854, Hourly: $135</li>
<li>East of Main: Average $239,916 Median $230,000, Count: $47893, Total: $11,490,308,602, Hourly: $96</li>
<li>West of Main: Average $462,241 Median $375,000, Count: $36790, Total: $17,005,877,252, Hourly: $186</li>
</ul>


<p>To round things up, here are the numbers for RS, RT, RM and FSD zones averaged over 10 years:</p>

<h5>Land Value Rise in RS, RT, RM and FSD (2006 - 2016)</h5>

<p><a href="http://www.mountainmath.ca/map/special/36?layer=12"><img  src="http://doodles.mountainmath.ca/images/RS+RT+RM+FSD.png" style="width:50%;float:right;"></a></p>

<p>City Wide: Average $119,596 Median $83,000, Count: 83479, Total: $9,983,800,611, Hourly: $48
East of Main: Average $72,767 Median $67,200, Count: 47254, Total: $3,438,565,286, Hourly: $29
West of Main: Average $180,682 Median $143,600, Count: 36225, Total: $6,545,235,325, Hourly: $72</p>

<p>And for good measure, <a href="http://www.mountainmath.ca/map/special/36?layer=12">here is the map for the properties included in the last analysis</a>.</p>

<h2>Later update</h2>

<p>After importing the <a href="http://doodles.mountainmath.ca/blog/2016/01/31/land-use/">Metro Vancouver land use data</a> we can
now pinpoint all Single Familiy Dwellings and don&rsquo;t have to use zoning as a proxy. Here are the numbers when running them
only on all of City of Vancouver&rsquo;s 78,740 Single Family Dwellings:</p>

<h5>SFH Land Value Rise (2015 - 2016)</h5>

<ul>
<li>City Wide: Average $313,072 Median $262,000, Count: 78740, Total: $24,651,312,001, Hourly: $126</li>
<li>East of Main: Average $230,106 Median $230,000, Count: 46014, Total: $10,588,122,001, Hourly: $92</li>
<li>West of Main: Average $429,725 Median $375,000, Count: 32726, Total: $14,063,190,000, Hourly: $173</li>
</ul>


<h5>SFH Land Value Rise (2006 - 2016)</h5>

<ul>
<li>City Wide: Average $109,969 Median $79,800, Count: 77981, Total: $8,575,547,711, Hourly: $44</li>
<li>East of Main: Average $68,656 Median $66,700, Count: 45647, Total: $3,133,947,703, Hourly: $27</li>
<li>West of Main: Average $168,293 Median $141,700, Count: 32334, Total: $5,441,600,008, Hourly: $67</li>
</ul>


<p>The total number of units for the 2006 to 2016 analysis are a little lower since not all properties can be traced over that time
period without resorting to more tedious analysis.</p>

<p>And <a href="https://mountainmath.ca/map/assessment?filter=sfh&amp;layer=12">here is the obligatory map for the properties included in the last analysis</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Teardowns]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/"/>
    <updated>2016-01-18T09:52:33-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/special/17"><img  src="http://doodles.mountainmath.ca/images/teardowns.png" style="width:50%;float:left;margin-right:10px;"></a>
On the heels of the <a href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/">new assessment data</a> we
can start to slice the data in different ways to understand various aspects of the real estate landscape in Vancouver. The
fact that <a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data</a> makes historic data available
gives the ability to look for changes over time.</p>

<p><a href="https://mountainmath.ca/map/assessment">Our maps</a> explore this by visualizing some aspects of these changes for all
properties, but it might also be useful to filter the properties we show to focus in on specific criteria.</p>

<p>&ldquo;Teardowns&rdquo; always triggers lots of emotions in Vancouver. Without looking at the emotional side and trying to avoid any
judgement we will
investigate the data to understand what buildings have been torn down recently and predict which buildings will get torn
down next. And map them. Long story short, we predict that
1 in 6 buildings <a href="https://mountainmath.ca/map/special/17?layer=4">on this map</a> (and then some more with lower teardown probability)
will get torn down and rebuilt by 2026.</p>

<h3>Building age temporal distribution</h3>

<!-- more -->


<p>To start understanding teardowns and rebuilds let&rsquo;s look at the <a href="https://mountainmath.ca/map/assessment?layer=7">age of the building stock</a>.</p>

<p>To get a better overview of the building stock through time we can graph the number of buildings by age. We look at buildings, not units. So a stratified
building with 100 units would still count as one building in our graph. And it is not looking at how many buildings were
built in each year, but how many buildings that were built in a given year are still standing today.</p>

<p>We still have 7 buildings in Vancouver that were built before 1900 (the earliest from 1800). Skipping these we graph the
rest to get:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_age" style="height:200px;max-width:640px;" data-url="/data/building_age.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of Buildings by Building Age <span style="float:right;margin-right:10px;" id="building_age_value"></span></p>
</div>
</div>


<p>Starting with 1950 the distribution of buildings by age is quite uniform, with a short peak around the early 1990s.</p>

<p>The dip at the end is due to some lag in new buildings showing up in the property dataset. Looking at the more recent history
it is safe to assume that the number of buildings still standing corresponds well to the buildings of units built in that year.
So the pace of new buildings right now seems to fit in quite well with the recent history and is a little lower than the
peak in the early 1990s.</p>

<h3>Recent Building stock (and recent teardowns) spatial distribution</h3>

<p><a href="https://mountainmath.ca/map/special/15"><img  src="http://doodles.mountainmath.ca/images/rebuilds.png" style="width:50%;float:right"></a>
The next question is to focus on the spatial distribution of recent redevelopment by filtering out older buildings.
Being too lazy to add a bush for dynamic selection of time ranges I just made a static (in time) view only showing the
<a href="https://mountainmath.ca/map/special/15?zoom=13">6883 properties built after 2006</a>. It is quite safe to assume that most
of those new buildings replaced older ones that were torn down. So this map of new buildings is also a map of locations
of buildings that were torn down in the last 10 years.</p>

<p>What&rsquo;s interesting is when selecting
<a href="https://mountainmath.ca/map/special/15?zoom=16&amp;lat=49.2341&amp;lng=-123.1824&amp;layer=4">relative building value view</a> that
there are some properties that have been recently re-developed with increadibly low building value, like the property
at 5649 Dunbar St. <strike>This gives a window into some of the imperfections of the BC Assessment process where the building
value after re-development is not properly reflected in their dataset.</strike> In this case it seems to be
<a href="https://www.google.ca/maps/@49.2351182,-123.1853038,3a,75y,264.36h,81.1t/data=!3m6!1e1!3m4!1s_weW4bEWpmcKGA-Ppk0d_Q!2e0!7i13312!8i6656">a property whose only &ldquo;improvement&rdquo; seems to be the pavement on it</a>.</p>

<p>It also shows that recent building (or teardown) activity is fairly uniform across the city, with only some areas standing out as having little
development like the West End, parts of Kitsilano and Strathcona.</p>

<h3>What gets torn down and rebuilt next?</h3>

<p>The big question is of course where new buildings get built next. In a built up space like Vancouver there are few sites
left where building a new building does not mean tearing down an old one. So another way to ask that question is: What
gets torn down next?</p>

<h3>Teardown Probability</h3>

<p>Predicting which building will get torn down next is of course impossible. So what we try to do is assign a &ldquo;teardown
probability&rdquo; to each building.</p>

<p>Let&rsquo;s first try to understand why a particular building might get torn down as opposed to the one next door. Typically
buildings get torn down at the time when they change ownership. So if a building is not sold, it is far less likely to
get torn down. So what makes a building more likely to get torn down when it is sold? One hypothesis would be that the
value of the building relative to the land should play an important factor. Let&rsquo;s test this hypothesis using the data.</p>

<p>We take the 2006 tax dataset as a baseline and check how many of the buildings have been torn down by 2016. Refer to the
Methodology and Data section at the botton for the messy details. We only
count buildings, so we count a strata lot with 100 units in the same building as one building. Then we use the 2016 dataset
to check how many of them are still around, identifying them by their tax coordinate and again asking they be marked as
being built no later than 2006.</p>

<p>These criteria capture well what we are looking for, but they are not perfect. As a predictive variable we use the</p>

<div style="padding:5px;border: 1px solid grey;border-radius:4px;width:80%;margin:0 auto;">
<h5 style="text-align:center;">Teardown Coefficient</h5>
The *teardown coefficient* is the percentage of the total assessed value that is attributed to the building. More formally
it&#8217;s the ratio of the building value by the sum of the building and land values.
</div>


<p>So we sort the properties by their <em>teardown coefficient</em> using the
2006 tax assessment data and we check how each group fares.</p>

<p>First up a graph of the distribution of buildigs in 2006 by their <em>teardown coefficient</em>.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_buildings_by_teardown" style="height:200px;max-width:640px;" data-url="/data/buildings_by_teardown.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of Buildings by *teardown coefficient* <span style="float:right;margin-right:10px;" id="buildings_by_teardown_value"></span></p>
</div>
</div>


<p>Next up the number of buildings in each category that got torn down and rebuilt by 2016:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_teardowns" style="height:200px;max-width:640px;" data-url="/data/teardowns.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of Torn Down Buildings by *teardown coefficient* <span style="float:right;margin-right:10px;" id="teardowns_value"></span></p>
</div>
</div>


<p>We see that our initial hypothesis seems to hold up quite well. The number of buildings that got torn down and rebuilt
decreases as the <em>teardown coefficient</em> increases. Remember that we defined the <em>teardown coefficient</em> to be the percentage
of the building value out of the total value of the property.</p>

<p>Refer to the methodology and data section for further information on how these numbers were extracted.</p>

<p>To explore this further let&rsquo;s graph the frequency with which a building in a given <em>teardown coefficeint</em> range gets torn down.
To keep things cleaner where we only plot up to a <em>teardown coefficient</em> of 50%:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_teardown_probability" style="height:200px;max-width:640px;" data-url="/data/teardown_probability.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Probability of Building being torn down<span style="float:right;margin-right:10px;" id="teardown_probability_value"></span></p>
</div>
</div>


<p>We see that the <em>teardown coefficient</em> has high predictive value for a building to be torn down and
being rebuilt in the following 10 years. Buildings with a <em>teardown coefficient</em> below 5% have about an 18% chance, and the
probability declines exponentially down to zero at a <em>teardown coefficient</em> of about 50%.</p>

<p>If we were more serious about this
we would fit and exponential curve to the data and compute how well it fits the data, repeat the computation for
other time frames, run it on individual neighbourhoods and maybe also on <a href="https://data.surrey.ca">data from other municipalities</a>
to properly validate our model. We could also refine the model by refining our filters, see the methodology and data section for
more details.</p>

<p>And we could add other factors that likely effect the teardown probability, like building age, proximity to arterials and
others. Of course these are not independent factors, so this kind of analysis requires more time.</p>

<h3>Predicting Teardowns</h3>

<p>Now to the main part: Predicting teardowns. How many buildings will get torn down and rebuilt in the next 10 years? Let&rsquo;s
use what we have just learned to extrapolate.</p>

<p>First up the graph of the 2016 building stock by <em>teardown coefficient</em>:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_buildings_by_teardown_2" style="height:200px;max-width:640px;" data-url="/data/buildings_by_teardown_2.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> 2016 Building stock by *teardown coefficient* <span style="float:right;margin-right:10px;" id="buildings_by_teardown_2_value"></span></p>
</div>
</div>


<p>To estimate how many buildings will get torn down and rebuilt in each category we simply multiply each bin with the teardown probability
from the frequency graph above:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_teardowns_2" style="height:200px;max-width:640px;" data-url="/data/teardowns_2.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Estimate of Buildings rebuilt by 2026 <span style="float:right;margin-right:10px;" id="teardowns_2_value"></span></p>
</div>
</div>


<p>Bottom line, we predict around 8,000 buildings to be torn down and rebuilt by 2026. That&rsquo;s significantly more than the
around 5,900 buildings that we identified as going through this process during the prior 10 years.</p>

<h3>Open Question</h3>

<p>There are lots of assumptions that went into this estimate. While we are confident in our analysis
that properties with low <em>teardown coefficient</em> are the ones most likely to be torn down, it is less clear if the number
of properties being torn down grows linearly as the properties with low <em>teardown coefficient</em> grow. In our case the number
of properties with <em>teardown coefficient</em> below 5% grew from 20492 (21% of the 2006 stock) to 32509 (33.5% of the 2016) stock,
which may be out of the range where our simplistic extrapolation holds. One could try to understand this by carefully
analyzing all available tax years, and not just the two extremes of the available spectrum.</p>

<h3>Mapping Teardowns</h3>

<p><a href="https://mountainmath.ca/map/special/17"><img  src="http://doodles.mountainmath.ca/images/teardowns.png" style="width:50%;float:right"></a>Now that we
understand how to assign a teardown probability to buildings, let&rsquo;s map them! To keep things as simple as
possible let&rsquo;s focus in on the homes with a <em>teardown coefficient</em> below 5%. They make up the bulk in our prediction and
have the simple interpretation that a little more than 1 in 6 of these will get replaced by something else by 2026. So
<a href="https://mountainmath.ca/map/special/17">here is the interactive map</a> of just these 31301 buildings, where we have
filtered out some parks, marinas and rail lines. And this only accounts
for the 5,700 buildings predicted to be torn down with a <em>teardown coefficient</em> below 5% cutoff and neglects the roughly 2,000 more that
are predicted to be torn down that have a <em>teardown coefficient</em> above 5%.</p>

<h3>Methodology and Data</h3>

<p>Only for people who love getting their hands dirty or who want to reproduce or expand on the analysis.</p>

<p>First thing to note is that there is no way to detect &ldquo;teardowns&rdquo; in the dataset, the only way is to look at what has been
rebuilt and what has &lsquo;dropped off&rsquo;. To be more precise, there data fields to look at is the &ldquo;land coordinate&rdquo;, which links
a taxable property to a physical structure, and the &ldquo;year built&rdquo;. And both fields have problems.</p>

<p>The &ldquo;land coordinate&rdquo;
gets de-commissioned and re-assigned during certain re-develpments. And the city dataset provides no way to link the
old one to the new one. One way to do that is through the polygons that mark the property boundaries, that would allow
tracking of complex re-assemblies of land. But the city does not publish historic records of property polygons.</p>

<p>The &ldquo;year built&rdquo; also has lots of issues. Sometimes it is blank even though it records the value of the building as
greater than zero. Sometimes the &ldquo;year built&rdquo; will be set to a date later than the date of the dataset, for example the
2006 tax dataset has buildings with &ldquo;year built&rdquo; all they way up to 2013.</p>

<p>Then comes the issue of filtering. We decided to filter out parks, rail lines and marinas without structures on them. The
algorithm is somewhat simplistic, it&rsquo;s the same one that was used to filter properties for the maps. Additionally we
filter out properties from the heritage dataset. There is definitely
room for improvement here, but without a clear question of what exactly to measure
(only single family homes, or also condos or apartments, treat commercial separately, &hellip;)
it does not make much sense to invest energy into this. After all, this is just looking for a rough model.</p>

<p>So how do we detect rebuilds? We take the land coordinates from properties identified as park or heritage and sieve through
the 2006 tax data to retrieve all records that don&rsquo;t match these land coordinates and have a &ldquo;year built&rdquo; column set
as 2006 or earlier or don&rsquo;t have a &ldquo;year built&rdquo; set at all but change from zero to non-zero building value from 2006 to 2016.</p>

<p>Pretty messy. We mapped <a href="https://mountainmath.ca/map/special/15">about 6,900 buildings were built after 2006</a>,
but only traced 5,869 buildings in the 2006 tax dataset as
being torn down and rebuilt. That difference is largely explained by different selection criteria. The map only considers
properties with a &ldquo;year built&rdquo; field set, but for the analysis we also added properties that don&rsquo;t have that field set
but go from zero building value in 2006 to non-zero building value in 2016 which gets us to 7,784 &ldquo;rebuilds&rdquo;. On the other hand
in the analysis we don&rsquo;t consder the roughly 140 heritage buildings that would pass our filter of being built after 2006, and
the 2016 tax dataset has 2,422 more buildings than the 2006 dataset, some of which can be seen
<a href="https://mountainmath.ca/map/special/13">on this map</a> and are due to subdivisions being split off of the original
property.</p>

<p>Anyway, if you want to get your hand dirty on this shoot me a message and I will hook you up with my scripts.</p>

<script>


function bar_graph(div,shiftAxis,domainFormatter,rangeFormatter,domainLabelFormatter){
    if (!domainFormatter) domainFormatter=d3.format("d")
    if (!rangeFormatter)
     rangeFormatter = function (y) {
        return y;
     };
     if (!domainLabelFormatter) domainLabelFormatter=domainFormatter;

var margin = {top: 20, right: 20, bottom: 40, left: 70},
    width = parseInt(div.style("width")) - margin.left - margin.right,
    height = parseInt(div.style("height")) - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);


var xAxis = d3.svg.axis()
    .scale(x)
    .tickFormat(domainFormatter)
    .orient("bottom");


var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .tickFormat(rangeFormatter)
    .ticks(5, rangeFormatter);

var svg = div.append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var data_url=div[0][0].dataset.url;

d3.json(data_url, function(error, json) {
  if (error) throw error;
  var graphData=json[0];
  var data=graphData.data;
  x.domain(data.map(function(d) { return d.date }));
  y.domain([0, d3.max(data, function(d) { return d.count; })]);
  
  var domainTickValues=[];
  var skip=Math.round(40/x.rangeBand());
  if (skip<=0) skip=1;
  for (var i=0;i<x.domain().length;i++) {
    if (i % skip==0) domainTickValues.push(x.domain()[i]);
  }
  if (x.domain().length % 5 !=0) domainTickValues.push(x.domain()[x.domain().length-1]);
  xAxis.tickValues(domainTickValues);

  var xShift=shiftAxis ?  x.rangeBand()/2.0 * 1.1 : 0;
  
  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(" + xShift + "," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis);
//    .append("text")
//      .attr("transform", "rotate(-90)")
//      .attr("y", 6)
//      .attr("dy", ".71em")
//      .style("text-anchor", "end")
//      .text("Probability");

  svg.selectAll(".bar")
      .data(data)
    .enter().append("rect")
      .attr("class", graphData.class + " bar")
      .attr("fill", graphData.color)
      .attr("x", function(d) { return x(d.date); })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.count); })
      .attr("height", function(d) { return height - y(d.count); })
      .on('mouseover',function(d){
         d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('click',function(d){
       d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('touch',function(d){
         d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});

      
});

}

var percentageFormatter=d3.format(".1%");
var binFormatter=function(d){return percentageFormatter(d-0.025) + ' - ' + percentageFormatter(d);}
//ready_for_graph(d3.select("#graph"));
bar_graph(d3.select("#graph_age"),false);
//ready_for_graph(d3.select("#graph_buildings_by_teardown"),percentageFormatter);
//ready_for_graph(d3.select("#graph_teardowns"),percentageFormatter);
bar_graph(d3.select("#graph_buildings_by_teardown"),true,percentageFormatter,null,binFormatter);
bar_graph(d3.select("#graph_teardowns"),true,percentageFormatter,null,binFormatter);
bar_graph(d3.select("#graph_teardown_probability"),true,percentageFormatter,percentageFormatter,binFormatter);
bar_graph(d3.select("#graph_buildings_by_teardown_2"),true,percentageFormatter,null,binFormatter);
bar_graph(d3.select("#graph_teardowns_2"),true,percentageFormatter,null,binFormatter);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updated Vancouver Assessment Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/"/>
    <updated>2016-01-17T11:34:09-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/assessment?layer=4"><img  src="http://doodles.mountainmath.ca/images/relative_land.jpg" style="width:50%;float:right"></a>
The friendly folks at
<a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data</a> just
<a href="https://twitter.com/VanOpenData/status/688060388190097408">updated their property assessment data</a> with the fresh 2016
property tax assessments. Time to run the script to update the <a href="https://mountainmath.ca/map/assessment">Vancouver Assessment Map</a>
with the new data. And for good measure I pasted over some of the thematic map engine from <a href="https://censusmapper.ca">CensusMapper</a>
to improve the mapping performance.</p>

<h4>Maps</h4>

<!-- more -->


<p><img  src="http://doodles.mountainmath.ca/images/map_menu.png" style="width:25%;margin-left:5px;float:right">The <a href="https://mountainmath.ca/map/assessment">interactive assessment map</a> offers several views. In the panel on the top
right we can select how to view the data. It offers standard thematic maps for value change, total value, building value
and building age and zoning. And there are some options that warrant more explanation:</p>

<ul>
<li><a href="https://mountainmath.ca/map/assessment?layer=5"><em>Relative Land Value</em>:</a> The colours on the map show each property by the
land value per m&sup2;. We can immediately spot the east-west land gradient, as well as how zoning affects land value.
When zooming in we also see the effect of lot size on land value.</li>
<li><a href="https://mountainmath.ca/map/assessment?layer=10"><em>Tax Density</em>:</a> This map looks at the tax dollars collected
by the city by area. It tells us the relative rate at what each property is contributing to city services. We can again
observe the impact of exclusionary zoning.</li>
<li><a href="https://mountainmath.ca/map/assessment?layer=4"><em>Relative Value of Building</em>:</a>
<a href="https://mountainmath.ca/map/assessment?layer=4"><img  src="http://doodles.mountainmath.ca/images/teardown.jpg" style="width:50%;float:right"></a>
This map simply divides the building value
by the total property value. There are many ways to interpret this map, my favourite is to use this as a &ldquo;Teardown Predictor&rdquo;.
Essentially, as the percentage of the building value approaches zero the probability that it will get torn down in the
near future increases. Imagine someone spending $1.5m to buy a property with a house valued at $37k. Many people don&rsquo;t
mind living in a house worth $37k, but someone who is spending $1.5m would probably prefer to buy a different property
with a higher quality house. Or spend more money to upgrade the house. How will it be upgraded? Renovating is in most
cases economically unsound, most people will choose to tear down and rebuild. In fact, the <em>teardown threshold</em> is
likely higher than the 2.5% in the example given. The percentage of properties in Vancouver where the building value is
less than 2.5% of the total value has slightly decreaed in the last year from 17.9% to 17.8%, but the percentage of properties
with building value less than 5% of the total value has increased from 32% to 33.5% during the last year.</li>
</ul>


<h4>The Data</h4>

<p>The data originates with BC Assessment, which estimates land and building values of each property based on recent sales
of comparable properties. The assessment was done in summer 2015 and is based on sales before that, so at this point in
time the data lags the market by about one year. Values for individual properties may well be off, depending how well
renovations and improvements were reported and how well the BC Assessment estimates work for the given property. On
average they should reflect the market about half a year to one year ago.</p>

<p>Sadly, BC Assessment does not give out their data with a license that would allow mapping it the way I do, so we have
to rely on municipalities to release it through their open data portals. The format of the data from each municipality is
different, so lazy me is only importing data from City of Vancouver, although some other nearby municipalities are also releasing
there data.</p>

<p>The motivation behind the map was to understand the building stock. Some effort was made to filter out parks, but the
algorithm is far from perfect and will often includes parks that host building
structures, as well as marinas with structures on them.</p>

<p>The new city dataset does not include the 2016 tax levy, so we still only show the 2015 tax levies until CoV updated their dataset.</p>

<h4>History</h4>

<p>Here is a quick history of the overall land and building values aggregated for Vancouver between 2006 and 2016.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph" style="height:200px;max-width:640px;" data-lines="/data/vancouver_stats.json"></div>
<div class="legend no-margin">
  <p><i style="background:blue"></i> Land Value <span style="float:right;margin-right:10px;" id="land_value"></span></p>
  <p><i style="background:green"></i> Building Value <span style="float:right;margin-right:10px;" id="building_value"></span></p>
</div>
</div>


<p>When looking at all properties in the city, the increase in land
value year over year was 21.4% ($45.2bn), while overall building values increased by 7.3% ($5bn). Hover, click or touch
the points in the graph to get the values for the corresponding year.</p>

<h4>Neighbourhoods</h4>

<p>Lastly a quick overview over the neighbourhoods. Land and building values have not increased evenly throughout in the year.
I aggregated all tax data by neighbourhood and split it into land value and building value increases.
These numbers should be used as guidance only, they mix lots of different types of properties and include parks.</p>

<p>Here is the breakdown by neighbourhood:</p>

<ul>
<li>Renfrew-Collingwood: Land: 30.6% ($1.2bn), Building: 5.8% ($46.0m)</li>
<li>Sunset: Land: 26.6% ($1.6bn), Building: 5.9% ($74.2m)</li>
<li>Oakridge: Land: 17.6% ($1.2bn), Building: 12.6% ($207.4m)</li>
<li>Downtown: Land: 16.9% ($0.8bn), Building: 3.2% ($91.2m)</li>
<li>Kerrisdale: Land: 18.7% ($1.3bn), Building: 3.3% ($54.1m)</li>
<li>Victoria-Fraserview: Land: 28.0% ($1.5bn), Building: 5.6% ($65.2m)</li>
<li>Grandview-Woodland: Land: 24.9% ($1.7bn), Building: 0.7% ($17.4m)</li>
<li>West End: Land: 22.6% ($2.5bn), Building: 3.5% ($188.4m)</li>
<li>Hastings-Sunrise: Land: 25.1% ($1.1bn), Building: 4.9% ($63.8m)</li>
<li>Killarney: Land: 17.8% ($0.3bn), Building: 14.1% ($241.6m)</li>
<li>Marpole: Land: 17.8% ($0.3bn), Building: 14.1% ($241.6m)</li>
<li>Kitsilano: Land: 21.6% ($3.3bn), Building: 4.3% ($134.5m)</li>
<li>Shaughnessy: Land: 18.6% ($1.7bn), Building: 7.4% ($117.7m)</li>
<li>West Point Grey: Land: 20.2% ($2.3bn), Building: 5.2% ($88.3m)</li>
<li>Fairview: Land: 19.4% ($2.0bn), Building: -1.0% (-$52.4m)</li>
<li>Downtown Eastside: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)</li>
<li>Kensington-Cedar Cottage: Land: 24.9% ($1.7bn), Building: 0.7% ($17.4m)</li>
<li>Riley Park: Land: 26.4% ($1.0bn), Building: 4.0% ($40.2m)</li>
<li>Mount Pleasant: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)</li>
<li>South Cambie: Land: 20.9% ($1.5bn), Building: 21.4% ($268.1m)</li>
<li>Strathcona: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)</li>
<li>Dunbar Southlands: Land: 21.8% ($1.0bn), Building: 11.2% ($126.6m)</li>
<li>Arbutus Ridge: Land: 21.4% ($1.6bn), Building: 1.1% ($17.3m)</li>
</ul>


<p>It becomes immediately clear that the increase in property values is mostly driven by land, note that total value increases
for land and buildings are reported in billions and millions, respectively. The building stock does not
have time to catch up, with the exception of South Cambie. Fairview stands out with declining overall building values.</p>

<script>
var ready_for_graph = function() {
    var d3lines=[];
    var padding = {top: 20, right: 20, bottom: 30, left: 90};
    var prevChartWidth = 0, prevChartHeight = 0;
    var updateTransistionMS = 750; // milliseconds
    var sourceData, lineData, xScale, yScale, line;

    var symbol;
    var prefix;
    var numberFormatter = function (y) {
        return '$' + Math.round(prefix.scale(y*10))/10.0 + symbol;
    };

    var graphs=d3.select("#graph");
    var div=graphs[0][0];
    if (div==null|| div.childElementCount!=0) {return;}
    var data_url=div.dataset.url;

    // create svg and g to contain the chart contents
    var baseSvg = graphs.append("svg");
    var chartSvg=baseSvg
        .append("g")
        .attr("class", "chartContainer")
        .attr("transform", "translate(" + padding.left + "," + padding.top + ")");

    // create the x axis container
    chartSvg.append("g")
        .attr("class", "x axis");

    // create the y axis container
    chartSvg.append("g")
        .attr("class", "y axis");
    var line;
    var largest=null;
    var lineData;
    if (div.dataset.lines) {
        d3.json(div.dataset.lines,function(error,json){
        lineData=json;
        var domain=[null,null];
        var range=[null,null];
        for (var i=0;i<lineData.length;i++){
            lineData[i].data.forEach(function(d) {
                d.date = +d.date;
                d.count = +d.count;
                if (domain[0]==null || domain[0]> d.date) domain[0]= d.date;
                if (domain[1]==null || domain[1]< d.date) domain[1]= d.date;
                if (range[0]==null || range[0]> d.count) range[0]= d.count;
                if (range[1]==null || range[1]< d.count) range[1]= d.count;
            });
        }
        xScale=d3.scale.linear().domain(domain);
        var toAdd=(range[1]-range[0])/10;
        range[0]-=toAdd;
        range[1]+=toAdd;
        yScale=d3.scale.linear()
            .domain(range);

        line = d3.svg.line()
            .x(function(d) { return xScale(d.date); })
            .y(function(d) { return yScale(d.count); })
            .interpolate("linear");

        xAxis = d3.svg.axis()
            .scale(xScale)
            .orient("bottom")
            .tickFormat(d3.format("d"))
            .tickValues(domain);

        yAxis = d3.svg.axis()
            .scale(yScale)
            .orient("left")
            .tickFormat(numberFormatter)
            .ticks(5);

        prefix = d3.formatPrefix(range[1]);
        if (prefix.symbol=='K') {
            symbol='k'
        } else if (prefix.symbol=='M') {
                symbol='m'
        } else if (prefix.symbol=='G') {
            symbol='bn'
        } else if (prefix.symbol=='T') {
            symbol='tn'
        }
        updateChart(true);
        });

    }


    function updateChart(init)
    {
        // get the height and width subtracting the padding
//    var innerHeight = window.innerHeight - 20;
        var innerWidth = window.innerWidth - 20;
        var divWidth=$(div).width();
        if (divWidth==0) divWidth=$(div.parentElement.parentElement).width();
        var maxWidth=parseInt($(div).css('max-width'));
        if (divWidth==0) divWidth=innerWidth*0.8;
        if (divWidth>maxWidth) divWidth=maxWidth;
        var chartWidth = divWidth-padding.left-padding.right;//960 - margin.left - margin.right,
        var chartHeight = $(div).height()-padding.top-padding.bottom;//500 - margin.top - margin.bottom;


        // only update if chart size has changed
        if ((prevChartWidth != chartWidth) ||
            (prevChartHeight != chartHeight)) {
            prevChartWidth = chartWidth;
            prevChartHeight = chartHeight;

            //set the width and height of the SVG element
            chartSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);
            baseSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);

            // ranges are based on the width and height available so reset
            xScale.range([0, chartWidth]);
            yScale.range([chartHeight, 0]);

            if (init) {
                // if first run then just display axis with no transition
                chartSvg.select(".x")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                chartSvg.select(".y")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .call(yAxis);
            }
            else {
                // for subsequent updates use a transistion to animate the axis to the new position
                var t = chartSvg.transition().duration(updateTransistionMS);

                t.select(".x")
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                t.select(".y")
                    .call(yAxis);
            }

            for (var i = 0; i < lineData.length; i++) {
                var sourceData=lineData[i].data;
                var color=lineData[i].color;
                var label=lineData[i].label;
                var className=lineData[i].class;

                // bind up the data to the line
                var lines = chartSvg.selectAll("path.line."+className)
                    .data([lineData[i].data]); // needs to be an array (size of 1 for our data) of arrays

                // transistion to new position if already exists
                lines.transition()
                    .duration(updateTransistionMS)
                    .attr("d", line);

                // add line if not already existing
                lines.enter().append("path")
                    .attr("class", "line")
                    .attr("stroke", color)
                    .attr("stroke-width", 2)
                    .attr('fill','none')
                    .attr("d", line);

                // bind up the data to an array of circles
                var circle = chartSvg.selectAll("circle."+className)
                    .data(sourceData);

                // if already existing then transistion each circle to its new position
                circle.transition()
                    .duration(updateTransistionMS)
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", function (d) {
                        return yScale(d.count);
                    });

                // if new circle then just display
                circle.enter().append("circle")
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", function (d) {
                        return yScale(d.count);
                    })
                    .attr("r", 4)
                    .attr('fill', 'transparent')
                    .style("stroke", color)
                    .style("stroke-width", 8)
                    .attr("class", className)
                    .on('mouseover',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + numberFormatter(d.count)) 
                    }).on('click',function(d){
                     d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + numberFormatter(d.count)) 
                    }).on('touch',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + numberFormatter(d.count)) 
                    }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});

            }
        }
    }

    // look for resize but use timer to only call the update script when a resize stops
    var resizeTimer;
    window.onresize = function(event) {
        clearTimeout(resizeTimer);
        resizeTimer = setTimeout(function()
        {
            updateChart(false);
        }, 100);
    }


};
ready_for_graph();
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Canvas vs SVG]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/12/27/canvas-vs-svg/"/>
    <updated>2015-12-27T15:53:02-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/12/27/canvas-vs-svg</id>
    <content type="html"><![CDATA[<h4>CensusMapper Behind the Scenes</h4>

<p>The idea behind <a href="https://censusmapper.ca">CensusMapper</a> is that it takes away all the technical barriers to dealing with census data. So how does
CensusMapper work behind the scenes?</p>

<h4>CensusMapper Data Workflow</h4>

<!-- more -->


<p>The general setup is quite simple. We use the lean javascript open mapping platform <a href="http://leafletjs.com">leaflet</a>
as the base for mapping information. Leaflet handles the logic of dealing with zoom and pan and keeping track of the
geographic boundaries that should be mapped. That information gets then passed on to the CensusMapper servers.</p>

<p>CensusMapper will then send the appropriate census geographic polygons to the browser for leaflet to display. Once the
geographic data is available for mapping, some custom code checks what kind of information the user wants to display and
requests the census data required to make the map. The census information is then assembled on the server, sent down
and attached to the polygons and drawn
on the browser window within leaflet. This two-tier process allows the highly dynamic mapping in CensusMapper where the
data-heavy geographic polygons are kept separately thus can be cached and re-used.</p>

<h4>Drawing Census Data</h4>

<p>There are a number of ways how we can display census data in the browser. At CensusMapper we have played with three
different technologies to map data that vary in performance and browser support. They all have in common that they
won&rsquo;t run on Internet Explorer 8 or earlier, but we have just about reached the point in time where it is acceptable to
ignore IE8- in products meant for the &ldquo;general internet audience&rdquo;.</p>

<h5>SVG</h5>

<p><a href="https://en.wikipedia.org/wiki/Scalable_Vector_Graphics">SVG</a> is what our maps have been using so far. SVGs are fairly high level, which means it&rsquo;s very little work
to implement and map information. One simply passes a polygon to the browser, tells it how to color it, and the browser
takes care of the rest. SVG elements can easily be styled via CSS, so there is essentially no work involved to deal with
highlight on hover, scaling for retina displays, patterns for census data quality flags, etc. We use
<a href="https://github.com/mbostock/d3">d3.js</a> to attach the geographic and census data right to the SVG elements for easy
manipulation.</p>

<p>While mapping data this way is very easy, for CensusMapper there are two problems.</p>

<ol>
<li>We are restricted in how we can display information by the capabilities of SVG.</li>
<li>SVG rendering is done by the browser, and not all browsers are equal. Most importantly, SVG rendering in Internet
Explorer is excruciatingly slow. So slow, that CensusMapper becomes essentially useless within Internet Explorer. We
felt compelled to add a warning messeage that displayed when people opened CensusMapper with Internet Explorer. And
when you do that, that&rsquo;s a sure sign that your app has a serious problem.
<img src="http://doodles.mountainmath.ca/images/chrome.png" alt="Browser Warning" />
So how to get around those issues? Enter Canvas.</li>
</ol>


<h5>Canvas</h5>

<p><a href="https://en.wikipedia.org/wiki/Canvas_element">Canvas</a> offers a way to draw images in a browser. Unlike SVG, the drawing has to be done &ldquo;by hand&rdquo;. And the result is just
an image, with no clear way to tell where it came from. There is no way to attach any information to individual
structures drawn on a canvas. All the logic for highlight on hover, figuring out what data is associated with the mouse
position, dealing with retina displays, etc. needs to be added by hand.</p>

<p>On the upside, a good canvas implementation is a lot faster than SVG. And it opens the door to changes in how the data
is handled that bring additional performance improvements. In particular, we can now chop up census polygons and render
the pieces separately, greatly cutting down on the size of the downloaded data, as well as the complexity of the
polygons that get rendered. And the performance improvements are noticeable across all browsers and platforms.</p>

<p>At the end of the day it is actually not that much work and we flipped the switch on this just before the Christmas
break. CensusMapper is now running using canvas instead SVG for
the main maps. We
kept the look and feel the same, so unless you dig into the code you won&rsquo;t notice the difference.
Some parts of CensusMapper still utilized SVGs, like the d3-based
<a href="http://doodles.mountainmath.ca/blog/2015/10/24/census-drilldown/">Census Wheel</a>.</p>

<h5>WebGL</h5>

<p><a href="https://en.wikipedia.org/wiki/WebGL">WebGL</a> also draws on a canvas element, but the work is offloaded onto the GPU (graphics processor) giving enormous
speed improvements. Regular canvas rendering is fast enough for our purposes, but with WebGL we can do more complex
renderings that previously we could not even dream of: <a href="https://en.wikipedia.org/wiki/OpenGL_Shading_Language">Shaders</a>
and Interactive 3D data maps. We had previously
<a href="https://mountainmath.ca/census3">toyed with 3D data visualization</a> to explore Vancouver&rsquo;s household density in 3D using Three.js,
but did not pursue this further because of the complexities of writing code for navigating a Canada-wide map. Then we came
across the super-customizable 3D open mapping platform built by <a href="https://mapzen.com/projects/tangram/">Mapzen</a>, and that
suddenly made it extremely easy to do interactive 3D data mapping live in the browser. A quick test
<a href="https://mountainmath.ca/vancouver_lidar/map">using Vancouver&rsquo;s open LIDAR generated building height data</a> showed how easy Mapzen&rsquo;s
tangram engine is to use.</p>

<p>After digging deeper into tangram, and with help from the friendly people at Mapzen, we figured out a way to fit
CensusMapper&rsquo;s two-stage data workflow into tangram&rsquo;s mapping engine. The result are real-time 3D maps where height
and color of each geographic area can be independently (and dynamically) controlled. Here is an example where mouseover
trigged the area west of Coal Harbor to &lsquo;pop up&rsquo;.
<img src="http://doodles.mountainmath.ca/images/webGL.jpg" alt="webGL" /></p>

<p>At the same time we gain the ability to easily pull in all kinds of other data and map it. On our canvas or svg maps we
added regular image tiles, either a road and label&rsquo;s overlay or a base map (which then requires opacity to be added to
the census data that is mapped on top of that) as orientation aid. Short of baking our own image tiles we have very
little control over the look and feel of this. With Mapzen&rsquo;s tangram we can very easily pick and style individual
geographic objects from Mapzen&rsquo;s OSM vector tile server, resulting in crisp and clear maps. In the above example we
decide dynamically what level of roads to render, how to style them, what labels to display and we also added bodies of
water, where we filter by size depending on the zoom level.</p>

<p>At this stage it is still an ongoing project to get this production-ready. One obvious obstacle is that WebGL browser
support is still lagging. And on top of that it also requires updated graphics card drivers, which is a big problem on
windows machines that are already a couple of years old. So for now we still need to have a plain canvas or svg fallback.</p>

<p>And then there are the details that need to get worked out. 3D maps sounds great, but it will take us some time to figure
out how to best utilize this in thematic maps. But even without utilizing 3D capabilities, the dynamic shaders and increased
rendering performance are already pushing the boundary of what&rsquo;s possible in web maps.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bike Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/12/15/bike-data/"/>
    <updated>2015-12-15T15:16:48-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/12/15/bike-data</id>
    <content type="html"><![CDATA[<p><a href="http://doodles.mountainmath.ca/bike_map2.html"><img  src="http://doodles.mountainmath.ca/images/bike-routes2.png" style="width:25%;float:right"></a>Maps live and die with the quality
of the underlying data. So I decided to dive a little deeper down the OSM bike data rabbit hole. Task number one was
to expand display data for a wider region. My primitive workflow to pull data out of OSM only allows for extracting a quarter
of a degree at a time. For playing around with all of Vancouver&rsquo;s data I again turned to Mapzen for their
<a href="https://mapzen.com/data/metro-extracts">metro extracts</a> as a convenient shortcut for OSM data. For the case of Vancouver
the word &ldquo;metro&rdquo; is a bit of an exaggeration, it just covers the City of Vancouver. But good enough for some more testing.</p>

<h4>More data</h4>

<!-- more -->


<p>More data means a a larger variety of <a href="http://wiki.openstreetmap.org/wiki/Key:cycleway">bike infrastructure types</a>.
At first glance tagging seems to be reasonably accurate, but on closer inspection one quickly spots lots of issues.
First off, the <a href="https://twitter.com/alexwarrior/status/675820327331479552">edits Alex made earlier</a> are <strike>not
showing up, that&rsquo;s because the metro extracts are only done weekly. Just a matter of time.</strike> showing up now after
updating with new OSM data.</p>

<p>And then there are lots of little issues. Off street paths
don&rsquo;t connect to roads (or anything for that matter), making the useless for routing. Some changes in the bike network
are tagged differently, take Point Grey Road as an example. Some paths are labeled to be ok for bikes, but bikes aren&rsquo;t
allowed there. Some tags seem off.</p>

<p>And I don&rsquo;t seem to have captured all relevant bike infrastructure, will have to spend some time one of these days to check
through all the different taggings in OSM to make sure I pull out all the correct ones.</p>

<p>So to make more sense out of the network I made a new map, added some more categories for optionally fading them out when
displayed. And on hover I display the tags, just for interest.</p>

<h4>Improving data</h4>

<p>This gets us to task number two. The folks at Mapzen gave me some friendly pointers to allow easy editing of features.
So I pasted a couple of lines of code in so if you hold down the shift key and
click on a feature, it will take you automatically to OSM to edit the feature. You may have to sign in the first time
you do this. If you hold the shift key while clicking anywhere else it also takes you to the OSM editor for that spot,
in case you want to add something there. It takes a lot of the pain out of editing bike infrastructure, I just fixed a
whole bunch of things in short time.</p>

<iframe src="http://doodles.mountainmath.ca/bike_map2.html" width="100%" height="550"></iframe>


<p>Go for it and fix some problems that you see, either by shift-clicking on the embedded map or by
<a href="http://doodles.mountainmath.ca/bike_map2.html">taking the map full-screen first</a>. Remember that some of the issues may be fixed already, the bike map will not reflect
updates until a week later or so.</p>

<h4>Where to go from here?</h4>

<p>It looks like making a decent bike map with OSM data is feasible. The hard work will be to collaboratively do all the
OSM edits required to get the data into good shape.</p>

<p>One problem is that edits won&rsquo;t show up on the bike map for up to a week, that&rsquo;s the frequency at which the metro extracts
are updated. And then I have to update the file for displaying the bike data. A minor inconvenience, in theory there
are ways to seed this up if one wants to be serious about this. But then again, once the OSM bike data is fixed up in a
given region, it won&rsquo;t need updating very frequently.</p>

<p>I guess I will have to mull this over and decide how deep down this rabbit hole I would like to go. Looking back at my
<a href="http://doodles.mountainmath.ca/blog/2015/12/13/how-to-make-a-bike-map/">rant on what&rsquo;s wrong with most bike maps</a> I am
asking myself how much the current map has accomplished.</p>

<ol>
<li>The accuracy of the infrastructure mapped is in the hands of the OSM community. With easy access to the OSM editing
functionality from the map (desktop only) it&rsquo;s in the hands of the people that know best: cyclists.</li>
<li>Still coming up blank when it comes to showing comfort level. Will have to think more how to best do this, it might
have to go into a separate database. Which is doable, but requires work to get it right and is probably a little too
involved to do it off the side.</li>
<li>Click and zoom is not an issue, but now I am in the opposite corner where it might be nice sometimes to actually have
a glossy big paper map. And it&rsquo;s really easy to save the map as an image. Some tinkering should be able to produce a nice
high-resolution one covering a large area.</li>
<li>The map already adapts to user preferences quite well, different types of infrastructure will fade out if deselected.</li>
<li>Not on the original list, but routing should be part of this. Routing works reasonably well to get this off the ground,
but work is still required to make it work properly with user preferences. This may well require running a custom router,
again a little more than I bargained for.</li>
</ol>


<p>So is this really worth the trouble to make yet another bike map? I am not sure. It&rsquo;s always easy to through up a quick
proof of concept, but to do it really well requires quite a bit more work. For now I could just build one focused on
Vancouver and see how it goes. And it would be great if the comfort level rating could somehow be automated.</p>

<p><img  src="http://doodles.mountainmath.ca/images/cycletrack.PNG" style="width:50%;float:right">Which brings to another one of pet projects that
I never took beyond the testing realm. My CycleTrack App that records
all bike trips in the background (as long as you carry your phone with you). No pressing of &ldquo;start&rdquo; or &ldquo;stop&rdquo; buttons,
the app notices when you are moving and will take gyroscope and accelerometer readings to figure out if you are cycling,
running, walking, driving or taking a train. Then it stores your cycling trips and computes aggregate data. It avoids
using GPS so not to drain your battery too much, on a typical day it will consume about 3% of battery power. The downside
is that the accuracy and frequency of the location updates is not as high, so things get a little messy. But not too bad.</p>

<p>How does this fit into the bike map project? Simple. If one can collect regular cycling data from normal people
cycling (not just
the lycra crowd that presses start cycles in circles and presses stop again and recharges their phone while taking a shower),
once can infer a lot about comfort levels just by looking at the data. And to collect data from regular &ldquo;citizen cyclist&rdquo;
one cannot expect them to press &ldquo;start&rdquo; and &ldquo;stop&rdquo; to delineate their bike trips, and one cannot have an app that will
require them to recharge their phone after every trip.</p>

<p>But then the project gets even bigger&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bike Routing]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/12/14/routing/"/>
    <updated>2015-12-14T20:10:16-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/12/14/routing</id>
    <content type="html"><![CDATA[<p><a href="http://doodles.mountainmath.ca/bike_routing.html"><img  src="http://doodles.mountainmath.ca/images/routing.png" style="width:25%;float:right"></a>Routing is a hard problem. Routing for drivers is pretty good at this point, mostly because we have been very good at
designing for cars and creating predicable infrastructure. Routing for bikes is a whole other story, data quality is
poor and the physical infrastructure is, at least in North America, not strongly predictive of cycling comfort/safety.
And cycling comfort/safety is the top priority for the
<a href="http://usa.streetsblog.org/2015/03/13/the-first-ever-nationwide-survey-of-interested-but-concerned-bikers-is-here/">vast majority of (potential) cyclists</a>.</p>

<p>And it&rsquo;s the ones that don&rsquo;t cycle frequently, often out of concern for safety, that would benefit most from effective
bike routing.</p>

<p>Read on or <a href="http://doodles.mountainmath.ca/bike_routing.html">go directly to the routing demo</a>.</p>

<!-- more -->


<p>Google does a decent job directing a relatively experienced cyclist from A to B, but it has a hard time to learn about
places where cyclists can go but cars can&rsquo;t. And it won&rsquo;t be able to answer my fundamental question: <em>Can I bring my 6
year old along?</em>. And Apple doesn&rsquo;t even try and offer bike routing.</p>

<h4>Routing test</h4>

<p>So what&rsquo;s really needed apart from <a href="http://doodles.mountainmath.ca/blog/2015/12/13/how-to-make-a-bike-map/">better bike maps</a>
is better bike routing. So building on yesterday&rsquo;s post, I decided to take a quick look at routing. Time to try out
<a href="https://mapzen.com/projects/valhalla">Mapzen&rsquo;s routing engine</a> which, as expected, was really easy to set up:</p>

<p>Feel free to drag the endpoints to test your favorite routes.</p>

<iframe src="http://doodles.mountainmath.ca/bike_routing.html" width="100%" height="550"></iframe>


<p><a href="http://doodles.mountainmath.ca/bike_routing.html">Full screen view</a></p>

<p>Initial testing seems to indicate that this works reasonably well. And while the engine allows for some customization
on rider needs, right now there is no way to get the &ldquo;dad&rsquo;s routing&rdquo; that I would like to have.</p>

<p>Part of the problem is of course that I still don&rsquo;t have enough information in OSM to even make a &ldquo;dad&rsquo;s map&rdquo;
<a href="http://doodles.mountainmath.ca/blog/2015/12/13/how-to-make-a-bike-map/">as I lamented earlier</a>. But at least OSM gets
me half-way there by giving me a finer control over distinguishing infrastructure that I may deem as generally more
suitable so that I can fade selected bike infrastructure out by checking the appropriate boxes in the map.</p>

<h4>Route costing options</h4>

<p>The bike routing options in Mapzen&rsquo;s routing engine allow for some level of control on wheter gravel should be avoided
(great feature for the lycra cowed but useless for dads), whether hill should be avoided (helpful) and whether to avoid
roads without bike infrastructure. But when I cycle with my 6 year old a bike lane between parked cars and 50 traffic
is as good as no bike infrastructure at all. And there is currently no way to cost different types of bike
infrastructure, so they can&rsquo;t be used as a proxy for cycling comfort.</p>

<h4>Where to go from here</h4>

<p>Next steps are to <a href="https://github.com/valhalla/thor">look deeper into Mapzen&rsquo;s routing engine</a> and see how hard it would
be to hack some of these more advanced costing options into their routing engine and open up a feature request.</p>

<p>Wrapping up the three-night trials in bike mapping is the
<a href="http://doodles.mountainmath.ca/blog/2015/12/15/bike-data/">post on data</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to make a bike map]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/12/13/how-to-make-a-bike-map/"/>
    <updated>2015-12-13T12:04:25-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/12/13/how-to-make-a-bike-map</id>
    <content type="html"><![CDATA[<p><a href="http://doodles.mountainmath.ca/bike_map.html"><img  src="http://doodles.mountainmath.ca/images/bike-map.png" style="width:25%;float:right"></a>Here come some general thoughts on bike maps. Not throught through yet, just jotting down some ideas so that I don&rsquo;t
forget and maybe to start a discussion.</p>

<p>Bike enthusiasts, OSM folks and mapping technology wonks read on!</p>

<!-- more -->


<h4>Why make a new bike map?</h4>

<p>Plainly put, I don&rsquo;t like a lot of the bike maps out there. Time for my little bike map rant:</p>

<ol>
<li>Lots of maps don&rsquo;t accomplish their core mission:
accurately map bike infrastructure. For example, when in real life a bike lane vanishes 100m in front of an intersection
and reappears 100m after the intersection, leaving cyclists exposed right where protection is needed the most, most
bike maps will mark the entire section as having a bike lane.</li>
<li>Most bike maps focus on the wrong issue. They focus on physical infrastructure, as a cyclist I am interested in
cycling comfort level. Can I take my 6 year old along that route? Can I cycle leisurely or will I feel hastened by a
car breathing down my neck? Will I arrive at my destination relaxed or will I be riled up by 3 near misses and 5 drivers
swearing at me? Some of these questions can be answered by mapping infrastructure. A physically separated bike path will
give me the comfort I need to bing my 6 year old and enjoy my ride. But when it comes to &ldquo;bike lane&rdquo; or &ldquo;shared road&rdquo;
designations, I have no idea what it will feel like until I get there.</li>
<li>Most bike maps have not yet made the transition from paper/PDF based maps to clickable and zoomable maps that can be
easily consumed on the go.</li>
<li>Most maps are static, they don&rsquo;t adapt to the needs of the user. Why do I have to deal with 5 different colors and
3 types of shading when reading a bike map, when all I want to know is if I can bring my 6 year old along for the ride.
I want an &ldquo;AAA&rdquo; button that fades out all routes that are not suitable for all ages and abilities. For that purpose I
don&rsquo;t care if a road has infrastructure like a bike lane wedged between parked cars and 50km/h traffic. If we absolutely
have to ride that way it will be on the sidewalk.</li>
</ol>


<h4>How to make a new bike map?</h4>

<p>First let&rsquo;s start with examples of how other have tried to solve these problems. Probably one of the best maps out there
is the <a href="https://www.crd.bc.ca/docs/default-source/crd-document-library/maps/transportation/bikemap2014-frontback-web.pdf?sfvrsn=8">Capital Regional District Bike Map</a>.
I chatted with <a href="https://twitter.com/Burgundavia">Corey</a> who worked for CRD on these maps on how it was done. They collected all the bike infrastructure
in the region by contacting all the municipalities. <img  src="http://doodles.mountainmath.ca/images/victoria.png" style="width:50%;float:left"> Then they meticulously
checked that what the municipalities reported actually existed on the ground.  Consider Fort road on their bike map where they accurately show the bike lane
cutting out. And they added in shortcuts and connections
that the municipalities did not report on, like a short section of footway that may technically require walking the bike
but shortcuts out of a suburban cul-de-sac maze.</p>

<p>Next the CRD map does not only map infrastructure, but color-codes it to show cycling comfort. <img  src="http://doodles.mountainmath.ca/images/victoria-legend.png" style="width:50%;float:right">
This gives me a good idea how comfortable the rider will feel. Unfortunately all that complexity starts to make it difficult to
read the bike map.</p>

<p>That&rsquo;s where technology could come to the rescure. The CRD map is a PDF map and suffers the usual limitations that come
with it. The map cannot adapt to the needs of the reader. So how about moving the map online and letting the user decide
how to display it?</p>

<h4>Bike maps that adapt to the user</h4>

<p>The <a href="http://www.washingtonpost.com/blogs/wonkblog/wp/2015/04/01/bleak-maps-of-how-cities-look-using-only-their-bike-lanes/">Washington Post Wonkblog</a>
had an excellent feature mapping only the bike infrastructure, nothing else. And for good measure, they removed the
shared streets bike infrastructure. The rational was that in most cases, comfort level on shared street bike routes is really no different from
cycling on a parallel road. Inspired by that I decided <a href="http://doodles.mountainmath.ca/blog/2015/04/01/bike-paths/">map Vancouver and a couple of other cities</a>
and I added in some button to allow selectively turning on or off typed of infrastructure.</p>

<iframe src="http://doodles.mountainmath.ca/bike_paths.html?fh=50&nh=true" width="50%" height="300" style="float:left;"></iframe>


<p>That&rsquo;s getting one step closer to adaptable bike maps, but it is still missing the main point that as a cyclist I am
most interested in comfort level, and infrastructure type is only a proxy for that. And a poor one in many cases.</p>

<h4>Inspiration</h4>

<p>Not convinced we need a new bike map? Let&rsquo;s draw on some local (for me) maps that inspired this effort.</p>

<p>UBC Campus Planning <a href="https://twitter.com/ubc_candcp/status/662027568204357633">keeps tweeting out a bike map</a>
that lists several dangerous roads as &ldquo;designated on street cycling routes&rdquo;, including an unlit divided 4 lane highway
with a posted speed limit of 60km/h and typical traffic speeds significantly exceeding that. And all of that apparently
aimed at people that don&rsquo;t cycle to campus yet but are considering doing so.</p>

<p>TransLink just <a href="http://www.translink.ca/-/media/Documents/cycling/cycling_routes/full_maps/TransLink%20Regional%20Cycling%20Map%20West.pdf">updated their bike map</a>
and marks said highway as &ldquo;recommended by cyclists&rdquo;. The do acknowlege that the road &ldquo;does not have special treatment
for cyclists&rdquo;. So why include it on the map? Filling in the the gaps in the cycling network should happen in real life,
not just on the map.</p>

<p>Even HUB keeps <a href="https://bikehub.ca/about-us/our-positions/ungapthemap">marking Wesbrook Mall north of Agronomy as &ldquo;existing cycling route&rdquo;</a>
when in real life it is everything but that.</p>

<h4>How to make a bike map</h4>

<p>So how should we make a bike map? I was always thinking about setting up my own database and somehow adding and rating
infrastructure. Then <a href="https://twitter.com/alexwarrior">Alex</a> decided to start making a bike
map for the UBC area. And he chose the most straight-forward path: Editing Open Street Map data. That way the edits are
immediately (better: after their map tile refresh cycle) available online, for example on
<a href="http://www.opencyclemap.org/">OpenCycleMaps</a>. The map is easily accessible
on the go, it zooms and scrolls. It is missing a &lsquo;locate me&rsquo; button (which is easy to fix). The look and feel is a little
dated. But most importantly, it does not adapt to the user&rsquo;s needs. I can&rsquo;t ask it to display the &ldquo;dad&rsquo;s version&rdquo; of the
cycle network, showing me only pieces that I will be comfortable cycling with my 6 year old.</p>

<p>Technology has moved along since that map was built. A very easy way to solve these issues is to
utilized the awesome tools built by the folks at <a href="https://mapzen.com">Mapzen</a>. Their
<a href="https://mapzen.com/projects/tangram">tangram mapping engine</a> taps into their
<a href="https://mapzen.com/projects/vector-tiles">OSM vector tile service</a> to make flexible mapping of bike data a breeze. And
to top things off, they offer very easy to use and extremely powerful ways to style the map. Only problem: Mapzen&rsquo;s
OSM extracts don&rsquo;t have cycling information. Not a big problem though, we can just pull them out separately and add
them on by hand. Here is an example where only the bike routes near UBC are added.</p>

<iframe src="http://doodles.mountainmath.ca/bike_map.html" width="100%" height="550"></iframe>


<p><a href="http://doodles.mountainmath.ca/bike_map.html">Full screen view</a></p>

<p>One small drawback is that WebGL, the technology Tangram is based on, is not available for much of the windows world.
WebGL requires modern browsers (IE9 does not count) and also modern hardware/graphics card drivers. A couple of years
old windows machine will likely not be able to render WebGL no matter what browser you use. But the main target is
mobile, and iOS and most android won&rsquo;t have a problem with WebGL. If really needed, could add a fallback or use older
and web technology to make the map, but Mapzen&rsquo;s Tangram makes it so ridiculously easy to make and style nice maps&hellip;</p>

<h4>The main problem left to solve</h4>

<p>One major problem left. OSM does not have and &ldquo;cycling comfort&rdquo; tags right now. There are tags for physical infrastructure,
and in some cases the comfort level can be correctly inferred from those. But in many cases it can&rsquo;t.</p>

<p>There are two ways around that. One could
keep those tags in a separate file, but that becomes difficult to maintain when OSM features change. Or one can add the
tag to the OSM data. That way better bike maps can scale easily, and the information can also be used in OSM-based
routing services where their real value lies.</p>

<p>Not sure which is the way to go here, a similar tag
<a href="http://wiki.openstreetmap.org/wiki/Proposed_features/bike_safety">has been proposed before</a> but apparently did not
go anywhere. Looking at the <a href="http://wiki.openstreetmap.org/wiki/Any_tags_you_like">tag guidelines</a> the fact they these
new tags are very useful for routing seems to speak for adding them to OSM, but the fact that they are measuring
something explicitly non-geographic by going beyond the already existing physical infrastructure markers might mean
they are better kept in an external database. Will have to think this over, feedback appreciated.</p>

<h4>Where to go from here.</h4>

<p>The logical next step is to add routing. Again, Mapzen&rsquo;s <a href="https://mapzen.com/projects/valhalla/">flexible routing service</a>
seems to be a natural match. Also, adding editing capabilities right onto the map would be quite useful. The bike map is
where data issues are best seen, and saving the step to head over to OSM to fix it (after creating an account) and then
waiting for the data to update on the map seems like a workflow that will discourage wide participation.</p>

<p>We follow up by <a href="http://doodles.mountainmath.ca/blog/2015/12/14/routing/">exploring routing</a>
and <a href="http://doodles.mountainmath.ca/blog/2015/12/15/bike-data/">data quality</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Census Drilldown]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/10/24/census-drilldown/"/>
    <updated>2015-10-24T20:45:16-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/10/24/census-drilldown</id>
    <content type="html"><![CDATA[<h4>Next steps in CensusMapper</h4>

<p>The <a href="http://censusmapper.ca">Census Mapper Project</a> is moving along slowly, public beta unearthed some bugs and we gathered
feedback (thanks to everyone reporting back!). There are still a couple of steps that need to be taken care of before
we can unleash the full map making power to all users. We feel that the complexity of census data requires more guidance
than the current map making system is providing. Anyone who does not mind getting there hands dirty and having to look
up census variable definitions by themselves when making maps is welcome to contact us and we will hook you up with a
beta mapmaking account.</p>

<p>In the meantime we added one important feature to the CensusMapper.</p>

<h5>Content Drilldown</h5>

<p>CensusMapper is a great way to explore single census variables (or a single function built out of census variables)
across many geographic regions and aggregation levels. But sometimes we would like to do the opposite: Drill down into
a specific census region and explore other census variables. We have now added an easy way to do this. To access it
simply select the &ldquo;more&rdquo; button in the basic popup when you select a census region. This brings up the <em>census wheel</em>,
which is our way to navigate through census data.
<img src="http://doodles.mountainmath.ca/images/wheel.png" alt="Census Wheel" /></p>

<p>Try it out right away on <a href="http://censusmapper.ca">CensusMapper</a> or read on for details on how this works.</p>

<!-- more -->


<p>There are almost 4,000 census variables available, right now we do not offer to split up by gender, which reduces the
available variables to 1,429. To further simplify things we throw out all variables with zero values for the give
geographic area, still leaving a
sizeable number of variables to browse through.
<img src="http://doodles.mountainmath.ca/images/mother_tounge.png" alt="Mother Tounge" style="max-width:400px;margin-right:15px;margin-top:10px;" align="left"/>
Each arc in the census wheel represents a variable, or a category of
variables. Selecting an arc will zoom into that arc and turn it into the &ldquo;center&rdquo; of the wheel, collapsing all other
components. That&rsquo;s the content <em>drilldown</em> process. Once it makes sense to display data as proportions, we switch to the
<em>proportional view</em> which shows the data as hierarchical pie chart.</p>

<p>This gives a visual representation of the proportions of each of the variables. Hovering (or touching) an arc will
display more detailed information, selecting one will drill down further. To reverse that process either select the
center or us the <em>content breadcrumbs</em> at the top that were created during the drilldown process.</p>

<h5>Data Problems</h5>

<p>Census data is messy. Now that all census data for each region is generally accessible in CensusMapper we need to
explain some of the inherent data problems.</p>

<h6>Rounding</h6>

<p>Census Canada will <a href="http://www12.statcan.gc.ca/census-recensement/2011/dp-pd/prof/help-aide/N2.cfm?Lang=E">round</a>
(almost) all data to preserve anonymity and don&rsquo;t create false impressions of accuracy that
the data does not achieve. Data is generally reported in increments of 5, rounding includes randomness to preserve
anonymity. The value of the measured value
is <a href="https://www12.statcan.gc.ca/census-recensement/2011/ref/DQ-QD/conf-eng.cfm">within 4</a> of the reported one. And
remember that even the measured variable is only an estimate of the actual value of the variable.
Rounding may lead to situations where, for example, the sum of all people listed
by age bracket will not add up to the total number of people. Generally, this difference will be small and we ignore it
in our visualization.</p>

<h6>Omitted Data</h6>

<p>Census Canada will at times not report data. This could be due to very low return rates or other problems that make data
so unreliable that it is better not reported at all. Or it could be that releasing the information could compromize
the anonymity of the census data for some people in that area. The latter can take
the form of Census Canada not reporting any data for the region, or Census Canada zeroing out specific variables that
&ldquo;are too low to be reported&rdquo;.
<img src="http://doodles.mountainmath.ca/images/unaccounted.png" alt="Unaccounted" style="max-width:400px;margin-left:15px;margin-top:10px;" align="right"/>
We have not been able to find clear guidelines how the &ldquo;zeroing&rdquo; works, but often this will leave
detectable traces in the data. Looking at the example in the image, looking at &ldquo;Mode of Transport&rdquo; to work only
&ldquo;Driving&rdquo; has non-zero values, the
other options &ldquo;Passenger&rdquo;, &ldquo;Transit&rdquo;, &ldquo;Bike&rdquo;, &ldquo;Walk&rdquo; and &ldquo;Other&rdquo; are all zero. There were 160 people getting to work,
115 are listed as &ldquo;Driver&rdquo;, leaving 45 unaccounted for. This is outside of the range that could be explained by the
rounding of variables. We alert the user by adding in
a grey area for the missing 45. This also ensures that the visual representation remains accurate.</p>

<h6>Multiple Responses</h6>

<p>Some census questions allow for multiple responses. For exaple &ldquo;Language Spoken Most Often At Home&rdquo;. In this particular
case the census variable breaks out single responses and multiple responses and is very transparent to the user. In
other cases, for example &ldquo;Ethnicity&rdquo;, single and multiple responses are not reported separately but responses are all
added up. This leads to the sum of lower level variables being higher than the base variable. We alert the user to this
by overlaying small white dots on the base variable.
<img src="http://doodles.mountainmath.ca/images/multiple_responses.png" alt="Multiple Responses" style="max-width:400px;margin-right:15px;margin-top:10px;" align="left"/>
In this particular case the total for for &ldquo;Ethnic Origin&rdquo; was 12,140 people. But there were 1,430 more responses at the
next level, so up to 1,430 people had given multiple responses to this question listing more than one of the aggregate
(mostly continent level) origins, some possibly listing more than two. The same patter repeats at different ethnic
origin aggregation
levels, for example 2,565 people claimed at least one of the &ldquo;British Island origins&rdquo;, but many listed more than one
resulting in the sum of the individual regions with the British Islands exceeding the British Island count by 1,445.
Again, we alert the user by overlaying dots over the &ldquo;British Island origins&rdquo; arc. Hovering over the arg will display
the exact numbers of the &ldquo;overcounting&rdquo; due to multiple responses.</p>

<p>In these cases where mulitple repsonses are not broken out the dots will aler the user that the proportional
representation in the hierarchical pie chart does not represent proportions out of a total given by the value of the
variable at the centre (or lower level), but as a proportion of all responses which exceeds the value of the
lower level variable.</p>

<h6>Basic Census</h6>

<p>The Basic Census is generally speaking quite reliable, every single person is required to fill it
out and return rates are generally above 95%. Serious problems will only occur if response rates are very low. We alert
the user by shading geographic regions is this has been the case.</p>

<h6>NHS</h6>

<p>The National Household Survey is quite different in nature, it was only
sent out to a smaller portion (~30%) of society and return rates were much lower (~69%). Even with 100% return rates
there are likely to be geographic regions where the results severely misrepresent reality in that region due to sampling
bias. For each region that bias is small, but the probability for bias grows as the number of people in the geographic
region declines. So this is mostly a problem for Dissemination Areas. But even there, the probability of severe sampling
bias in each region is small, but there are many regions and the probability that some of these regions suffer from
sever sampling bias is quite large.</p>

<p>On top of this basic statistical sampling bias, we also have self selection bias due to some deomgraphics being more
likely to return the survey than others. This bias is a product of the decision of scrapping the madatory
&lsquo;long form census&rsquo; and replacing it with the voluntary NHS. The return rates can give some indication of the
likelyhood of self-selection
bias, we shade regions with a non-return rate lower than 50%, the cutoff Census Canada set for reliability of the NHS.
It is especially problematic when trying to detect change in variables (for example poverty) from one census to another
as the differences in the variable over time are often small and similar in magnitude to possible self-selection bias.</p>

<p>The 50% cutoff we highlight in CensusMapper is just a guideline, the exact return rates are displayed on hover or when
selecting regions and should always to be taken into
account when interpreting results,
especially at the Dissemination Area level.</p>

<p>If all this information did not turn you off, head over to <a href="http://censusmapper.ca">CensusMapper</a> and drill down into
some geographic areas.</p>
]]></content>
  </entry>
  
</feed>
