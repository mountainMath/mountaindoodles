<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Mountain Doodles]]></title>
  <link href="http://doodles.mountainmath.ca/feed.xml" rel="self"/>
  <link href="http://doodles.mountainmath.ca/"/>
  <updated>2016-01-20T09:25:19-08:00</updated>
  <id>http://doodles.mountainmath.ca/</id>
  <author>
    <name><![CDATA[MountainMath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On Teardowns]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment/"/>
    <updated>2016-01-18T09:52:33-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/01/18/redevelopment</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/special/17"><img  src="http://doodles.mountainmath.ca/images/teardowns.png" style="width:50%;float:left;margin-right:10px;"></a>
On the heels of the <a href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/">new assessment data</a> we
can start to slice the data in different ways to understand various aspects of the real estate landscape in Vancouver. The
fact that <a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data</a> makes historic data available
gives the ability to look for changes over time.</p>

<p><a href="https://mountainmath.ca/map/assessment">Our maps</a> explore this by visualizing some aspects of these changes for all
properties, but it might also be useful to filter the properties we show to focus in on specific criteria.</p>

<p>&ldquo;Teardowns&rdquo; always triggers lots of emotions in Vancouver. Without looking at the eomtional side and trying to avoid any
judgement we will
investigate the data to undestand what buildings have been torn down recently and predict which buildings will get torn
down next. And map them. Long story short, we predict that
1 in 6 buildings <a href="https://mountainmath.ca/map/special/17">on this map</a> (and then some more with lower teardown probability)
will get torn down and rebuilt by 2026.</p>

<h3>Building age temporal distribution</h3>

<!-- more -->


<p>To start understanding teardowns and rebuilds let&rsquo;s look at the <a href="https://mountainmath.ca/map/assessment?layer=7">age of the building stock</a>.</p>

<p>To get a better overview of the building stock through time we can graph the number of buildings by age. We look at buildings, not units. So a stratified
building with 100 units would still count as one building in our graph. And it is not looking at how many buildings were
built in each year, but how many buildings that were built in a given year are still standing today.</p>

<p>We still have 7 buildings in Vancouver that were built before 1900 (the earliest from 1800). Skipping these we graph the
rest to get:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_age" style="height:200px;max-width:640px;" data-url="/data/building_age.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of Buildings by Building Age <span style="float:right;margin-right:10px;" id="building_age_value"></span></p>
</div>
</div>


<p>Starting with 1950 the distribution of buildings by age is quite uniform, with a short peak around the early 1990s.</p>

<p>The dip at the end is due to some lag in new buildings showing up in the property dataset. Looking at the more recent history
it is safe to assume that the number of buildings still standing corresponds well to the buildings of units built in that year.
So the pace of new buildings right now seems to fit in quite well with the recent history and is a little lower than the
peak in the early 1990s.</p>

<h3>Recent Building stock (and recent teardowns) spatial distribution</h3>

<p><a href="https://mountainmath.ca/map/special/15"><img  src="http://doodles.mountainmath.ca/images/rebuilds.png" style="width:50%;float:right"></a>
The next question is to focus in at the spatial distribution of recent redevelopment by filtering out older buildings.
Being too lazy to add a bush for dynamic selection of time ranges I just made a static (in time) view only showing the
<a href="https://mountainmath.ca/map/special/15?zoom=13">6883 properties built after 2006</a>. It is quite safe to assume that most
of those new buildings replaced older ones that were torn down. So this map of new buildings is also a map of locations
of buildings that were torn down in the last 10 years.</p>

<p>What&rsquo;s interesting is when selecting
<a href="https://mountainmath.ca/map/special/15?zoom=16&amp;lat=49.2341&amp;lng=-123.1824&amp;layer=4">relative building value view</a> that
there are some properties that have been recently re-developed with increadibly low building value, like the property
at 5649 Dunbar St. This gives a window into some of the imperfections of the BC Assessment process where the building
value after re-development is not properly reflected in their dataset.</p>

<p>It also shows that recent building (or teardown) activity is fairly uniform across the city, with only some areas standing out as having little
development like the West End, parts of Kitsilano and Strathcona.</p>

<h3>What gets torn down and rebuilt next?</h3>

<p>The big question is of course where new buildings get built next. In a built up space like Vancouver there are few sites
left where building a new building does not mean tearing down an old one. So another way to ask that question is: What
gets torn down next?</p>

<h3>Teardown Probability</h3>

<p>Predicting which building will get torn down next is of course impossible. So what we try to do is assign a &ldquo;teardown
probability&rdquo; to each building.</p>

<p>Let&rsquo;s first try to understand why a particular building might get torn down as opposed to the one next door. Typically
buildings get torn down at the time when they change ownership. So if a building is not sold, it is far less likely to
get torn down. So what makes a building more likely to get torn down when it is sold? One hypothesis would be that the
value of the building relative to the land should play an important factor. Let&rsquo;s test this hypothesis using the data.</p>

<p>We take the 2006 tax dataset as a baseline and check how many of the buildings have been torn down by 2016. Refer to the
Methodology and Data section at the botton for the messy details. We only
count buildings, so we count a strata lot with 100 units in the same building as one building. Then we use the 2016 dataset
to check how many of them are still around, identifying them by their tax coordinate and again asking they be marked as
being built no later than 2006.</p>

<p>These criteria capture well what we are looking for, but they are not perfect. As a predictive variable we use the</p>

<h5>Teardown Coefficient</h5>

<p>The <em>teardown coefficient</em> is the percentage of the total assessed value that is attributed to the building. More formally
it&rsquo;s the ratio of the building value by the sum of the building and land values.</p>

<p>So we sort the properties by their <em>teardown coefficient</em> using the
2006 tax assessment data and we check how each group fares.</p>

<p>First up a graph of the distribution of buildigs in 2006 by their <em>teardown coefficient</em>.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_buildings_by_teardown" style="height:200px;max-width:640px;" data-url="/data/buildings_by_teardown.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of Buildings by *teardown coefficient* <span style="float:right;margin-right:10px;" id="buildings_by_teardown_value"></span></p>
</div>
</div>


<p>Next up the number of buildings in each category that got torn down:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_teardowns" style="height:200px;max-width:640px;" data-url="/data/teardowns.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Number of Torn Down Buildings by *teardown coefficient* <span style="float:right;margin-right:10px;" id="teardowns_value"></span></p>
</div>
</div>


<p>We see that our initial hypothesis seems to hold up quite well. The number of buildings that got torn down and rebuilt
decreases as the <em>teardown coefficient</em> increases. Remember that we defined the <em>teardown coefficient</em> to be the percentage
of the building value out of the total value of the property.</p>

<p>Refer to the methodology and data section for further information on how these numbers were extracted.</p>

<p>To explore this further let&rsquo;s graph the frequency with which a building in a given <em>teardown coefficeint</em> range gets torn down.
To keep things cleaner where we only plot up to a <em>teardown coefficient</em> of 50%:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_teardown_probability" style="height:200px;max-width:640px;" data-url="/data/teardown_probability.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Probability of Building being torn down<span style="float:right;margin-right:10px;" id="teardown_probability_value"></span></p>
</div>
</div>


<p>We see that the <em>teardown coefficient</em> has high predictive value for a building to be torn down and
being rebuilt in the following 10 years. Buildings with a <em>teardown coefficient</em> below 5% have about an 18% chance, and the
probability declines exponentially down to zero at a <em>teardown coefficient</em> of about 50%.</p>

<p>If we were more serious about this
we would fit and exponential curve to the data and compute how well it fits the data, repeat the computation for
other time frames, run it on individual neighbourhoods and maybe also on <a href="https://data.surrey.ca">data from other municipalities</a>
to properly validate our model. We could also refine the model by refining our filters, see the methodology and data section for
more details.</p>

<p>And we could add other factors that likely effect the teardown probability, like building age, proximity to arterials and
others. Of course these are not independent factors, so this kind of analysis requires more care.</p>

<h3>Predicting Teardowns</h3>

<p>Now to the main part: Predicting teardowns. How many buildings will get torn down and rebuilt in the next 10 years? Let&rsquo;s
use what we have just learned to extrapolate.</p>

<p>First up the graph of the 2016 building stock by <em>teardown coefficient</em>:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_buildings_by_teardown_2" style="height:200px;max-width:640px;" data-url="/data/buildings_by_teardown_2.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> 2016 Building stock by *teardown coefficient* <span style="float:right;margin-right:10px;" id="buildings_by_teardown_2_value"></span></p>
</div>
</div>


<p>To estimate how many buildings will get torn down and rebuilt in each category we simply multiply each bin with the teardown probability
from the frequency graph above:</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph_teardowns_2" style="height:200px;max-width:640px;" data-url="/data/teardowns_2.json"></div>
<div class="legend no-margin">
  <p><i style="background:steelblue"></i> Estimate of Buildings rebuilt by 2026 <span style="float:right;margin-right:10px;" id="teardowns_2_value"></span></p>
</div>
</div>


<p>Bottom line, we predict around 8,000 buildings to be torn down and rebuilt by 2026. That&rsquo;s significantly more than the
around 5,900 buildings that we identified as going through this process during the prior 10 years.</p>

<h3>Open Question</h3>

<p>There are lots of assumptions that went into this estimate. While we are confident in our analysis
that properties with low <em>teardown coefficient</em> are the ones most likely to be torn down, it is less clear if the number
of properties being torn down grows linearly as the properties with low <em>teardown coefficient</em> grow. In our case the number
of properties with <em>teardown coefficient</em> below 5% grew from 20492 (21% of the 2006 stock) to 32509 (33.5% of the 2016) stock,
which may be out of the range where our simplistic extrapolation holds. One could try to understand this by carefully
analyzing all available tax years, and not just the two extremes of the available spectrum.</p>

<h3>Mapping Teardowns</h3>

<p><a href="https://mountainmath.ca/map/special/17"><img  src="http://doodles.mountainmath.ca/images/teardowns.png" style="width:50%;float:right"></a>Now that we
understand how to assign a teardown probability to buildings, let&rsquo;s map them! To keep things as simple as
possible let&rsquo;s focus in on the homes with a <em>teardown coefficient</em> below 5%. They make up the bulk in our prediction and
have the simple interpretation that a little more than 1 in 6 of these will get replaced by something else by 2026. So
<a href="https://mountainmath.ca/map/special/17">here is the interactive map</a> of just these 31301 buildings. And this only accounts
for the 5,700 buildings predicted to be torn down with a <em>teardown coefficient</em> below 5% cutoff and neglects the roughly 2,000 more that
are predicted to be torn down that have a <em>teardown coefficient</em> above 5%.</p>

<h3>Methodology and Data</h3>

<p>Only for people who love getting their hands dirty or who want to reproduce or expand on the analysis.</p>

<p>First thing to note is that there is no way to detect &ldquo;teardowns&rdquo; in the dataset, the only way is to look at what has been
rebuilt and what has &lsquo;dropped off&rsquo;. To be more precise, there data fields to look at is the &ldquo;land coordinate&rdquo;, which links
a taxable property to a physical structure, and the &ldquo;year built&rdquo;. And both fields have problems.</p>

<p>The &ldquo;land coordinate&rdquo;
gets de-commissioned and re-assigned during certain re-develpments. And the city dataset provides no way to link the
old one to the new one. One way to do that is through the polygons that mark the property boundaries, that would allow
tracking of complex re-assemblies of land. But the city does not publish historic records of property polygons.</p>

<p>The &ldquo;year built&rdquo; also has lots of issues. Sometimes it is blank even though it records the value of the building as
greater than zero. Sometimes the &ldquo;year built&rdquo; will be set to a date later than the date of the dataset, for example the
2006 tax dataset has buildings with &ldquo;year built&rdquo; all they way up to 2013.</p>

<p>Then comes the issue of filtering. We decided to filter out parks, rail lines and marinas without structures on them. The
algorithm is somewhat simplistic, it&rsquo;s the same one that was used to filter properties for the maps. Additionally we
filter out properties from the heritage dataset. There is definitely
room for improvement here, but without a clear question of what exactly to measure
(only single family homes, or also condos or apartements, treat commercial separately, &hellip;)
it does not make much sense to invest energy into this. After all, this is just looking for a rough model.</p>

<p>So how do we detect rebuilds? We take the land coordinates from properties identified as park or heritage and sieve through
the 2006 tax data to retrieve all records that don&rsquo;t match these land coordinates and have a &ldquo;year built&rdquo; column set
as 2006 or earlier or don&rsquo;t have a &ldquo;year built&rdquo; set at all but change from zero to non-zero building value from 2006 to 2016.</p>

<p>Pretty messy. We mapped <a href="https://mountainmath.ca/map/special/15">about 6,900 buildings were built after 2006</a>,
but only traced 5,869 buildings in the 2006 tax dataset as
being torn down and rebuilt. That difference is largely explained by different selection criteria. The map only considers
properties with a &ldquo;year built&rdquo; field set, but for the analysis we also added properties that don&rsquo;t have that field set
but go from zero building value in 2006 to non-zero building value in 2016 which gets us to 7,784 &ldquo;rebuilds&rdquo;. On the other hand
in the analysis we don&rsquo;t consder the roughly 140 heritage buildings that would pass our filter of being built after 2006, and
the 2016 tax dataset has 2,422 more buildings than the 2006 dataset, some of which can be seen
<a href="https://mountainmath.ca/map/special/13">on this map</a> and are due to subdivisions being split off of the original
property.</p>

<p>Anyway, if you want to get your hand dirty on this shoot me a message and I will hook you up with my scripts.</p>

<script>


function bar_graph(div,shiftAxis,domainFormatter,rangeFormatter,domainLabelFormatter){
    if (!domainFormatter) domainFormatter=d3.format("d")
    if (!rangeFormatter)
     rangeFormatter = function (y) {
        return y;
     };
     if (!domainLabelFormatter) domainLabelFormatter=domainFormatter;

var margin = {top: 20, right: 20, bottom: 40, left: 70},
    width = parseInt(div.style("width")) - margin.left - margin.right,
    height = parseInt(div.style("height")) - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);


var xAxis = d3.svg.axis()
    .scale(x)
    .tickFormat(domainFormatter)
    .orient("bottom");


var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .tickFormat(rangeFormatter)
    .ticks(5, rangeFormatter);

var svg = div.append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var data_url=div[0][0].dataset.url;

d3.json(data_url, function(error, json) {
  if (error) throw error;
  var graphData=json[0];
  var data=graphData.data;
  x.domain(data.map(function(d) { return d.date }));
  y.domain([0, d3.max(data, function(d) { return d.count; })]);
  
  var domainTickValues=[];
  var skip=Math.round(40/x.rangeBand());
  if (skip<=0) skip=1;
  for (var i=0;i<x.domain().length;i++) {
    if (i % skip==0) domainTickValues.push(x.domain()[i]);
  }
  if (x.domain().length % 5 !=0) domainTickValues.push(x.domain()[x.domain().length-1]);
  xAxis.tickValues(domainTickValues);

  var xShift=shiftAxis ?  x.rangeBand()/2.0 * 1.1 : 0;
  
  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(" + xShift + "," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis);
//    .append("text")
//      .attr("transform", "rotate(-90)")
//      .attr("y", 6)
//      .attr("dy", ".71em")
//      .style("text-anchor", "end")
//      .text("Probability");

  svg.selectAll(".bar")
      .data(data)
    .enter().append("rect")
      .attr("class", graphData.class + " bar")
      .attr("fill", graphData.color)
      .attr("x", function(d) { return x(d.date); })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.count); })
      .attr("height", function(d) { return height - y(d.count); })
      .on('mouseover',function(d){
         d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('click',function(d){
       d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('touch',function(d){
         d3.select('#'+this.classList[0]+'_value').text(domainLabelFormatter(d.date) + ': ' + rangeFormatter(d.count)) 
      }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});

      
});

function type(d) {
  d.frequency = +d.frequency;
  return d;
}
}

var ready_for_graph = function(graphs,domainFormatter,numberFormatter) {
    var d3lines=[];
    var padding = {top: 20, right: 20, bottom: 30, left: 90};
    var prevChartWidth = 0, prevChartHeight = 0;
    var updateTransistionMS = 750; // milliseconds
    var sourceData, lineData, xScale, yScale, line;

    var symbol;
    var prefix;
    if (!domainFormatter) domainFormatter=d3.format("d")
    if (!numberFormatter)
     numberFormatter = function (y) {
        return y;
     };

    //var graphs=d3.select("#graph");
    var div=graphs[0][0];
    if (div==null|| div.childElementCount!=0) {return;}
    var data_url=div.dataset.url;

    // create svg and g to contain the chart contents
    var baseSvg = graphs.append("svg");
    var chartSvg=baseSvg
        .append("g")
        .attr("class", "chartContainer")
        .attr("transform", "translate(" + padding.left + "," + padding.top + ")");

    // create the x axis container
    chartSvg.append("g")
        .attr("class", "x axis");

    // create the y axis container
    chartSvg.append("g")
        .attr("class", "y axis");
    var line;
    var largest=null;
    var lineData;
    if (div.dataset.lines) {
        d3.json(div.dataset.lines,function(error,json){
        lineData=json;
        var domain=[null,null];
        var range=[null,null];
        for (var i=0;i<lineData.length;i++){
            lineData[i].data.forEach(function(d) {
                d.date = +d.date;
                d.count = +d.count;
                if (domain[0]==null || domain[0]> d.date) domain[0]= d.date;
                if (domain[1]==null || domain[1]< d.date) domain[1]= d.date;
                if (range[0]==null || range[0]> d.count) range[0]= d.count;
                if (range[1]==null || range[1]< d.count) range[1]= d.count;
            });
        }
        xScale=d3.scale.linear().domain(domain);
        var toAdd=(range[1]-range[0])/10;
        range[0]-=toAdd;
        range[1]+=toAdd;
        yScale=d3.scale.linear()
            .domain(range);

        line = d3.svg.line()
            .x(function(d) { return xScale(d.date); })
            .y(function(d) { return yScale(d.count); })
            .interpolate("linear");

        xAxis = d3.svg.axis()
            .scale(xScale)
            .orient("bottom")
            .tickFormat(domainFormatter)
            .tickValues(domain);

        yAxis = d3.svg.axis()
            .scale(yScale)
            .orient("left")
            .tickFormat(numberFormatter)
            .ticks(5);

        prefix = d3.formatPrefix(range[1]);
        if (prefix.symbol=='K') {
            symbol='k'
        } else if (prefix.symbol=='M') {
                symbol='m'
        } else if (prefix.symbol=='G') {
            symbol='bn'
        } else if (prefix.symbol=='T') {
            symbol='tn'
        }
        updateChart(true);
        });

    }


    function updateChart(init)
    {
        // get the height and width subtracting the padding
//    var innerHeight = window.innerHeight - 20;
        var innerWidth = window.innerWidth - 20;
        var divWidth=$(div).width();
        if (divWidth==0) divWidth=$(div.parentElement.parentElement).width();
        var maxWidth=parseInt($(div).css('max-width'));
        if (divWidth==0) divWidth=innerWidth*0.8;
        if (divWidth>maxWidth) divWidth=maxWidth;
        var chartWidth = divWidth-padding.left-padding.right;//960 - margin.left - margin.right,
        var chartHeight = $(div).height()-padding.top-padding.bottom;//500 - margin.top - margin.bottom;


        // only update if chart size has changed
        if ((prevChartWidth != chartWidth) ||
            (prevChartHeight != chartHeight)) {
            prevChartWidth = chartWidth;
            prevChartHeight = chartHeight;

            //set the width and height of the SVG element
            chartSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);
            baseSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);

            // ranges are based on the width and height available so reset
            xScale.range([0, chartWidth]);
            yScale.range([chartHeight, 0]);

            if (init) {
                // if first run then just display axis with no transition
                chartSvg.select(".x")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                chartSvg.select(".y")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .call(yAxis);
            }
            else {
                // for subsequent updates use a transistion to animate the axis to the new position
                var t = chartSvg.transition().duration(updateTransistionMS);

                t.select(".x")
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                t.select(".y")
                    .call(yAxis);
            }

            for (var i = 0; i < lineData.length; i++) {
                var sourceData=lineData[i].data;
                var color=lineData[i].color;
                var label=lineData[i].label;
                var className=lineData[i].class;

                // bind up the data to the line
                var lines = chartSvg.selectAll("path.line."+className)
                    .data([lineData[i].data]); // needs to be an array (size of 1 for our data) of arrays

                // transistion to new position if already exists
                lines.transition()
                    .duration(updateTransistionMS)
                    .attr("d", line);

                // add line if not already existing
                lines.enter().append("path")
                    .attr("class", "line")
                    .attr("stroke", color)
                    .attr("stroke-width", 2)
                    .attr('fill','none')
                    .attr("d", line);

                // bind up the data to an array of circles
                var circle = chartSvg.selectAll("circle."+className)
                    .data(sourceData);

                // if already existing then transistion each circle to its new position
                circle.transition()
                    .duration(updateTransistionMS)
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", function (d) {
                        return yScale(d.count);
                    });

                // if new circle then just display
                circle.enter().append("circle")
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", function (d) {
                        return yScale(d.count);
                    })
                    .attr("r", 3)
                    .attr('fill', 'transparent')
                    .style("stroke", color)
                    .style("stroke-width", 6)
                    .attr("class", className)
                    .on('mouseover',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(domainFormatter(d.date) + ': ' + numberFormatter(d.count)) 
                    }).on('click',function(d){
                     d3.select('#'+this.classList[0]+'_value').text(domainFormatter(d.date) + ': ' + numberFormatter(d.count)) 
                    }).on('touch',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(domainFormatter(d.date) + ': ' + numberFormatter(d.count)) 
                    }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});

            }
        }
    }

    // look for resize but use timer to only call the update script when a resize stops
    var resizeTimer;
    window.onresize = function(event) {
        clearTimeout(resizeTimer);
        resizeTimer = setTimeout(function()
        {
            updateChart(false);
        }, 100);
    }


};
var percentageFormatter=d3.format(".1%");
var binFormatter=function(d){return percentageFormatter(d-0.025) + ' - ' + percentageFormatter(d);}
//ready_for_graph(d3.select("#graph"));
bar_graph(d3.select("#graph_age"),false);
//ready_for_graph(d3.select("#graph_buildings_by_teardown"),percentageFormatter);
//ready_for_graph(d3.select("#graph_teardowns"),percentageFormatter);
bar_graph(d3.select("#graph_buildings_by_teardown"),true,percentageFormatter,null,binFormatter);
bar_graph(d3.select("#graph_teardowns"),true,percentageFormatter,null,binFormatter);
bar_graph(d3.select("#graph_teardown_probability"),true,percentageFormatter,percentageFormatter,binFormatter);
bar_graph(d3.select("#graph_buildings_by_teardown_2"),true,percentageFormatter,null,binFormatter);
bar_graph(d3.select("#graph_teardowns_2"),true,percentageFormatter,null,binFormatter);
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updated Vancouver Assessment Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data/"/>
    <updated>2016-01-17T11:34:09-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2016/01/17/updated-vancouver-assessment-data</id>
    <content type="html"><![CDATA[<p><a href="https://mountainmath.ca/map/assessment?layer=4"><img  src="http://doodles.mountainmath.ca/images/relative_land.jpg" style="width:50%;float:right"></a>
The friendly folks at
<a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver Open Data</a> just
<a href="https://twitter.com/VanOpenData/status/688060388190097408">updated their property assessment data</a> with the fresh 2016
property tax assessments. Time to run the script to update the <a href="https://mountainmath.ca/map/assessment">Vancouver Assessment Map</a>
with the new data. And for good measure I pasted over some of the thematic map engine from <a href="https://censusmapper.ca">CensusMapper</a>
to improve the mapping performance.</p>

<h4>Maps</h4>

<!-- more -->


<p><img  src="http://doodles.mountainmath.ca/images/map_menu.png" style="width:25%;margin-left:5px;float:right">The <a href="https://mountainmath.ca/map/assessment">interactive assessment map</a> offers several views. In the panel on the top
right we can select how to view the data. It offers standard thematic maps for value change, total value, building value
and building age and zoning. And there are some options that warrant more explanation:</p>

<ul>
<li><a href="https://mountainmath.ca/map/assessment?layer=5"><em>Relative Land Value</em>:</a> The colours on the map show each property by the
land value per m&sup2;. We can immediately spot the east-west land gradient, as well as how zoning affects land value.
When zooming in we also see the effect of lot size on land value.</li>
<li><a href="https://mountainmath.ca/map/assessment?layer=10"><em>Tax Density</em>:</a> This map looks at the tax dollars collected
by the city by area. It tells us the relative rate at what each property is contributing to city services. We can again
observe the impact of exclusionary zoning.</li>
<li><a href="https://mountainmath.ca/map/assessment?layer=4"><em>Relative Value of Building</em>:</a>
<a href="https://mountainmath.ca/map/assessment?layer=4"><img  src="http://doodles.mountainmath.ca/images/teardown.jpg" style="width:50%;float:right"></a>
This map simply divides the building value
by the total property value. There are many ways to interpret this map, my favourite is to use this as a &ldquo;Teardown Predictor&rdquo;.
Essentially, as the percentage of the building value approaches zero the probability that it will get torn down in the
near future increases. Imagine someone spending $1.5m to buy a property with a house valued at $37k. Many people don&rsquo;t
mind living in a house worth $37k, but someone who is spending $1.5m would probably prefer to buy a different property
with a higher quality house. Or spend more money to upgrade the house. How will it be upgraded? Renovating is in most
cases economically unsound, most people will choose to tear down and rebuild. In fact, the <em>teardown threshold</em> is
likely higher than the 2.5% in the example given. The percentage of properties in Vancouver where the building value is
less than 2.5% of the total value has slightly decreaed in the last year from 17.9% to 17.8%, but the percentage of properties
with building value less than 5% of the total value has increased from 32% to 33.5% during the last year.</li>
</ul>


<h4>The Data</h4>

<p>The data originates with BC Assessment, which estimates land and building values of each property based on recent sales
of comparable properties. The assessment was done in summer 2015 and is based on sales before that, so at this point in
time the data lags the market by about one year. Values for individual properties may well be off, depending how well
renovations and improvements were reported and how well the BC Assessment estimates work for the given property. On
average they should reflect the market about half a year to one year ago.</p>

<p>Sadly, BC Assessment does not give out their data with a license that would allow mapping it the way I do, so we have
to rely on municipalities to release it through their open data portals. The format of the data from each municipality is
different, so lazy me is only importing data from City of Vancouver, although some other nearby municipalities are also releasing
there data.</p>

<p>The motivation behind the map was to understand the building stock. Some effort was made to filter out parks, but the
algorithm is far from perfect and will often includes parks that host building
structures, as well as marinas with structures on them.</p>

<p>The new city dataset does not include the 2016 tax levy, so we still only show the 2015 tax levies until CoV updated their dataset.</p>

<h4>History</h4>

<p>Here is a quick history of the overall land and building values aggregated for Vancouver between 2006 and 2016.</p>

<div style="margin:10px 50px;padding:5px;border: 1px solid black;border-radius:5px;">
<div id="graph" style="height:200px;max-width:640px;" data-lines="/data/vancouver_stats.json"></div>
<div class="legend no-margin">
  <p><i style="background:blue"></i> Land Value <span style="float:right;margin-right:10px;" id="land_value"></span></p>
  <p><i style="background:green"></i> Building Value <span style="float:right;margin-right:10px;" id="building_value"></span></p>
</div>
</div>


<p>When looking at all properties in the city, the increase in land
value year over year was 21.4% ($45.2bn), while overall building values increased by 7.3% ($5bn). Hover, click or touch
the points in the graph to get the values for the corresponding year.</p>

<h4>Neighbourhoods</h4>

<p>Lastly a quick overview over the neighbourhoods. Land and building values have not increased evenly throughout in the year.
I aggregated all tax data by neighbourhood and split it into land value and building value increases.
These numbers should be used as guidance only, they mix lots of different types of properties and include parks.</p>

<p>Here is the breakdown by neighbourhood:</p>

<ul>
<li>Renfrew-Collingwood: Land: 30.6% ($1.2bn), Building: 5.8% ($46.0m)</li>
<li>Sunset: Land: 26.6% ($1.6bn), Building: 5.9% ($74.2m)</li>
<li>Oakridge: Land: 17.6% ($1.2bn), Building: 12.6% ($207.4m)</li>
<li>Downtown: Land: 16.9% ($0.8bn), Building: 3.2% ($91.2m)</li>
<li>Kerrisdale: Land: 18.7% ($1.3bn), Building: 3.3% ($54.1m)</li>
<li>Victoria-Fraserview: Land: 28.0% ($1.5bn), Building: 5.6% ($65.2m)</li>
<li>Grandview-Woodland: Land: 24.9% ($1.7bn), Building: 0.7% ($17.4m)</li>
<li>West End: Land: 22.6% ($2.5bn), Building: 3.5% ($188.4m)</li>
<li>Hastings-Sunrise: Land: 25.1% ($1.1bn), Building: 4.9% ($63.8m)</li>
<li>Killarney: Land: 17.8% ($0.3bn), Building: 14.1% ($241.6m)</li>
<li>Marpole: Land: 17.8% ($0.3bn), Building: 14.1% ($241.6m)</li>
<li>Kitsilano: Land: 21.6% ($3.3bn), Building: 4.3% ($134.5m)</li>
<li>Shaughnessy: Land: 18.6% ($1.7bn), Building: 7.4% ($117.7m)</li>
<li>West Point Grey: Land: 20.2% ($2.3bn), Building: 5.2% ($88.3m)</li>
<li>Fairview: Land: 19.4% ($2.0bn), Building: -1.0% (-$52.4m)</li>
<li>Downtown Eastside: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)</li>
<li>Kensington-Cedar Cottage: Land: 24.9% ($1.7bn), Building: 0.7% ($17.4m)</li>
<li>Riley Park: Land: 26.4% ($1.0bn), Building: 4.0% ($40.2m)</li>
<li>Mount Pleasant: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)</li>
<li>South Cambie: Land: 20.9% ($1.5bn), Building: 21.4% ($268.1m)</li>
<li>Strathcona: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)</li>
<li>Dunbar Southlands: Land: 21.8% ($1.0bn), Building: 11.2% ($126.6m)</li>
<li>Arbutus Ridge: Land: 21.4% ($1.6bn), Building: 1.1% ($17.3m)</li>
</ul>


<p>It becomes immediately clear that the increase in property values is mostly driven by land, note that total value increases
for land and buildings are reported in billions and millions, respectively. The building stock does not
have time to catch up, with the exception of South Cambie. Fairview stands out with declining overall building values.</p>

<script>
var ready_for_graph = function() {
    var d3lines=[];
    var padding = {top: 20, right: 20, bottom: 30, left: 90};
    var prevChartWidth = 0, prevChartHeight = 0;
    var updateTransistionMS = 750; // milliseconds
    var sourceData, lineData, xScale, yScale, line;

    var symbol;
    var prefix;
    var numberFormatter = function (y) {
        return '$' + Math.round(prefix.scale(y*10))/10.0 + symbol;
    };

    var graphs=d3.select("#graph");
    var div=graphs[0][0];
    if (div==null|| div.childElementCount!=0) {return;}
    var data_url=div.dataset.url;

    // create svg and g to contain the chart contents
    var baseSvg = graphs.append("svg");
    var chartSvg=baseSvg
        .append("g")
        .attr("class", "chartContainer")
        .attr("transform", "translate(" + padding.left + "," + padding.top + ")");

    // create the x axis container
    chartSvg.append("g")
        .attr("class", "x axis");

    // create the y axis container
    chartSvg.append("g")
        .attr("class", "y axis");
    var line;
    var largest=null;
    var lineData;
    if (div.dataset.lines) {
        d3.json(div.dataset.lines,function(error,json){
        lineData=json;
        var domain=[null,null];
        var range=[null,null];
        for (var i=0;i<lineData.length;i++){
            lineData[i].data.forEach(function(d) {
                d.date = +d.date;
                d.count = +d.count;
                if (domain[0]==null || domain[0]> d.date) domain[0]= d.date;
                if (domain[1]==null || domain[1]< d.date) domain[1]= d.date;
                if (range[0]==null || range[0]> d.count) range[0]= d.count;
                if (range[1]==null || range[1]< d.count) range[1]= d.count;
            });
        }
        xScale=d3.scale.linear().domain(domain);
        var toAdd=(range[1]-range[0])/10;
        range[0]-=toAdd;
        range[1]+=toAdd;
        yScale=d3.scale.linear()
            .domain(range);

        line = d3.svg.line()
            .x(function(d) { return xScale(d.date); })
            .y(function(d) { return yScale(d.count); })
            .interpolate("linear");

        xAxis = d3.svg.axis()
            .scale(xScale)
            .orient("bottom")
            .tickFormat(d3.format("d"))
            .tickValues(domain);

        yAxis = d3.svg.axis()
            .scale(yScale)
            .orient("left")
            .tickFormat(numberFormatter)
            .ticks(5);

        prefix = d3.formatPrefix(range[1]);
        if (prefix.symbol=='K') {
            symbol='k'
        } else if (prefix.symbol=='M') {
                symbol='m'
        } else if (prefix.symbol=='G') {
            symbol='bn'
        } else if (prefix.symbol=='T') {
            symbol='tn'
        }
        updateChart(true);
        });

    }


    function updateChart(init)
    {
        // get the height and width subtracting the padding
//    var innerHeight = window.innerHeight - 20;
        var innerWidth = window.innerWidth - 20;
        var divWidth=$(div).width();
        if (divWidth==0) divWidth=$(div.parentElement.parentElement).width();
        var maxWidth=parseInt($(div).css('max-width'));
        if (divWidth==0) divWidth=innerWidth*0.8;
        if (divWidth>maxWidth) divWidth=maxWidth;
        var chartWidth = divWidth-padding.left-padding.right;//960 - margin.left - margin.right,
        var chartHeight = $(div).height()-padding.top-padding.bottom;//500 - margin.top - margin.bottom;


        // only update if chart size has changed
        if ((prevChartWidth != chartWidth) ||
            (prevChartHeight != chartHeight)) {
            prevChartWidth = chartWidth;
            prevChartHeight = chartHeight;

            //set the width and height of the SVG element
            chartSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);
            baseSvg.attr("width", chartWidth + padding.left + padding.right)
                .attr("height", chartHeight + padding.top + padding.bottom);

            // ranges are based on the width and height available so reset
            xScale.range([0, chartWidth]);
            yScale.range([chartHeight, 0]);

            if (init) {
                // if first run then just display axis with no transition
                chartSvg.select(".x")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                chartSvg.select(".y")
                    .style({ 'stroke': 'grey', 'fill': 'none', 'stroke-width': '1px'})
                    .call(yAxis);
            }
            else {
                // for subsequent updates use a transistion to animate the axis to the new position
                var t = chartSvg.transition().duration(updateTransistionMS);

                t.select(".x")
                    .attr("transform", "translate(0," + chartHeight + ")")
                    .call(xAxis);

                t.select(".y")
                    .call(yAxis);
            }

            for (var i = 0; i < lineData.length; i++) {
                var sourceData=lineData[i].data;
                var color=lineData[i].color;
                var label=lineData[i].label;
                var className=lineData[i].class;

                // bind up the data to the line
                var lines = chartSvg.selectAll("path.line."+className)
                    .data([lineData[i].data]); // needs to be an array (size of 1 for our data) of arrays

                // transistion to new position if already exists
                lines.transition()
                    .duration(updateTransistionMS)
                    .attr("d", line);

                // add line if not already existing
                lines.enter().append("path")
                    .attr("class", "line")
                    .attr("stroke", color)
                    .attr("stroke-width", 2)
                    .attr('fill','none')
                    .attr("d", line);

                // bind up the data to an array of circles
                var circle = chartSvg.selectAll("circle."+className)
                    .data(sourceData);

                // if already existing then transistion each circle to its new position
                circle.transition()
                    .duration(updateTransistionMS)
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", function (d) {
                        return yScale(d.count);
                    });

                // if new circle then just display
                circle.enter().append("circle")
                    .attr("cx", function (d) {
                        return xScale(d.date);
                    })
                    .attr("cy", function (d) {
                        return yScale(d.count);
                    })
                    .attr("r", 4)
                    .attr('fill', 'transparent')
                    .style("stroke", color)
                    .style("stroke-width", 8)
                    .attr("class", className)
                    .on('mouseover',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + numberFormatter(d.count)) 
                    }).on('click',function(d){
                     d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + numberFormatter(d.count)) 
                    }).on('touch',function(d){
                       d3.select('#'+this.classList[0]+'_value').text(d.date + ': ' + numberFormatter(d.count)) 
                    }).on('mouseout',function(){d3.select('#'+this.classList[0]+'_value').text('')});

            }
        }
    }

    // look for resize but use timer to only call the update script when a resize stops
    var resizeTimer;
    window.onresize = function(event) {
        clearTimeout(resizeTimer);
        resizeTimer = setTimeout(function()
        {
            updateChart(false);
        }, 100);
    }


};
ready_for_graph();
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Canvas vs SVG]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/12/27/canvas-vs-svg/"/>
    <updated>2015-12-27T15:53:02-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/12/27/canvas-vs-svg</id>
    <content type="html"><![CDATA[<h4>CensusMapper Behind the Scenes</h4>

<p>The idea behind <a href="https://censusmapper.ca">CensusMapper</a> is that it takes away all the technical barriers to dealing with census data. So how does
CensusMapper work behind the scenes?</p>

<h4>CensusMapper Data Workflow</h4>

<!-- more -->


<p>The general setup is quite simple. We use the lean javascript open mapping platform <a href="http://leafletjs.com">leaflet</a>
as the base for mapping information. Leaflet handles the logic of dealing with zoom and pan and keeping track of the
geographic boundaries that should be mapped. That information gets then passed on to the CensusMapper servers.</p>

<p>CensusMapper will then send the appropriate census geographic polygons to the browser for leaflet to display. Once the
geographic data is available for mapping, some custom code checks what kind of information the user wants to display and
requests the census data required to make the map. The census information is then assembled on the server, sent down
and attached to the polygons and drawn
on the browser window within leaflet. This two-tier process allows the highly dynamic mapping in CensusMapper where the
data-heavy geographic polygons are kept separately thus can be cached and re-used.</p>

<h4>Drawing Census Data</h4>

<p>There are a number of ways how we can display census data in the browser. At CensusMapper we have played with three
different technologies to map data that vary in performance and browser support. They all have in common that they
won&rsquo;t run on Internet Explorer 8 or earlier, but we have just about reached the point in time where it is acceptable to
ignore IE8- in products meant for the &ldquo;general internet audience&rdquo;.</p>

<h5>SVG</h5>

<p><a href="https://en.wikipedia.org/wiki/Scalable_Vector_Graphics">SVG</a> is what our maps have been using so far. SVGs are fairly high level, which means it&rsquo;s very little work
to implement and map information. One simply passes a polygon to the browser, tells it how to color it, and the browser
takes care of the rest. SVG elements can easily be styled via CSS, so there is essentially no work involved to deal with
highlight on hover, scaling for retina displays, patterns for census data quality flags, etc. We use
<a href="https://github.com/mbostock/d3">d3.js</a> to attach the geographic and census data right to the SVG elements for easy
manipulation.</p>

<p>While mapping data this way is very easy, for CensusMapper there are two problems.</p>

<ol>
<li>We are restricted in how we can display information by the capabilities of SVG.</li>
<li>SVG rendering is done by the browser, and not all browsers are equal. Most importantly, SVG rendering in Internet
Explorer is excruciatingly slow. So slow, that CensusMapper becomes essentially useless within Internet Explorer. We
felt compelled to add a warning messeage that displayed when people opened CensusMapper with Internet Explorer. And
when you do that, that&rsquo;s a sure sign that your app has a serious problem.
<img src="http://doodles.mountainmath.ca/images/chrome.png" alt="Browser Warning" />
So how to get around those issues? Enter Canvas.</li>
</ol>


<h5>Canvas</h5>

<p><a href="https://en.wikipedia.org/wiki/Canvas_element">Canvas</a> offers a way to draw images in a browser. Unlike SVG, the drawing has to be done &ldquo;by hand&rdquo;. And the result is just
an image, with no clear way to tell where it came from. There is no way to attach any information to individual
structures drawn on a canvas. All the logic for highlight on hover, figuring out what data is associated with the mouse
position, dealing with retina displays, etc. needs to be added by hand.</p>

<p>On the upside, a good canvas implementation is a lot faster than SVG. And it opens the door to changes in how the data
is handled that bring additional performance improvements. In particular, we can now chop up census polygons and render
the pieces separately, greatly cutting down on the size of the downloaded data, as well as the complexity of the
polygons that get rendered. And the performance improvements are noticeable across all browsers and platforms.</p>

<p>At the end of the day it is actually not that much work and we flipped the switch on this just before the Christmas
break. CensusMapper is now running using canvas instead SVG for
the main maps. We
kept the look and feel the same, so unless you dig into the code you won&rsquo;t notice the difference.
Some parts of CensusMapper still utilized SVGs, like the d3-based
<a href="http://doodles.mountainmath.ca/blog/2015/10/24/census-drilldown/">Census Wheel</a>.</p>

<h5>WebGL</h5>

<p><a href="https://en.wikipedia.org/wiki/WebGL">WebGL</a> also draws on a canvas element, but the work is offloaded onto the GPU (graphics processor) giving enormous
speed improvements. Regular canvas rendering is fast enough for our purposes, but with WebGL we can do more complex
renderings that previously we could not even dream of: <a href="https://en.wikipedia.org/wiki/OpenGL_Shading_Language">Shaders</a>
and Interactive 3D data maps. We had previously
<a href="https://mountainmath.ca/census3">toyed with 3D data visualization</a> to explore Vancouver&rsquo;s household density in 3D using Three.js,
but did not pursue this further because of the complexities of writing code for navigating a Canada-wide map. Then we came
across the super-customizable 3D open mapping platform built by <a href="https://mapzen.com/projects/tangram/">Mapzen</a>, and that
suddenly made it extremely easy to do interactive 3D data mapping live in the browser. A quick test
<a href="https://mountainmath.ca/vancouver_lidar/map">using Vancouver&rsquo;s open LIDAR generated building height data</a> showed how easy Mapzen&rsquo;s
tangram engine is to use.</p>

<p>After digging deeper into tangram, and with help from the friendly people at Mapzen, we figured out a way to fit
CensusMapper&rsquo;s two-stage data workflow into tangram&rsquo;s mapping engine. The result are real-time 3D maps where height
and color of each geographic area can be independently (and dynamically) controlled. Here is an example where mouseover
trigged the area west of Coal Harbor to &lsquo;pop up&rsquo;.
<img src="http://doodles.mountainmath.ca/images/webGL.jpg" alt="webGL" /></p>

<p>At the same time we gain the ability to easily pull in all kinds of other data and map it. On our canvas or svg maps we
added regular image tiles, either a road and label&rsquo;s overlay or a base map (which then requires opacity to be added to
the census data that is mapped on top of that) as orientation aid. Short of baking our own image tiles we have very
little control over the look and feel of this. With Mapzen&rsquo;s tangram we can very easily pick and style individual
geographic objects from Mapzen&rsquo;s OSM vector tile server, resulting in crisp and clear maps. In the above example we
decide dynamically what level of roads to render, how to style them, what labels to display and we also added bodies of
water, where we filter by size depending on the zoom level.</p>

<p>At this stage it is still an ongoing project to get this production-ready. One obvious obstacle is that WebGL browser
support is still lagging. And on top of that it also requires updated graphics card drivers, which is a big problem on
windows machines that are already a couple of years old. So for now we still need to have a plain canvas or svg fallback.</p>

<p>And then there are the details that need to get worked out. 3D maps sounds great, but it will take us some time to figure
out how to best utilize this in thematic maps. But even without utilizing 3D capabilities, the dynamic shaders and increased
rendering performance are already pushing the boundary of what&rsquo;s possible in web maps.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bike Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/12/15/bike-data/"/>
    <updated>2015-12-15T15:16:48-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/12/15/bike-data</id>
    <content type="html"><![CDATA[<p><a href="http://doodles.mountainmath.ca/bike_map2.html"><img  src="http://doodles.mountainmath.ca/images/bike-routes2.png" style="width:25%;float:right"></a>Maps live and die with the quality
of the underlying data. So I decided to dive a little deeper down the OSM bike data rabbit hole. Task number one was
to expand display data for a wider region. My primitive workflow to pull data out of OSM only allows for extracting a quarter
of a degree at a time. For playing around with all of Vancouver&rsquo;s data I again turned to Mapzen for their
<a href="https://mapzen.com/data/metro-extracts">metro extracts</a> as a convenient shortcut for OSM data. For the case of Vancouver
the word &ldquo;metro&rdquo; is a bit of an exaggeration, it just covers the City of Vancouver. But good enough for some more testing.</p>

<h4>More data</h4>

<!-- more -->


<p>More data means a a larger variety of <a href="http://wiki.openstreetmap.org/wiki/Key:cycleway">bike infrastructure types</a>.
At first glance tagging seems to be reasonably accurate, but on closer inspection one quickly spots lots of issues.
First off, the <a href="https://twitter.com/alexwarrior/status/675820327331479552">edits Alex made earlier</a> are <strike>not
showing up, that&rsquo;s because the metro extracts are only done weekly. Just a matter of time.</strike> showing up now after
updating with new OSM data.</p>

<p>And then there are lots of little issues. Off street paths
don&rsquo;t connect to roads (or anything for that matter), making the useless for routing. Some changes in the bike network
are tagged differently, take Point Grey Road as an example. Some paths are labeled to be ok for bikes, but bikes aren&rsquo;t
allowed there. Some tags seem off.</p>

<p>And I don&rsquo;t seem to have captured all relevant bike infrastructure, will have to spend some time one of these days to check
through all the different taggings in OSM to make sure I pull out all the correct ones.</p>

<p>So to make more sense out of the network I made a new map, added some more categories for optionally fading them out when
displayed. And on hover I display the tags, just for interest.</p>

<h4>Improving data</h4>

<p>This gets us to task number two. The folks at Mapzen gave me some friendly pointers to allow easy editing of features.
So I pasted a couple of lines of code in so if you hold down the shift key and
click on a feature, it will take you automatically to OSM to edit the feature. You may have to sign in the first time
you do this. If you hold the shift key while clicking anywhere else it also takes you to the OSM editor for that spot,
in case you want to add something there. It takes a lot of the pain out of editing bike infrastructure, I just fixed a
whole bunch of things in short time.</p>

<iframe src="http://doodles.mountainmath.ca/bike_map2.html" width="100%" height="550"></iframe>


<p>Go for it and fix some problems that you see, either by shift-clicking on the embedded map or by
<a href="http://doodles.mountainmath.ca/bike_map2.html">taking the map full-screen first</a>. Remember that some of the issues may be fixed already, the bike map will not reflect
updates until a week later or so.</p>

<h4>Where to go from here?</h4>

<p>It looks like making a decent bike map with OSM data is feasible. The hard work will be to collaboratively do all the
OSM edits required to get the data into good shape.</p>

<p>One problem is that edits won&rsquo;t show up on the bike map for up to a week, that&rsquo;s the frequency at which the metro extracts
are updated. And then I have to update the file for displaying the bike data. A minor inconvenience, in theory there
are ways to seed this up if one wants to be serious about this. But then again, once the OSM bike data is fixed up in a
given region, it won&rsquo;t need updating very frequently.</p>

<p>I guess I will have to mull this over and decide how deep down this rabbit hole I would like to go. Looking back at my
<a href="http://doodles.mountainmath.ca/blog/2015/12/13/how-to-make-a-bike-map/">rant on what&rsquo;s wrong with most bike maps</a> I am
asking myself how much the current map has accomplished.</p>

<ol>
<li>The accuracy of the infrastructure mapped is in the hands of the OSM community. With easy access to the OSM editing
functionality from the map (desktop only) it&rsquo;s in the hands of the people that know best: cyclists.</li>
<li>Still coming up blank when it comes to showing comfort level. Will have to think more how to best do this, it might
have to go into a separate database. Which is doable, but requires work to get it right and is probably a little too
involved to do it off the side.</li>
<li>Click and zoom is not an issue, but now I am in the opposite corner where it might be nice sometimes to actually have
a glossy big paper map. And it&rsquo;s really easy to save the map as an image. Some tinkering should be able to produce a nice
high-resolution one covering a large area.</li>
<li>The map already adapts to user preferences quite well, different types of infrastructure will fade out if deselected.</li>
<li>Not on the original list, but routing should be part of this. Routing works reasonably well to get this off the ground,
but work is still required to make it work properly with user preferences. This may well require running a custom router,
again a little more than I bargained for.</li>
</ol>


<p>So is this really worth the trouble to make yet another bike map? I am not sure. It&rsquo;s always easy to through up a quick
proof of concept, but to do it really well requires quite a bit more work. For now I could just build one focused on
Vancouver and see how it goes. And it would be great if the comfort level rating could somehow be automated.</p>

<p><img  src="http://doodles.mountainmath.ca/images/cycletrack.PNG" style="width:50%;float:right">Which brings to another one of pet projects that
I never took beyond the testing realm. My CycleTrack App that records
all bike trips in the background (as long as you carry your phone with you). No pressing of &ldquo;start&rdquo; or &ldquo;stop&rdquo; buttons,
the app notices when you are moving and will take gyroscope and accelerometer readings to figure out if you are cycling,
running, walking, driving or taking a train. Then it stores your cycling trips and computes aggregate data. It avoids
using GPS so not to drain your battery too much, on a typical day it will consume about 3% of battery power. The downside
is that the accuracy and frequency of the location updates is not as high, so things get a little messy. But not too bad.</p>

<p>How does this fit into the bike map project? Simple. If one can collect regular cycling data from normal people
cycling (not just
the lycra crowd that presses start cycles in circles and presses stop again and recharges their phone while taking a shower),
once can infer a lot about comfort levels just by looking at the data. And to collect data from regular &ldquo;citizen cyclist&rdquo;
one cannot expect them to press &ldquo;start&rdquo; and &ldquo;stop&rdquo; to delineate their bike trips, and one cannot have an app that will
require them to recharge their phone after every trip.</p>

<p>But then the project gets even bigger&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bike Routing]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/12/14/routing/"/>
    <updated>2015-12-14T20:10:16-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/12/14/routing</id>
    <content type="html"><![CDATA[<p><a href="http://doodles.mountainmath.ca/bike_routing.html"><img  src="http://doodles.mountainmath.ca/images/routing.png" style="width:25%;float:right"></a>Routing is a hard problem. Routing for drivers is pretty good at this point, mostly because we have been very good at
designing for cars and creating predicable infrastructure. Routing for bikes is a whole other story, data quality is
poor and the physical infrastructure is, at least in North America, not strongly predictive of cycling comfort/safety.
And cycling comfort/safety is the top priority for the
<a href="http://usa.streetsblog.org/2015/03/13/the-first-ever-nationwide-survey-of-interested-but-concerned-bikers-is-here/">vast majority of (potential) cyclists</a>.</p>

<p>And it&rsquo;s the ones that don&rsquo;t cycle frequently, often out of concern for safety, that would benefit most from effective
bike routing.</p>

<p>Read on or <a href="http://doodles.mountainmath.ca/bike_routing.html">go directly to the routing demo</a>.</p>

<!-- more -->


<p>Google does a decent job directing a relatively experienced cyclist from A to B, but it has a hard time to learn about
places where cyclists can go but cars can&rsquo;t. And it won&rsquo;t be able to answer my fundamental question: <em>Can I bring my 6
year old along?</em>. And Apple doesn&rsquo;t even try and offer bike routing.</p>

<h4>Routing test</h4>

<p>So what&rsquo;s really needed apart from <a href="http://doodles.mountainmath.ca/blog/2015/12/13/how-to-make-a-bike-map/">better bike maps</a>
is better bike routing. So building on yesterday&rsquo;s post, I decided to take a quick look at routing. Time to try out
<a href="https://mapzen.com/projects/valhalla">Mapzen&rsquo;s routing engine</a> which, as expected, was really easy to set up:</p>

<p>Feel free to drag the endpoints to test your favorite routes.</p>

<iframe src="http://doodles.mountainmath.ca/bike_routing.html" width="100%" height="550"></iframe>


<p><a href="http://doodles.mountainmath.ca/bike_routing.html">Full screen view</a></p>

<p>Initial testing seems to indicate that this works reasonably well. And while the engine allows for some customization
on rider needs, right now there is no way to get the &ldquo;dad&rsquo;s routing&rdquo; that I would like to have.</p>

<p>Part of the problem is of course that I still don&rsquo;t have enough information in OSM to even make a &ldquo;dad&rsquo;s map&rdquo;
<a href="http://doodles.mountainmath.ca/blog/2015/12/13/how-to-make-a-bike-map/">as I lamented earlier</a>. But at least OSM gets
me half-way there by giving me a finer control over distinguishing infrastructure that I may deem as generally more
suitable so that I can fade selected bike infrastructure out by checking the appropriate boxes in the map.</p>

<h4>Route costing options</h4>

<p>The bike routing options in Mapzen&rsquo;s routing engine allow for some level of control on wheter gravel should be avoided
(great feature for the lycra cowed but useless for dads), whether hill should be avoided (helpful) and whether to avoid
roads without bike infrastructure. But when I cycle with my 6 year old a bike lane between parked cars and 50 traffic
is as good as no bike infrastructure at all. And there is currently no way to cost different types of bike
infrastructure, so they can&rsquo;t be used as a proxy for cycling comfort.</p>

<h4>Where to go from here</h4>

<p>Next steps are to <a href="https://github.com/valhalla/thor">look deeper into Mapzen&rsquo;s routing engine</a> and see how hard it would
be to hack some of these more advanced costing options into their routing engine and open up a feature request.</p>

<p>Wrapping up the three-night trials in bike mapping is the
<a href="http://doodles.mountainmath.ca/blog/2015/12/15/bike-data/">post on data</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to make a bike map]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/12/13/how-to-make-a-bike-map/"/>
    <updated>2015-12-13T12:04:25-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/12/13/how-to-make-a-bike-map</id>
    <content type="html"><![CDATA[<p><a href="http://doodles.mountainmath.ca/bike_map.html"><img  src="http://doodles.mountainmath.ca/images/bike-map.png" style="width:25%;float:right"></a>Here come some general thoughts on bike maps. Not throught through yet, just jotting down some ideas so that I don&rsquo;t
forget and maybe to start a discussion.</p>

<p>Bike enthusiasts, OSM folks and mapping technology wonks read on!</p>

<!-- more -->


<h4>Why make a new bike map?</h4>

<p>Plainly put, I don&rsquo;t like a lot of the bike maps out there. Time for my little bike map rant:</p>

<ol>
<li>Lots of maps don&rsquo;t accomplish their core mission:
accurately map bike infrastructure. For example, when in real life a bike lane vanishes 100m in front of an intersection
and reappears 100m after the intersection, leaving cyclists exposed right where protection is needed the most, most
bike maps will mark the entire section as having a bike lane.</li>
<li>Most bike maps focus on the wrong issue. They focus on physical infrastructure, as a cyclist I am interested in
cycling comfort level. Can I take my 6 year old along that route? Can I cycle leisurely or will I feel hastened by a
car breathing down my neck? Will I arrive at my destination relaxed or will I be riled up by 3 near misses and 5 drivers
swearing at me? Some of these questions can be answered by mapping infrastructure. A physically separated bike path will
give me the comfort I need to bing my 6 year old and enjoy my ride. But when it comes to &ldquo;bike lane&rdquo; or &ldquo;shared road&rdquo;
designations, I have no idea what it will feel like until I get there.</li>
<li>Most bike maps have not yet made the transition from paper/PDF based maps to clickable and zoomable maps that can be
easily consumed on the go.</li>
<li>Most maps are static, they don&rsquo;t adapt to the needs of the user. Why do I have to deal with 5 different colors and
3 types of shading when reading a bike map, when all I want to know is if I can bring my 6 year old along for the ride.
I want an &ldquo;AAA&rdquo; button that fades out all routes that are not suitable for all ages and abilities. For that purpose I
don&rsquo;t care if a road has infrastructure like a bike lane wedged between parked cars and 50km/h traffic. If we absolutely
have to ride that way it will be on the sidewalk.</li>
</ol>


<h4>How to make a new bike map?</h4>

<p>First let&rsquo;s start with examples of how other have tried to solve these problems. Probably one of the best maps out there
is the <a href="https://www.crd.bc.ca/docs/default-source/crd-document-library/maps/transportation/bikemap2014-frontback-web.pdf?sfvrsn=8">Capital Regional District Bike Map</a>.
I chatted with <a href="https://twitter.com/Burgundavia">Corey</a> who worked for CRD on these maps on how it was done. They collected all the bike infrastructure
in the region by contacting all the municipalities. <img  src="http://doodles.mountainmath.ca/images/victoria.png" style="width:50%;float:left"> Then they meticulously
checked that what the municipalities reported actually existed on the ground.  Consider Fort road on their bike map where they accurately show the bike lane
cutting out. And they added in shortcuts and connections
that the municipalities did not report on, like a short section of footway that may technically require walking the bike
but shortcuts out of a suburban cul-de-sac maze.</p>

<p>Next the CRD map does not only map infrastructure, but color-codes it to show cycling comfort. <img  src="http://doodles.mountainmath.ca/images/victoria-legend.png" style="width:50%;float:right">
This gives me a good idea how comfortable the rider will feel. Unfortunately all that complexity starts to make it difficult to
read the bike map.</p>

<p>That&rsquo;s where technology could come to the rescure. The CRD map is a PDF map and suffers the usual limitations that come
with it. The map cannot adapt to the needs of the reader. So how about moving the map online and letting the user decide
how to display it?</p>

<h4>Bike maps that adapt to the user</h4>

<p>The <a href="http://www.washingtonpost.com/blogs/wonkblog/wp/2015/04/01/bleak-maps-of-how-cities-look-using-only-their-bike-lanes/">Washington Post Wonkblog</a>
had an excellent feature mapping only the bike infrastructure, nothing else. And for good measure, they removed the
shared streets bike infrastructure. The rational was that in most cases, comfort level on shared street bike routes is really no different from
cycling on a parallel road. Inspired by that I decided <a href="http://doodles.mountainmath.ca/blog/2015/04/01/bike-paths/">map Vancouver and a couple of other cities</a>
and I added in some button to allow selectively turning on or off typed of infrastructure.</p>

<iframe src="http://doodles.mountainmath.ca/bike_paths.html?fh=50&nh=true" width="50%" height="300" style="float:left;"></iframe>


<p>That&rsquo;s getting one step closer to adaptable bike maps, but it is still missing the main point that as a cyclist I am
most interested in comfort level, and infrastructure type is only a proxy for that. And a poor one in many cases.</p>

<h4>Inspiration</h4>

<p>Not convinced we need a new bike map? Let&rsquo;s draw on some local (for me) maps that inspired this effort.</p>

<p>UBC Campus Planning <a href="https://twitter.com/ubc_candcp/status/662027568204357633">keeps tweeting out a bike map</a>
that lists several dangerous roads as &ldquo;designated on street cycling routes&rdquo;, including an unlit divided 4 lane highway
with a posted speed limit of 60km/h and typical traffic speeds significantly exceeding that. And all of that apparently
aimed at people that don&rsquo;t cycle to campus yet but are considering doing so.</p>

<p>TransLink just <a href="http://www.translink.ca/-/media/Documents/cycling/cycling_routes/full_maps/TransLink%20Regional%20Cycling%20Map%20West.pdf">updated their bike map</a>
and marks said highway as &ldquo;recommended by cyclists&rdquo;. The do acknowlege that the road &ldquo;does not have special treatment
for cyclists&rdquo;. So why include it on the map? Filling in the the gaps in the cycling network should happen in real life,
not just on the map.</p>

<p>Even HUB keeps <a href="https://bikehub.ca/about-us/our-positions/ungapthemap">marking Wesbrook Mall north of Agronomy as &ldquo;existing cycling route&rdquo;</a>
when in real life it is everything but that.</p>

<h4>How to make a bike map</h4>

<p>So how should we make a bike map? I was always thinking about setting up my own database and somehow adding and rating
infrastructure. Then <a href="https://twitter.com/alexwarrior">Alex</a> decided to start making a bike
map for the UBC area. And he chose the most straight-forward path: Editing Open Street Map data. That way the edits are
immediately (better: after their map tile refresh cycle) available online, for example on
<a href="http://www.opencyclemap.org/">OpenCycleMaps</a>. The map is easily accessible
on the go, it zooms and scrolls. It is missing a &lsquo;locate me&rsquo; button (which is easy to fix). The look and feel is a little
dated. But most importantly, it does not adapt to the user&rsquo;s needs. I can&rsquo;t ask it to display the &ldquo;dad&rsquo;s version&rdquo; of the
cycle network, showing me only pieces that I will be comfortable cycling with my 6 year old.</p>

<p>Technology has moved along since that map was built. A very easy way to solve these issues is to
utilized the awesome tools built by the folks at <a href="https://mapzen.com">Mapzen</a>. Their
<a href="https://mapzen.com/projects/tangram">tangram mapping engine</a> taps into their
<a href="https://mapzen.com/projects/vector-tiles">OSM vector tile service</a> to make flexible mapping of bike data a breeze. And
to top things off, they offer very easy to use and extremely powerful ways to style the map. Only problem: Mapzen&rsquo;s
OSM extracts don&rsquo;t have cycling information. Not a big problem though, we can just pull them out separately and add
them on by hand. Here is an example where only the bike routes near UBC are added.</p>

<iframe src="http://doodles.mountainmath.ca/bike_map.html" width="100%" height="550"></iframe>


<p><a href="http://doodles.mountainmath.ca/bike_map.html">Full screen view</a></p>

<p>One small drawback is that WebGL, the technology Tangram is based on, is not available for much of the windows world.
WebGL requires modern browsers (IE9 does not count) and also modern hardware/graphics card drivers. A couple of years
old windows machine will likely not be able to render WebGL no matter what browser you use. But the main target is
mobile, and iOS and most android won&rsquo;t have a problem with WebGL. If really needed, could add a fallback or use older
and web technology to make the map, but Mapzen&rsquo;s Tangram makes it so ridiculously easy to make and style nice maps&hellip;</p>

<h4>The main problem left to solve</h4>

<p>One major problem left. OSM does not have and &ldquo;cycling comfort&rdquo; tags right now. There are tags for physical infrastructure,
and in some cases the comfort level can be correctly inferred from those. But in many cases it can&rsquo;t.</p>

<p>There are two ways around that. One could
keep those tags in a separate file, but that becomes difficult to maintain when OSM features change. Or one can add the
tag to the OSM data. That way better bike maps can scale easily, and the information can also be used in OSM-based
routing services where their real value lies.</p>

<p>Not sure which is the way to go here, a similar tag
<a href="http://wiki.openstreetmap.org/wiki/Proposed_features/bike_safety">has been proposed before</a> but apparently did not
go anywhere. Looking at the <a href="http://wiki.openstreetmap.org/wiki/Any_tags_you_like">tag guidelines</a> the fact they these
new tags are very useful for routing seems to speak for adding them to OSM, but the fact that they are measuring
something explicitly non-geographic by going beyond the already existing physical infrastructure markers might mean
they are better kept in an external database. Will have to think this over, feedback appreciated.</p>

<h4>Where to go from here.</h4>

<p>The logical next step is to add routing. Again, Mapzen&rsquo;s <a href="https://mapzen.com/projects/valhalla/">flexible routing service</a>
seems to be a natural match. Also, adding editing capabilities right onto the map would be quite useful. The bike map is
where data issues are best seen, and saving the step to head over to OSM to fix it (after creating an account) and then
waiting for the data to update on the map seems like a workflow that will discourage wide participation.</p>

<p>We follow up by <a href="http://doodles.mountainmath.ca/blog/2015/12/14/routing/">exploring routing</a>
and <a href="http://doodles.mountainmath.ca/blog/2015/12/15/bike-data/">data quality</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Census Drilldown]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/10/24/census-drilldown/"/>
    <updated>2015-10-24T20:45:16-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/10/24/census-drilldown</id>
    <content type="html"><![CDATA[<h4>Next steps in CensusMapper</h4>

<p>The <a href="http://censusmapper.ca">Census Mapper Project</a> is moving along slowly, public beta unearthed some bugs and we gathered
feedback (thanks to everyone reporting back!). There are still a couple of steps that need to be taken care of before
we can unleash the full map making power to all users. We feel that the complexity of census data requires more guidance
than the current map making system is providing. Anyone who does not mind getting there hands dirty and having to look
up census variable definitions by themselves when making maps is welcome to contact us and we will hook you up with a
beta mapmaking account.</p>

<p>In the meantime we added one important feature to the CensusMapper.</p>

<h5>Content Drilldown</h5>

<p>CensusMapper is a great way to explore single census variables (or a single function built out of census variables)
across many geographic regions and aggregation levels. But sometimes we would like to do the opposite: Drill down into
a specific census region and explore other census variables. We have now added an easy way to do this. To access it
simply select the &ldquo;more&rdquo; button in the basic popup when you select a census region. This brings up the <em>census wheel</em>,
which is our way to navigate through census data.
<img src="http://doodles.mountainmath.ca/images/wheel.png" alt="Census Wheel" /></p>

<p>Try it out right away on <a href="http://censusmapper.ca">CensusMapper</a> or read on for details on how this works.</p>

<!-- more -->


<p>There are almost 4,000 census variables available, right now we do not offer to split up by gender, which reduces the
available variables to 1,429. To further simplify things we throw out all variables with zero values for the give
geographic area, still leaving a
sizeable number of variables to browse through.
<img src="http://doodles.mountainmath.ca/images/mother_tounge.png" alt="Mother Tounge" style="max-width:400px;margin-right:15px;margin-top:10px;" align="left"/>
Each arc in the census wheel represents a variable, or a category of
variables. Selecting an arc will zoom into that arc and turn it into the &ldquo;center&rdquo; of the wheel, collapsing all other
components. That&rsquo;s the content <em>drilldown</em> process. Once it makes sense to display data as proportions, we switch to the
<em>proportional view</em> which shows the data as hierarchical pie chart.</p>

<p>This gives a visual representation of the proportions of each of the variables. Hovering (or touching) an arc will
display more detailed information, selecting one will drill down further. To reverse that process either select the
center or us the <em>content breadcrumbs</em> at the top that were created during the drilldown process.</p>

<h5>Data Problems</h5>

<p>Census data is messy. Now that all census data for each region is generally accessible in CensusMapper we need to
explain some of the inherent data problems.</p>

<h6>Rounding</h6>

<p>Census Canada will <a href="http://www12.statcan.gc.ca/census-recensement/2011/dp-pd/prof/help-aide/N2.cfm?Lang=E">round</a>
(almost) all data to preserve anonymity and don&rsquo;t create false impressions of accuracy that
the data does not achieve. Data is generally reported in increments of 5, rounding includes randomness to preserve
anonymity. The value of the measured value
is <a href="https://www12.statcan.gc.ca/census-recensement/2011/ref/DQ-QD/conf-eng.cfm">within 4</a> of the reported one. And
remember that even the measured variable is only an estimate of the actual value of the variable.
Rounding may lead to situations where, for example, the sum of all people listed
by age bracket will not add up to the total number of people. Generally, this difference will be small and we ignore it
in our visualization.</p>

<h6>Omitted Data</h6>

<p>Census Canada will at times not report data. This could be due to very low return rates or other problems that make data
so unreliable that it is better not reported at all. Or it could be that releasing the information could compromize
the anonymity of the census data for some people in that area. The latter can take
the form of Census Canada not reporting any data for the region, or Census Canada zeroing out specific variables that
&ldquo;are too low to be reported&rdquo;.
<img src="http://doodles.mountainmath.ca/images/unaccounted.png" alt="Unaccounted" style="max-width:400px;margin-left:15px;margin-top:10px;" align="right"/>
We have not been able to find clear guidelines how the &ldquo;zeroing&rdquo; works, but often this will leave
detectable traces in the data. Looking at the example in the image, looking at &ldquo;Mode of Transport&rdquo; to work only
&ldquo;Driving&rdquo; has non-zero values, the
other options &ldquo;Passenger&rdquo;, &ldquo;Transit&rdquo;, &ldquo;Bike&rdquo;, &ldquo;Walk&rdquo; and &ldquo;Other&rdquo; are all zero. There were 160 people getting to work,
115 are listed as &ldquo;Driver&rdquo;, leaving 45 unaccounted for. This is outside of the range that could be explained by the
rounding of variables. We alert the user by adding in
a grey area for the missing 45. This also ensures that the visual representation remains accurate.</p>

<h6>Multiple Responses</h6>

<p>Some census questions allow for multiple responses. For exaple &ldquo;Language Spoken Most Often At Home&rdquo;. In this particular
case the census variable breaks out single responses and multiple responses and is very transparent to the user. In
other cases, for example &ldquo;Ethnicity&rdquo;, single and multiple responses are not reported separately but responses are all
added up. This leads to the sum of lower level variables being higher than the base variable. We alert the user to this
by overlaying small white dots on the base variable.
<img src="http://doodles.mountainmath.ca/images/multiple_responses.png" alt="Multiple Responses" style="max-width:400px;margin-right:15px;margin-top:10px;" align="left"/>
In this particular case the total for for &ldquo;Ethnic Origin&rdquo; was 12,140 people. But there were 1,430 more responses at the
next level, so up to 1,430 people had given multiple responses to this question listing more than one of the aggregate
(mostly continent level) origins, some possibly listing more than two. The same patter repeats at different ethnic
origin aggregation
levels, for example 2,565 people claimed at least one of the &ldquo;British Island origins&rdquo;, but many listed more than one
resulting in the sum of the individual regions with the British Islands exceeding the British Island count by 1,445.
Again, we alert the user by overlaying dots over the &ldquo;British Island origins&rdquo; arc. Hovering over the arg will display
the exact numbers of the &ldquo;overcounting&rdquo; due to multiple responses.</p>

<p>In these cases where mulitple repsonses are not broken out the dots will aler the user that the proportional
representation in the hierarchical pie chart does not represent proportions out of a total given by the value of the
variable at the centre (or lower level), but as a proportion of all responses which exceeds the value of the
lower level variable.</p>

<h6>Basic Census</h6>

<p>The Basic Census is generally speaking quite reliable, every single person is required to fill it
out and return rates are generally above 95%. Serious problems will only occur if response rates are very low. We alert
the user by shading geographic regions is this has been the case.</p>

<h6>NHS</h6>

<p>The National Household Survey is quite different in nature, it was only
sent out to a smaller portion (~30%) of society and return rates were much lower (~69%). Even with 100% return rates
there are likely to be geographic regions where the results severely misrepresent reality in that region due to sampling
bias. For each region that bias is small, but the probability for bias grows as the number of people in the geographic
region declines. So this is mostly a problem for Dissemination Areas. But even there, the probability of severe sampling
bias in each region is small, but there are many regions and the probability that some of these regions suffer from
sever sampling bias is quite large.</p>

<p>On top of this basic statistical sampling bias, we also have self selection bias due to some deomgraphics being more
likely to return the survey than others. This bias is a product of the decision of scrapping the madatory
&lsquo;long form census&rsquo; and replacing it with the voluntary NHS. The return rates can give some indication of the
likelyhood of self-selection
bias, we shade regions with a non-return rate lower than 50%, the cutoff Census Canada set for reliability of the NHS.
It is especially problematic when trying to detect change in variables (for example poverty) from one census to another
as the differences in the variable over time are often small and similar in magnitude to possible self-selection bias.</p>

<p>The 50% cutoff we highlight in CensusMapper is just a guideline, the exact return rates are displayed on hover or when
selecting regions and should always to be taken into
account when interpreting results,
especially at the Dissemination Area level.</p>

<p>If all this information did not turn you off, head over to <a href="http://censusmapper.ca">CensusMapper</a> and drill down into
some geographic areas.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Census Mapper]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/09/28/census-mapper/"/>
    <updated>2015-09-28T09:23:41-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/09/28/census-mapper</id>
    <content type="html"><![CDATA[<h5>We are excited to announce CensusMapper public beta launch!</h5>

<p>The project on mapping census data for Canada is entering the next stage. We are now mapping 3875 different
variables from the 2011 census, or any combination of them, across 67215 geographic regions covering all of Canada.</p>

<p>We are opening up the <a href="http://censusmapper.ca">Census Mapper</a> to a limited public beta. Limited means that anyone
can view maps created in CensusMapper, but we are only opening up the map creation tools to selected beta users.</p>

<p>Be aware that the web app makes use of modern web technology and renders large amount of data. It will only work on
modern browsers, best viewed in Chrome or Safari. Firefox works ok, Internet Explorer might grind to a halt and should be avoided.</p>

<p><a href="http://censusmapper.ca"><img src="http://doodles.mountainmath.ca/images/map-tools.jpg" alt="CenusMap Mapmaker" /></a></p>

<p>We are a little restrictive on creating maps right now for the simple reason that census data is somewhat tricky to
understand and at this point we don&rsquo;t have a comprehensive guide explaining all the variables and warning against many
of the pitfalls. We are planning to slowly integrate this and open up the map creation tools to the general public.</p>

<h3>Why CensusMapper?</h3>

<!-- more -->


<p>Census Canada data is extremely rich and useful in many cirumstances, but it is not being widely used. There are many
reasons for this, the somewhat unmanageable amount of data being one of them, the difficulty of accessing and standardizing
the in principle freely available datasets is another.</p>

<p>Census data is inherently geographic in nature, working with the data without proper visualization tools can be challenging
too. And even for people that have good access to the data and that are well-versed in mapping geographic data, it can
still take quite a bit of time to generate maps visualizing the data. CensusMapper greatly speeds up this process by
allowing straightforward mapping of any function derived from census variables through all geographic aggregation levels
Canada wide.</p>

<h3>Storytelling</h3>

<p>Census mapper does more than just mapping census data. It is designed as a storytelling tool. Few maps are so crisp and
clear that they are self-explanatory. A map of population density might fall into that category. But most census variables
are sufficiently complex that maps derived from them warrant narration. We think of CensusMapper as a storytelling tool
that allows &lsquo;readers&rsquo; of the map to interact with it, zoom in, zoom out, pan around, and jump to other maps linked in the
story provided by the mapmaker.</p>

<h3>Directions</h3>

<p>There are many ways to expand on this. On the map creation side we can offer more diverse coloring tools, allowing user
input and user defined map locations to be used in the mapping function, add data from previous census. We could allow
limited upload of user data to be integrated with census data, statistical and spacial analysis tools, custom mapping projects.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Hidden Mortgage]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/08/25/the-hidden-mortgage/"/>
    <updated>2015-08-25T23:18:12-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/08/25/the-hidden-mortgage</id>
    <content type="html"><![CDATA[<p>Some months ago I did a little side project and put some Census Canada data for Vancouver
<a href="https://mountainmath.ca/census">on an interactive map</a> online. After it generated quite some interest, to a large part
due to Ian Young&rsquo;s reporting
<a href="http://www.scmp.com/comment/blogs/article/1851003/bizarro-vancouver-25000-households-declare-less-income-they-spend">utilizing some of the data in the South China Morning Post</a>,
I talked to my friend Alejandro and we decided to set up a Census Mapper that allowes laymen to map any census variables
of their choosing Canada wide.</p>

<p>Since this is only a side project, it will take some time to set this up properly. In the meantime, I get to have some
fun to play with census data and map some variables of my choosing.</p>

<p>For a sneak preview of what the Census Mapper will be able to do read below the fold. No interaction, no chosing your
own variables and panning around Canada yet. Just some screenshots with a story to tie them together.</p>

<!-- more -->


<h3>Census Mapper Sneak Preview</h3>

<p>One of my many pet pieves has been the Drive Until You Qualify phrase. The idea is that you buy the house closest to your
place of work for which the bank will aprrove a loan. The obvious problem with this is that your commuting costs will
go up the further away from work you live &ndash; and you disposable income goes down accordingly. But the bank does not
seem to be bothered by that at all and often does not get much attention by househunting families either.</p>

<p>The fix? Easy. The cost of commuting and housing should not be separated by viewed in concert with one another.
People househunting right will have to do this based on their individual parameters, but let&rsquo;s take a look at what
choices people have made in the past based on Census Canada&rsquo;s 2011 NHS data.</p>

<h4>Dwelling Value</h4>

<p>In mapping terms, let&rsquo;s start with the map of Median Dwelling Values around Vancouver, taken from the NHS. Although
a little dated, it still serves to make the basic point.</p>

<p><img src="http://doodles.mountainmath.ca/images/dwelling_value.jpg" alt="&quot;Median Dwelling Value&quot;" /></p>

<p>The image is the familiar one, homes are expensive on the west side, a little less so toward the east. In Vancouver
proper the downtown area is somewhat affordable with prices dipping below $300,000 for the median dwelling. We are
considering the whole range, one bedroom condos all the way up to single family homes.</p>

<h4>Commute Time</h4>

<p>Next up, how much time to these people spend commuting? The NHS got you covered.</p>

<p><img src="http://doodles.mountainmath.ca/images/commute_time.jpg" alt="&quot;Median One Way Commute Time&quot;" /></p>

<p>The median one-way commute time give us a picture of how much time people spend to go to work. And we will assume they
spend the same time coming back.</p>

<p>How much does it cost to commute. We will think of the commute cost as being made up of two components. The money
spent to get to and from work and the dollar value of the time. That leaves us with two more questions. How much money
do people spent and how much is their time worth.</p>

<h4>Transportation Cost</h4>

<p>Luckily, again the NHS has something to say about this. To figure out the money spent on the commute we look at the mode
of transport. NHS tell us how many people drive, are passengers, take transit, cycle or walk to work. To keep things
simple we will compute the &lsquo;median commute cost&rsquo; by setting transit cost at $120 per month (roughly the price for a
2-zone transit pass) and for driving we set the car2go rate of 41c/min (which is quite generous, most people will spent
more if they own a car, and pay for gas, insurance, parking themselves and spent time to maintain it). For simplicit we
set the cost for passengers, cyclists and pedestrians at zero. So we sum over the number of drivers and transit users
and divide by the total number of commuters in each dissemination area to estimate that cost. Generally speaking it
turns out to be much smaller than the &lsquo;time cost&rsquo; calculated next.</p>

<h4>Time Cost</h4>

<p>To compute the &lsquo;time cost&rsquo; of commuting for the average person in each dissemination area we need to know what people&rsquo;s
time is worth. The NHS has a simple answer for that, the Median After Tax Household Income.</p>

<p><img src="http://doodles.mountainmath.ca/images/median_after_tax_income.jpg" alt="&quot;Median After Tax Household Income&quot;" /></p>

<p>We estimate the median annual time cost of the commute by taking the ratio of the daily commute time to the daily work time,
assumed to be eight hours, and multiplying it by the annual after tax income. That&rsquo;s more or less the money the average
person in that dissemination area makes in the time the person spent commuting. If there are several people in the
household contributing to the income, there will be several people spending time
commuting, so that effect roughly cancels out.</p>

<h4>Annual Commute Cost</h4>

<p>Putting it together we get the annual cost of the commute map.</p>

<p><img src="http://doodles.mountainmath.ca/images/commute_cost.jpg" alt="&quot;Annual Cost of Commute&quot;" /></p>

<p>This is only an estimate. From a technical perspective we probably should have been working with averages instead of
median values, but the result is very similar. Translating between time and money is inherintly tricky, most people can&rsquo;t
easily scale up or down the time they spent working to translate time into money. The easiest way to think of this is
someone who tries to work less hours (and get less pay) in order to spend more time with their family. One way to do
exactly that is to cut down on commute time.</p>

<h4>The Hidden Mortgage</h4>

<p>Now it&rsquo;s time to tie this back up with the price for housing. After all, the time you spent on your commute and your
mode choice for that commute, are intimately tied to where you choose to live. How do we combine the commute cost with
the housing cost?
As a rule of thumb, one makes $400 monthly payments for each $100,000 of mortgage. Give or take, but our numbers are
quite rough anyway. So taking the annual commute cost, dividing by 400 and multiplying by 100,000 we get the &lsquo;commute
cost mortgage equivalent&rsquo;. If we map it it looks the same as the previous map, just with different labels. But since I
like maps, here it is.</p>

<p><img src="http://doodles.mountainmath.ca/images/commute_mortgage_equivalent.jpg" alt="&quot;Commute Mortgage Equivalent&quot;" /></p>

<p>In other words, we translated the commute cost into &lsquo;hidden mortgage&rsquo; payments. Except, unlike a real mortgage, paying
it off does not generate any value for you.</p>

<h4>Combined Dwelling Value and Commute Cost</h4>

<p>Finally we have all we need for the end result. The Combined Dwelling Value and Commute Time/Mode Value Map where we
simply add the estimated commute mortgage cost onto the dwelling value.</p>

<p><img src="http://doodles.mountainmath.ca/images/combined_dwelling_commute.jpg" alt="&quot;Combined Dwelling Value And Commute Cost&quot;" /></p>

<p>It does not look radically different from the dwelling value map. It&rsquo;s almost the same around downtown where commute
time tends to be low and more expensive further out, where the &lsquo;hidden mortgage&rsquo; of commute time can add up to half a
million dollars.</p>

<p>Using Census Data can only help illustrate the hidden mortgage that people are already paying. It can only highlight
some general trends and ideas, it cannot make any statement about particular individual households, nor can the genral
reasoning used to derive the numbers apply to everyone. Or maybe even most people. The estimates are quite rough, likely
underestimating commute costs for drivers, not taking into account the well understood health benefits of active
transportation nor the negative health implications of driving. The commute time to money computation is very rough
and will have to be adjusted if applied to real-world examples. But this is only meant to illustrate ageneral point.</p>

<p>The takeaway should be that the &lsquo;hidden mortgage&rsquo; is real, and it&rsquo;s huge. And often overlooked.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vancouver 2011 Census Data on Housing]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/06/23/vancouver-2011-census-data-on-housing/"/>
    <updated>2015-06-23T15:26:23-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/06/23/vancouver-2011-census-data-on-housing</id>
    <content type="html"><![CDATA[<p>Last week my friend Alejandro emailed me some census data. After sitting on it for a couple of days I decided one evening
to take a look and map the dissemination areas. The dataset contained 125 fields around housing and income. So I started
plucking some of the fields off the table and mapping them.</p>

<p>It did not take long and colorful maps started showing up. Another evening and lots of map layers later I put the
<a href="https://mountainmath.ca/census">Vancouver Census Map</a> online
for people interested to look through the census graphs. The number of layers became a little unwieldly, just pick and
choose what you are interested in.</p>

<p><a href="https://mountainmath.ca/census/?layer=17"><img src="http://doodles.mountainmath.ca/images/affordability.jpg" alt="&quot;Affordability&quot;" /></a></p>

<h4>So what exactly does the Census Data show?</h4>

<!-- more -->


<p>The data is aggregated at the Dissemination Area level and rounded to protect privacy. That leaves some artefacts, but
generally gives a very detailed picture of what is going on in different parts of the city. The biggest drawback is the
age, the data is now 4 to 5 years old. Lots of things happened in the meantime, we will have to wait until 2016 for the
next Census Canada dataset.</p>

<h4>Where to go from here?</h4>

<p>When I get around to it I might map all of Metro Van Census data. It won&rsquo;t take any time to re-run the import script for
the larger dataset, but I will need to show higher aggregation level data at lower zoom levels to keep it repsonsive
for slower machines, just like I did for the <a href="https://mountainmath.ca/map/assessment">Vancouver Assessment Map</a></p>

<p>I should probably also take a look at what other fields are available, the &lsquo;affordability&rsquo; map linked above uses
individual income for full-time employees instead of combined household income, which differes from how affordability
is usually calculated. The reason is that the data I had did not have the household income. Not sure if it is available
at the dissemination area aggregation level, but if it is that one should be used. But I am lazy, so for now that&rsquo;s it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tax Density in Vancouver]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/05/31/density-in-vancouver/"/>
    <updated>2015-05-31T22:44:55-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/05/31/density-in-vancouver</id>
    <content type="html"><![CDATA[<p>The other day I saw that Downtown brings in 23% of CoV tax revenue but only makes up 5% of the city area. Intrigued by that I decided to
add a &lsquo;Tax Density&rsquo; layer to my <a href="https://mountainmath.ca/map/assessment">Vancouver Assessment Map</a>.
The idea was to try and understand the tax revenue generated by different areas in Vancouver. The data is already available
in the CoV open data property dataset so it would only take half an hour to ad it to my map.</p>

<p><a href="http://mountainmath.ca/map/assessment"><img src="http://doodles.mountainmath.ca/images/tax_density.jpg" alt="&quot;Tax Density&quot;" /></a></p>

<p>There has been a lot of discussion around density in Vancouver. One aspect of density is that it will generally increase
the tax revenue that the city can collect per square metre. At the same time the city spending for services of the added
density increases at a much lower rate. Leaving a net gain of revenue for the city. As property taxes are
need-based, this means lower property taxes for everyone when density increases. In short, density leads to efficiency
increases that materialize in form of lower property taxes for everyone.</p>

<p>That also points to one possible way to break some of the resistance to density increases that Vancouver currently sees.
If a neighbourhood (or smaller region) accepts density increases, maybe some of the associated benefits
of lower property taxes should be applied locally instead of everything being spread out over the entire city.</p>

<h4>So what exactly does the Tax Density layer show?</h4>

<!-- more -->


<p>The Tax Density I mapped is simply the amount of taxes collected per m&sup2;.
Under that measure, Downtown comes out at $127.1/m&sup2;, followed by the West End at $72.7/m&sup2; and Fairview at
$42.7/m&sup2;. The Vancouver average is $20/m&sup2;, only 6 of the 23 neighbourhoods have an above-average Tax Density.
A complete list of Tax Density by neighbourhood is at the bottom.</p>

<p>For this calculation I did not count the parks doward the area of
the neighbourhoods, but school for example are included. In light of this some of the aggregated data should be viewed
with caution.</p>

<h4>Where to go from here?</h4>

<p>I also thought about mapping the density of housing units, but sadly the data in the CoV open data catalogue is ill-suited to
do this. I can easily extract the number of tax entities and map the density of these. This works great for a traditional
single family home (one tax entity) and for stratas (one tax entity per unit), but it becomes a problem because e.g. laneway
houses and granny suites don&rsquo;t show up as separate tax entities. And it gets really bad with rental apartments, where the
whole building will be a single tax entity with potentially a very large number of units. And then the dataset does not
distinguish between commercial and residential units. That data could be reverse engineered from the tax data, but that&rsquo;s
more work than my usual half hour tolerance level for this kind of side project.</p>

<p>Another option is to use census data and merge the datasets. But that gets messy, the aggregation levels don&rsquo;t match and
this is well beyond a side project time frame.</p>

<p>Similarly, it would be interesting to map the changes in tax density and the changes in unit density in Vancouver, but
again the CoV dataset does hold the necessary information. In particular, when properties get a new tax coordinate (for
example when they get re-developed) the CoV dataset drops the old tax data associated with that physical location. This
makes it impossible to map the changes in tax density that redevelopment has brought.</p>

<p>Long story hort, if you are interested in browsing Vancouver by tax revenue collected per square meter, click through to
the <a href="https://mountainmath.ca/map/assessment">Vancouver Assessment Map</a> and select the Tax Density layer.</p>

<h2>Tax density by neighbourhood</h2>

<ol>
<li>Downtown: $127.1/m&sup2;</li>
<li>West End: $72.7/m&sup2;</li>
<li>Fairview: $42.7/m&sup2;</li>
<li>Kitsilano: $27.8/m&sup2;</li>
<li>Downtown Eastside: $26.4/m&sup2;</li>
<li>Mount Pleasant: $25.4/m&sup2;</li>
<li>Arbutus Ridge: $17.5/m&sup2;</li>
<li>Grandview-Woodland: $16.4/m&sup2;</li>
<li>West Point Grey: $16.3/m&sup2;</li>
<li>Dunbar Southlands: $15.0/m&sup2;</li>
<li>Riley Park: $14.4/m&sup2;</li>
<li>Oakridge: $13.8/m&sup2;</li>
<li>Shaughnessy: $13.5/m&sup2;</li>
<li>Kensington-Cedar Cottage: $12.8/m&sup2;</li>
<li>Renfrew-Collingwood: $12.4/m&sup2;</li>
<li>Strathcona: $12.2/m&sup2;</li>
<li>Kerrisdale: $11.9/m&sup2;</li>
<li>Marpole: $11.8/m&sup2;</li>
<li>South Cambie: $11.7/m&sup2;</li>
<li>Sunset: $11.2/m&sup2;</li>
<li>Hastings-Sunrise: $10.6/m&sup2;</li>
<li>Victoria-Fraserview: $10.0/m&sup2;</li>
<li>Killarney: $7.7/m&sup2;</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vancouver Bike Paths]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/04/01/bike-paths/"/>
    <updated>2015-04-01T16:24:34-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/04/01/bike-paths</id>
    <content type="html"><![CDATA[<p>Motivated by the excellent <a href="http://www.washingtonpost.com/blogs/wonkblog/wp/2015/04/01/bleak-maps-of-how-cities-look-using-only-their-bike-lanes/">Washington Post Wonkblog</a> I
mapped Vancouver&rsquo;s bike infrastructure. Looks good at first, but when you take out the &lsquo;fake&rsquo; &ldquo;Local Street&rdquo; bikeways more in line with Wonkblog&rsquo;s methodology it&rsquo;s looking pretty
bleak in large portions of the city.</p>

<p>Of course the devil is in the details and infrastructure cannot just be judged by how it&rsquo;s labeled. If one were to look for 8-80 infrastructure, then the separated lanes
would make the cut. Some of the bike lanes would be marginal, but most would probably make the cut. Although I can&rsquo;t say that I would be particularly keen to let an 8yo cycle on
some of them.</p>

<p>Vancouver&rsquo;s &ldquo;Shared Lanes&rdquo; are not for the faint of heart. They have 50 speed limits on multi-lane roadways
and feature sharrows to make it a little easier for cyclists to &lsquo;take the lane&rsquo;. In some cases cyclists share a lane with
the buses, which is a little better.</p>

<p>The local streets are mostly car tunnels like the &lsquo;Off-Broadway&rsquo; and won&rsquo;t qualify as 8-80, but there are some exceptions like
the local street portion of Pt. Grey Rd, where cycling is rather pleasant.</p>

<iframe src="http://doodles.mountainmath.ca/bike_paths.html?fh=50&nh=true" width="100%" height="580"></iframe>


<p><a href="http://doodles.mountainmath.ca/bike_paths.html">Full screen view</a></p>

<p>Methodology is simple, it&rsquo;s just the city&rsquo;s bike path data from <a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">their open data catalogue</a>. The data
comes with two fields, &lsquo;name&rsquo; and &lsquo;type&rsquo;. The &lsquo;type&rsquo; was used for the checkboxes at the bottom to selectively turn different
bike lane types on or off and the &lsquo;name&rsquo; will be displayed on hover.</p>

<h3>Update</h3>

<h4>Some more <strike>Canadian</strike> cities:</h4>

<p><a href="http://doodles.mountainmath.ca/bike_paths?city=Calgary,street&amp;type=BICYCLE_CL&amp;dataUrl=http://doodles.mountainmath.ca/data/CALGIS_TRAN_BIKEWAY.geojson">Calgary on street</a>,
<a href="http://doodles.mountainmath.ca/bike_paths?city=Calgary,pathways&amp;type=PRIORITY&amp;name=LOCATION_D&amp;dataUrl=http://doodles.mountainmath.ca/data/YYC_Path_BikeRoutes.geojson">Calgary pathways</a>,
<a href="http://doodles.mountainmath.ca/bike_paths?city=Calgary,trails&amp;type=TYPE_DESCR&amp;name=LOCATION_D&amp;dataUrl=http://doodles.mountainmath.ca/data/YYC_Trail_BikeRoutes.geojson">Calgary trails</a>,
<a href="http://doodles.mountainmath.ca/bike_paths?city=Victoria&amp;type=FullDescr&amp;dataUrl=http://doodles.mountainmath.ca/data/VicBikeRoutes.geojson">Victoria</a>,
<a href="http://doodles.mountainmath.ca/bike_paths?city=Montreal&amp;name=PROJET_NOM&amp;dataUrl=http://doodles.mountainmath.ca/data/MontrealBikeRoutes.geojson">Montreal</a>,
<a href="http://doodles.mountainmath.ca/bike_paths?city=Toronto&amp;type=CP_TYPE&amp;dataUrl=http://doodles.mountainmath.ca/data/TorontoBikeRoutes.geojson">Toronto</a>
<a href="http://doodles.mountainmath.ca/bike_paths?city=Portland&amp;type=FACILITYDE&amp;name=SEGMENTNAM&amp;dataUrl=http://doodles.mountainmath.ca/data/PDXBikeRoutes.geojson">Portland</a>
<a href="http://doodles.mountainmath.ca/bike_paths?city=NYC&amp;type=TF_Facilit&amp;name=Street&amp;fh=65&amp;dataUrl=http://doodles.mountainmath.ca/data/NYCBikeRoutes.geojson">NYC</a>
<a href="http://doodles.mountainmath.ca/bike_paths?city=Amsterdam&amp;type=HIGHWAY&amp;name=WIDTH&amp;dataUrl=http://doodles.mountainmath.ca/data/AmsterdamBikeRoutes.geojson">Amsterdam</a>
<a href="http://doodles.mountainmath.ca/bike_paths?city=Taipei&amp;name=RDNAME&amp;dataUrl=http://doodles.mountainmath.ca/data/TPEBikeRoutes.geojson">Taipei</a></p>

<p>A word of caution. I have done zero data cleaning or verification. Some of these maps are missing some type of infrastructure.
I am familiar with cycling conditions in Calgary, and their the extensive network along the river and other areas are missing in their main file.
I added two more separate maps for their trails and pathways, but was too lazy to merge them. For Taipei, I noticed that
the off-street paths along streets are missing, for example the one on Dunhua Bei Lu that I was using a lot a year ago. Not sure if or where these are available.
So some more ground truth is needed for proper interpretation. But fun anyway.</p>

<p>Want to map another city&rsquo;s data? No problem, just read on.</p>

<!-- more -->


<ol>
<li>Locate the city&rsquo;s bikeway data and download it.</li>
<li>Convert the data to geojson, with coordinates in latitude and longitude.</li>
<li>Put the geojson file online somewhere, e.g. your public dropbox folder.</li>
<li>Optionally look at the geojson file for a bikeway type descriptor and bikeway name, if available.</li>
<li>Build a url for your map by using [<a href="http://doodles.mountainmath.ca/bike_paths">http://doodles.mountainmath.ca/bike_paths</a>] as a base url and add query strings

<ul>
<li><code>dataUrl=&lt;url to your geojson&gt;</code></li>
<li><code>city=&lt;city name&gt;</code></li>
<li>optionally <code>type=&lt;bikeway type property&gt;</code></li>
<li>optionally <code>name=&lt;bikeway name property&gt;</code></li>
<li>optionally <code>zoom=true</code> if you want to be able to zoom and pan on the map</li>
</ul>
</li>
</ol>


<p>For example, to map Calgary&rsquo;s bike network you need to <a href="https://data.calgary.ca/opendata/Pages/DatasetListingAlphabetical.aspx#C">got to their open data website</a> and
download the Tranportation Bikeways shapefiles (SHP). To convert them go geojson using <code>ogr2ogr</code></p>

<pre><code>ogr2ogr -f GeoJSON -simplify 1 -s_srs CALGIS_TRAN_BIKEWAY.prj -t_srs "EPSG:4326" CALGIS_TRAN_BIKEWAY.geojson CALGIS_TRAN_BIKEWAY.shp
</code></pre>

<p>from the <a href="http://trac.osgeo.org/gdal/wiki/DownloadingGdalBinaries">GDAL package</a>.</p>

<p>I uploaded the geojson to <code>http://doodles.mountainmath.ca/data/CALGIS_TRAN_BIKEWAY.geojson</code>, so that will be the value of the <code>dataUrl</code> query string.</p>

<p>Looking at the resulting geojson file you will see that the data does not include bikeway names, but it does include
types and the property is called <code>BICYCLE_CL</code>.</p>

<p>The rather lengthy link to the map would then be
<a href="http://doodles.mountainmath.ca/bike_paths?city=Calgary&amp;type=BICYCLE_CL&amp;dataUrl=http://doodles.mountainmath.ca/data/CALGIS_TRAN_BIKEWAY.geojson">http://doodles.mountainmath.ca/bike_paths?city=Calgary&amp;type=BICYCLE_CL&amp;dataUrl=http://doodles.mountainmath.ca/data/CALGIS_TRAN_BIKEWAY.geojson</a></p>

<iframe src="http://doodles.mountainmath.ca/bike_paths?city=Calgary&type=BICYCLE_CL&fh=50&nh=true&dataUrl=http://doodles.mountainmath.ca/data/CALGIS_TRAN_BIKEWAY.geojson" width="100%" height="580"></iframe>


<p>Or if you want to zoom into Calgary&rsquo;s sprawling suburbs, you could also enable zooming (using double-click) and panning
<a href="http://doodles.mountainmath.ca/bike_paths?city=Calgary&amp;type=BICYCLE_CL&amp;zoom=true&amp;dataUrl=http://doodles.mountainmath.ca/data/CALGIS_TRAN_BIKEWAY.geojson">http://doodles.mountainmath.ca/bike_paths?city=Calgary&amp;type=BICYCLE_CL&amp;zoom=true&amp;dataUrl=http://doodles.mountainmath.ca/data/CALGIS_TRAN_BIKEWAY.geojson</a></p>

<p>Happy Mapping!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vancouver Assessment Data]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2015/03/23/vancouver-assessment-data/"/>
    <updated>2015-03-23T16:20:43-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2015/03/23/vancouver-assessment-data</id>
    <content type="html"><![CDATA[<p>The City of Vancouver has recently
updated <a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">their open data catalogue</a> with historic tax
data. We figured that would
be a good time to take our previous rather clumsy attempts to map tax data and polish that up a bit.</p>

<p>This time we put in a couple of evenings to build a more responsive map with multiple options of what to map. Head
straight for <a href="http://mountainmath.ca/map/assessment">&lsquo;the map&rsquo;</a> or read on for some background information, including
some methodology on data cleaning and interpolation.</p>

<p><a href="http://mountainmath.ca/map/assessment"><img src="http://doodles.mountainmath.ca/images/land_value.jpg" alt="&quot;Land Values&quot;" /></a></p>

<!-- more -->


<p>Our previous <a href="http://doodles.mountainmath.ca/teardown_map.html">&lsquo;teardown map&rsquo;</a> and <a href="http://doodles.mountainmath.ca/teardown_map.html">&lsquo;high value map&rsquo;</a> suffered from being a static
websites, and they loaded large amounts of data all at once, 30,000 properties for the teardown map, and display
the all at once. Options were limited for displaying more detailed data when selecting individual properties.
For the viewer this resulted more in an exercise of patience than in an &lsquo;interactive&rsquo; experience.</p>

<p>We decided to remedy this by importing the CoV data into a database and chop the data up dynamically and serve it as vector tiles.
For smaller zoom levels we aggregated data at the block level. There are lots of blocks in Vancouver, 4444 that contain
properties with tax information to be precise. So again the map is a little sluggish on mobile devices and slower machines when zoomed out,
but things get much faster as we zoom in.</p>

<p>Moreover, we now have the ability to easily display more detailed information once the use selects an individual
property or block aggregate. We can display a graph with the development of land and building values and other details.
And we integrated google streetview for good measure.</p>

<p>One issue that came up is that CoV historical data has lots of gaps. For example, when a particular property was
re-developed it gets a new tax number. The tax information for the old property is lost and cannot be connected to the
geographic site using the information provided by CoV. So when we display data showing &lsquo;value change&rsquo; between
2006 and 2014 we color these properties in grey. This problem persists when we aggregate information at the block level.
To avoid greying out lots of blocks we extrapolate the missing data by using citywide averages on growth rates
of land and building values for the properties in question. This will likely underestimate the aggregate growth in building
and land value, but will not have a big impact at the block level if the individual property value in question was
comparatively small. But this was not always the case. Better analysis could solve some of these issues, so does
zooming in to the individual property level.</p>

<p>Another issue is that CoV does not provide historic zoning information. It would be interesting to get historic zoning
and development permit information. Some of this data is already available, albeit not in an easily consumable form.</p>

<p>Other than that a big thanks to the folks maintaining the CoV open data catalogue!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mapping CACs]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2014/11/05/mapping-cacs/"/>
    <updated>2014-11-05T12:03:25-08:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2014/11/05/mapping-cacs</id>
    <content type="html"><![CDATA[<p>After reading the <a href="http://pricetags.wordpress.com/2014/11/04/the-daily-durning-back-and-forth-on-cacs/">post on Price Tags</a>,
originally from <a href="http://karensawatzky.ca/2014/10/29/7868/">Karen Sawatzky&rsquo;s blog</a>, I got curious about getting a
geographic overview over the CACs in Vancouver in 2010, 2011 and 2012.</p>

<p>As opposed to the <a href="http://doodles.mountainmath.ca/blog/2014/09/05/vancouver-teardown-map/">teardown map</a> the dataset is quite small, so there should
be no problems viewing this on a mobile device. As always, map is below the fold.</p>

<!-- more -->


<p>Some of the property addresses did not match any address in the CoV open data property database, probably because the
developments are quite new and new addresses got assigned. In these cases I mapped the CAC property to a nearby property
and added the &lsquo;Mapped Address&rsquo; field in the info pane. Enjoy.</p>

<iframe src="http://doodles.mountainmath.ca/cac_map.html" width="100%" height="500"></iframe>


<p><a href="http://doodles.mountainmath.ca/cac_map.html">Full screen view</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vancouver High Value Improvements Map]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2014/09/07/vancouver-high-value-improvements-map/"/>
    <updated>2014-09-07T22:11:08-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2014/09/07/vancouver-high-value-improvements-map</id>
    <content type="html"><![CDATA[<p>After the <a href="http://doodles.mountainmath.ca/blog/2014/09/05/vancouver-teardown-map/">teardown map</a> that looked at the low end of Vancouver&rsquo;s building
stock, a natural question is to look at the high end. Just as before, we are focusing on the (assessed) value of the
building relative to the total (assessed) value of land and improvements. We defined &ldquo;highly improved&rdquo; properties to
be properties where the ratio is at least 50%, so the building is worth at least as much as the land it is built on.</p>

<p>That&rsquo;s actually a pretty high bar in Vancouver, less than 1500 buildings pass that muster. And in concordance with the
teardown map, the &ldquo;high value improvements&rdquo; seem to concentrate in downtown. See the map below the fold
to explore for yourself.</p>

<!-- more -->


<p>When browsing the map it becomes clear that some properties have &ldquo;high value improvements&rdquo; because of an abnormally low
land value, rather than an abnormally high improvement value. This is probably due to intricacies of zoning in Vancouver
that maybe someone else can shed some light on.</p>

<iframe src="http://doodles.mountainmath.ca/improvement_map.html" width="100%" height="500"></iframe>


<p><a href="http://doodles.mountainmath.ca/improvement_map.html">Full screen view</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vancouver Teardown Map]]></title>
    <link href="http://doodles.mountainmath.ca/blog/2014/09/05/vancouver-teardown-map/"/>
    <updated>2014-09-05T13:27:18-07:00</updated>
    <id>http://doodles.mountainmath.ca/blog/2014/09/05/vancouver-teardown-map</id>
    <content type="html"><![CDATA[<p>There has been lots of talk about old homes being torn down and replaced in Vancouver. The likeliest targets are houses
of relatively low value sitting on expensive land. So how many teardown candidates are there in Vancouver, and where
in Vancouver are they located?</p>

<p>To answer this we use data from <a href="http://vancouver.ca/your-government/open-data-catalogue.aspx">Vancouver&rsquo;s open data catalogue</a>
to build an interactive map to explore the low end of the building stock. More specifically, we set a somewhat arbitrary
ratio of (assessed) house value to total (assessed) property value (house + land) of 5% and label everything below that
a &ldquo;teardown candidate&rdquo;.</p>

<p>The short answer is that almost one in three properties in Vancouver fall into this category. And they are distributed
quite evenly across all of Vancouver with some areas like downtown looking a little better.</p>

<!-- more -->


<p>On the level of the individual property the &ldquo;teardown candidate&rdquo; designation might be unfair and somewhat inflammatory.
By our above definition, a million dollar house sitting on a 20 million dollar property is a &ldquo;teardown candidate&rdquo;. In
some cases BC Assessment may have failed to properly account for recent renovations resulting in the building being
undervalued and more likely to show up as a &ldquo;teardown candidate&rdquo;. And of course not every &ldquo;teardown candidate&rdquo; will be
torn down, other options may be preferable. Depending on circumstance extensive renovations may be the better option,
and the pervasiveness of &ldquo;teardown candidates&rdquo; suggest that many people in Vancouver are happy living in a house that
is worth less than 5% of the overall assessed value of the property.</p>

<p>To aid a more individualized look at properties we included the address, the assessed land and improvement values, the
tax levy (CoV just added this information a couple of days ago), zoning information, the land area and the relative land
value. To display this information just hover your mouse over a property (or touch it on your mobile device). Opening a
second browser with a map with satellite imagery (or Apple maps) can provide additional context.</p>

<p>Take a look at the map and explore for yourself. You will need a reasonably fast computer and modern
browser to enjoy the map, it contains lots of data and may feel quite slow on mobile devices.</p>

<iframe src="http://doodles.mountainmath.ca/teardown_map.html" width="100%" height="500"></iframe>


<p><a href="http://doodles.mountainmath.ca/teardown_map.html">Full screen view</a></p>
]]></content>
  </entry>
  
</feed>
